{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Training an FNO with incremental meta-learning\nA demo of the Incremental FNO meta-learning algorithm on our small Darcy-Flow dataset.\n\nThis tutorial demonstrates incremental meta-learning for neural operators, which allows\nthe model to gradually increase its complexity during training. This approach can lead to:\n\n- Better convergence properties\n- More stable training dynamics\n- Improved generalization\n- Reduced computational requirements during early training\n\nThe incremental approach starts with a small number of Fourier modes and gradually\nincreases the model capacity as training progresses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## Import dependencies\nWe import the necessary modules for incremental FNO training\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport matplotlib.pyplot as plt\nimport sys\nfrom neuralop.models import FNO\nfrom neuralop.data.datasets import load_darcy_flow_small\nfrom neuralop.utils import count_model_params\nfrom neuralop.training import AdamW\nfrom neuralop.training.incremental import IncrementalFNOTrainer\nfrom neuralop.data.transforms.data_processors import IncrementalDataProcessor\nfrom neuralop import LpLoss, H1Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## Loading the Darcy-Flow dataset\nWe load the Darcy-Flow dataset with multiple resolutions for incremental training.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_loader, test_loaders, output_encoder = load_darcy_flow_small(\n    n_train=100,\n    batch_size=16,\n    test_resolutions=[16, 32],\n    n_tests=[100, 50],\n    test_batch_sizes=[32, 32],\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## Configuring incremental training\nWe set up the incremental FNO model with a small starting number of modes.\nThe model will gradually increase its capacity during training.\nWe choose to update the modes using the incremental gradient explained algorithm\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "incremental = True\nif incremental:\n    starting_modes = (2, 2)  # Start with very few modes\nelse:\n    starting_modes = (8, 8)  # Standard number of modes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## Creating the incremental FNO model\nWe create an FNO model with a maximum number of modes that can be reached\nduring incremental training. The model starts with fewer modes and grows.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = FNO(\n    max_n_modes=(8, 8),  # Maximum modes the model can reach\n    n_modes=starting_modes,  # Starting number of modes\n    hidden_channels=32,\n    in_channels=1,\n    out_channels=1,\n)\nmodel = model.to(device)\nn_params = count_model_params(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## Setting up the optimizer and scheduler\nWe use AdamW optimizer with weight decay for regularization\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=8e-3, weight_decay=1e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## Configuring incremental data processing\nIf one wants to use Incremental Resolution, one should use the IncrementalDataProcessor.\nWhen passed to the trainer, the trainer will automatically update the resolution.\n\nKey parameters for incremental resolution:\n\n- incremental_resolution: bool, default is False. If True, increase the resolution of the input incrementally\n- incremental_res_gap: parameter for resolution updates\n- subsampling_rates: a list of resolutions to use\n- dataset_indices: a list of indices of the dataset to slice to regularize the input resolution\n- dataset_resolution: the resolution of the input\n- epoch_gap: the number of epochs to wait before increasing the resolution\n- verbose: if True, print the resolution and the number of modes\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_transform = IncrementalDataProcessor(\n    in_normalizer=None,\n    out_normalizer=None,\n    device=device,\n    subsampling_rates=[2, 1],  # Resolution scaling factors\n    dataset_resolution=16,  # Base resolution\n    dataset_indices=[2, 3],  # Dataset indices for regularization\n    epoch_gap=10,  # Epochs between resolution updates\n    verbose=True,  # Print progress information\n)\n\ndata_transform = data_transform.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## Setting up loss functions\nWe use H1 loss for training and L2 loss for evaluation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "l2loss = LpLoss(d=2, p=2)\nh1loss = H1Loss(d=2)\ntrain_loss = h1loss\neval_losses = {\"h1\": h1loss, \"l2\": l2loss}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## Displaying training configuration\nWe display the model parameters, optimizer, scheduler, and loss functions\nto verify our incremental training setup\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n### N PARAMS ###\\n\", n_params)\nprint(\"\\n### OPTIMIZER ###\\n\", optimizer)\nprint(\"\\n### SCHEDULER ###\\n\", scheduler)\nprint(\"\\n### LOSSES ###\")\nprint(\"\\n### INCREMENTAL RESOLUTION + GRADIENT EXPLAINED ###\")\nprint(f\"\\n * Train: {train_loss}\")\nprint(f\"\\n * Test: {eval_losses}\")\nsys.stdout.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## Configuring the IncrementalFNOTrainer\nWe set up the IncrementalFNOTrainer with various incremental learning options.\nOther options include setting incremental_loss_gap = True.\nIf one wants to use incremental resolution, set it to True.\nIn this example we only update the modes and not the resolution.\nWhen using incremental resolution, keep in mind that the number of modes\ninitially set should be strictly less than the resolution.\n\nKey parameters for incremental training:\n\n- incremental_grad: bool, default is False. If True, use the base incremental algorithm based on gradient variance\n- incremental_grad_eps: threshold for gradient variance\n- incremental_buffer: number of buffer modes to calculate gradient variance\n- incremental_max_iter: initial number of iterations\n- incremental_grad_max_iter: maximum iterations to accumulate gradients\n- incremental_loss_gap: bool, default is False. If True, use the incremental algorithm based on loss gap\n- incremental_loss_eps: threshold for loss gap\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create the IncrementalFNOTrainer with our configuration\ntrainer = IncrementalFNOTrainer(\n    model=model,\n    n_epochs=20,\n    data_processor=data_transform,\n    device=device,\n    verbose=True,\n    incremental_loss_gap=False,  # Use gradient-based incremental learning\n    incremental_grad=True,  # Enable gradient-based mode updates\n    incremental_grad_eps=0.9999,  # Gradient variance threshold\n    incremental_loss_eps=0.001,  # Loss gap threshold\n    incremental_buffer=5,  # Buffer modes for gradient calculation\n    incremental_max_iter=1,  # Initial iterations\n    incremental_grad_max_iter=2,  # Maximum gradient accumulation iterations\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## Training the incremental FNO model\nWe train the model using incremental meta-learning. The trainer will:\n1. Start with a small number of Fourier modes\n2. Gradually increase the model capacity based on gradient variance\n3. Monitor the incremental learning progress\n4. Evaluate on test data throughout training\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer.train(\n    train_loader,\n    test_loaders,\n    optimizer,\n    scheduler,\n    regularizer=False,\n    training_loss=train_loss,\n    eval_losses=eval_losses,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## Visualizing incremental FNO predictions\nWe visualize the model's predictions after incremental training.\nNote that we trained on a very small resolution for a very small number of epochs.\nIn practice, we would train at larger resolution on many more samples.\n\nHowever, for practicality, we created a minimal example that:\ni) fits in just a few MB of memory\nii) can be trained quickly on CPU\n\nIn practice we would train a Neural Operator on one or multiple GPUs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "test_samples = test_loaders[32].dataset\n\nfig = plt.figure(figsize=(7, 7))\nfor index in range(3):\n    data = test_samples[index]\n    # Input x\n    x = data[\"x\"].to(device)\n    # Ground-truth\n    y = data[\"y\"].to(device)\n    # Model prediction: incremental FNO output\n    out = model(x.unsqueeze(0))\n\n    # Plot input x\n    ax = fig.add_subplot(3, 3, index * 3 + 1)\n    x = x.cpu().squeeze().detach().numpy()\n    y = y.cpu().squeeze().detach().numpy()\n    ax.imshow(x, cmap=\"gray\")\n    if index == 0:\n        ax.set_title(\"Input x\")\n    plt.xticks([], [])\n    plt.yticks([], [])\n\n    # Plot ground-truth y\n    ax = fig.add_subplot(3, 3, index * 3 + 2)\n    ax.imshow(y.squeeze())\n    if index == 0:\n        ax.set_title(\"Ground-truth y\")\n    plt.xticks([], [])\n    plt.yticks([], [])\n\n    # Plot model prediction\n    ax = fig.add_subplot(3, 3, index * 3 + 3)\n    ax.imshow(out.cpu().squeeze().detach().numpy())\n    if index == 0:\n        ax.set_title(\"Incremental FNO prediction\")\n    plt.xticks([], [])\n    plt.yticks([], [])\n\nfig.suptitle(\"Incremental FNO predictions on Darcy-Flow data\", y=0.98)\nplt.tight_layout()\nfig.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}