{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Sinusoidal Embeddings\n\nInputs to deep learning models often represent positions on a spatial, temporal, or \nspatio-temporal grid. To enrich these coordinates, positional embeddings can be introduced \nto improve a model's capacity to generalize across the domain. In this tutorial, we focus \non sinusoidal positional embeddings.\n\nSinusoidal embeddings encode inputs as periodic functions (sines and cosines), thereby \nlifting low-dimensional coordinates into a richer spectral representation. This spectral \nlifting enhances the model's ability to capture fine-scale variations and high-frequency \ndynamics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## Setup in 1D\nTo build intuition, consider a simple 1D example. Let $x \\in \\mathbb{R}$\nbe a single input, and define the embedding function\n\n\\begin{align}g: \\mathbb{R} \\rightarrow \\mathbb{R}^{2 L}, \\quad g(x)=[\\sin (x), \\  \\cos (x), \\ \\sin (2 x), \\ \\cos (2 x), \\ldots, \\ \\sin (L x), \\ \\cos (L x)],\\end{align}\n\nwhere $L$ defines the number of frequencies we wish to use for the embedding. Each\npair of sine and cosine terms introduces a higher frequency, enriching how positional\ninformation is represented.\n\nThis idea naturally extends to an entire 1D input. Let $\\vec{x} \\in \\mathbb{R}^N$\ndenote a discretized domain of $N$ points. Then the embedding function becomes\n\n\\begin{align}g: \\mathbb{R}^N \\rightarrow \\mathbb{R}^{N \\times 2 L}, \\quad g(\\vec{x})=\\operatorname{concat}(\\sin (\\vec{x}), \\cos (\\vec{x}), \\sin (2 \\vec{x}), \\cos (2 \\vec{x}), \\ldots, \\sin (L \\vec{x}), \\cos (L \\vec{x})),\\end{align}\n\nIn practice, both the original coordinate and its embedding are passed to the model:\n\n\\begin{align}\\operatorname{input}(\\vec{x})=\\operatorname{concat}(\\vec{x}, \\ g(\\vec{x})) \\in \\mathbb{R}^{N \\times 2 L + 1},\\end{align}\n\npreserving the original input, while augmenting it with a hierarchy of frequency components.\n\n.. raw:: html\n\n   <div style=\"margin-top: 2em;\"></div>\n\n### Domain Normalization\nWhen applying sinusoidal embeddings, it is often useful to normalize the input coordinates\nto a periodic interval that aligns with the natural period of the sine and cosine functions.\nFor example, a 1D spatial domain $\\vec{x} \\in[0,1]$ of $N$ points can be rescaled to\n\n\\begin{align}\\vec{x}^{\\prime}=2 \\pi \\vec{x},\\end{align}\n\nso that $\\vec{x}^{\\prime} \\in[0,2 \\pi]$.\n\nThis mapping preserves the number of sampling points $N$ and the overall shape of the domain\nwhile ensuring that the lowest-frequency sine and cosine components complete exactly one\nfull oscillation over the interval.\n\n.. raw:: html\n\n   <div style=\"margin-top: 2em;\"></div>\n\n### Choosing $L$ to Satisfy the Nyquist-Criterion\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>When choosing the number of frequency levels $L$, it is important to ensure that the\n   highest frequency component in the embedding does not exceed the Nyquist limit imposed by\n   the discretisation of the input domain.</p></div>\n\nFor a domain of $N$ points, the Nyquist frequency is\n\n\\begin{align}f_{\\text{Nyquist}} = \\frac{N}{2}.\\end{align}\n\nFor the sinusoidal embedding defined above, the Nyquist constraint becomes:\n\n\\begin{align}L < \\frac{N}{2}.\\end{align}\n\nThe Nyquist frequency represents the maximum frequency that can be correctly captured\nwhen sampling a signal, equal to half the sampling rate. If frequencies higher than this\nlimit are used, they will not be represented as true high frequencies but will instead appear\nas lower ones, producing distortion known as aliasing. This is why we must ensure that\nthe highest frequency in our embedding does not exceed the Nyquist limit.\n\n.. raw:: html\n\n   <div style=\"margin-top: 2em;\"></div>\n\n### Visualizing the Sinusoidal Embeddings\nBelow, we visualize the sinusoidal embeddings for a spatial input domain\n$\\vec{x} \\in[0,1]$ consisting of 1000 equally spaced points, using $L = 3$ frequency levels.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Import required libraries\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom neuralop.layers.embeddings import SinusoidalEmbedding\n\n# Set default font sizes for better readability\nplt.rcParams.update(\n    {\n        \"font.size\": 14,\n        \"axes.titlesize\": 18,\n        \"axes.labelsize\": 16,\n        \"xtick.labelsize\": 14,\n        \"ytick.labelsize\": 14,\n        \"legend.fontsize\": 14,\n    }\n)\n\ndevice = \"cpu\"\n\n# Define a spatial domain and number of frequencies\n# Create 1000 equally spaced points in [0, 1]\n#  and normalize to [0, 2\u03c0] for proper sinusoidal embedding\nx = torch.linspace(0, 1, 1000)\nx_normalized = torch.linspace(0, 2 * torch.pi, len(x))\n# Number of frequency levels for the embedding\nL = 3\n\n# Check if the number of frequencies satisfies the Nyquist-Criterion\nif L < len(x_normalized)/2:\n    print(f\"Nyquist-Shannon sampling theorem is satisfied for the given number of frequencies {L}.\")\nelse:\n    print(f\"Nyquist-Shannon sampling theorem is violated for the given number of frequencies {L}.\")\n\n# Build embedding: [sin(x), cos(x), sin(2x), cos(2x), ...]\n# Each frequency level contributes a sine and cosine pair\ng = []\nfor l in range(1, L + 1):\n    g.append(torch.sin(l * x_normalized))\n    g.append(torch.cos(l * x_normalized))\n\n# Construct input by concatenating the original input and the embedding\n# This preserves the original coordinates while adding spectral information\ninput_arr = np.asarray([x, *g])\ninput_tensor = torch.tensor(input_arr)\n\n# Plot the embedding components\ncolors = plt.cm.tab10.colors\n\nplt.figure(figsize=(10, 5))\nfor freq_idx in range(L):\n    color = colors[freq_idx % len(colors)]\n    sin_idx = 2 * freq_idx + 1\n    cos_idx = 2 * freq_idx + 2\n\n    plt.plot(x, input_tensor[sin_idx], color=color, label=f\"Frequency {freq_idx + 1}\")\n    plt.plot(x, input_tensor[cos_idx], color=color)\n\nplt.xlabel(\"x\", fontsize=16)\nplt.ylabel(\"Embedding value\", fontsize=16)\nplt.title(\"Sinusoidal Embedding Components (L = 3)\", fontsize=18)\nplt.legend(loc=\"lower left\", framealpha=1.0, fontsize=14)\nplt.locator_params(axis=\"y\", nbins=5)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 4em;\"></div>\n\n## Encoding Constant Parameters\nA particularly useful extension of sinusoidal embeddings is their ability to encode constant\nparameters. Consider a setting where you have a scalar parameter $m$ (such as a material\nproperty, boundary condition, or physical constant) that you wish to feed into a model.\nInstead of treating $m$ as a fixed scalar input, we can represent it using periodic\nfunctions, either by modulating the amplitude or the frequency of the sinusoidal components.\n\n.. raw:: html\n\n   <div style=\"margin-top: 2em;\"></div>\n\n**1. Amplitude Modulation:** To encode $m$ by scaling the amplitudes of the sinusoidal\nfunctions, we define the embedding as\n\n\\begin{align}m \\rightarrow m g(\\vec{x}),\\end{align}\n\nwhere each element of the embedding $g(\\vec{x})$ is multiplied by $m$.\n\n.. raw:: html\n\n   <div style=\"margin-top: 2em;\"></div>\n\n**2. Frequency Modulation:** Alternatively, to encode $m$ by scaling the frequencies,\nwe define\n\n\\begin{align}m \\rightarrow g(m\\vec{x})\\end{align}\n\nwhere $m$ multiplies the input argument of each sinusoidal component.\n\nWhen encoding constant parameters through frequency modulation, care must be taken to ensure\nthat the Nyquist criterion is satisfied. In this case, where the modulation factor $m$\nscales the frequencies, the Nyquist constraint becomes $L < \\frac{N}{2m}$.\n\n.. raw:: html\n\n   <div style=\"margin-top: 2em;\"></div>\n\nBelow, we demonstrate an example of encoding the parameter $m = 2.5$ through both\namplitude and frequency modulation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define a spatial domain and number of frequencies\nx = torch.linspace(0, 1, 1000)\nx_normalized = torch.linspace(0, 2 * torch.pi, len(x))\nL = 3\n\n# Define parameter to encode\nm = 2.5\nm_tensor = torch.tensor([m])\n\n# Check if the number of frequencies and parameter satisfies the Nyquist-Criterion\nif L <= len(x_normalized)/(2 * m):\n    print(f\"Nyquist-Shannon sampling theorem is satisfied for the given parameter {m} and number of frequencies {L}.\")\nelse:\n    print(f\"Nyquist-Shannon sampling theorem is violated for the given parameter {m} and number of frequencies {L}.\")\n\n# Build amplitude-modulated embedding: m * g(x)\ng_amplitude = []\nfor l in range(1, L + 1):\n    g_amplitude.append(torch.sin(l * x_normalized) * m_tensor)\n    g_amplitude.append(torch.cos(l * x_normalized) * m_tensor)\n\n# Build frequency-modulated embedding: g(m * x)\ng_frequency = []\nfor l in range(1, L + 1):\n    g_frequency.append(torch.sin(l * x_normalized * m_tensor))\n    g_frequency.append(torch.cos(l * x_normalized * m_tensor))\n\n# Convert to arrays for visualization\ninput_amplitude = torch.tensor(np.asarray([x, *g_amplitude]))\ninput_frequency = torch.tensor(np.asarray([x, *g_frequency]))\n\n# Plot both embeddings\ncolors = plt.cm.tab10.colors\nfig, axes = plt.subplots(2, 1, figsize=(10, 9), sharex=True)\n\n## Amplitude modulation\nfor freq_idx in range(L):\n    color = colors[freq_idx % len(colors)]\n    sin_idx, cos_idx = 2 * freq_idx + 1, 2 * freq_idx + 2\n    axes[0].plot(x, input_amplitude[sin_idx], color=color, label=f\"Channel {freq_idx + 1}\")\n    axes[0].plot(x, input_amplitude[cos_idx], color=color)\naxes[0].set_title(\"Amplitude Modulation\", fontsize=18, pad=20)\naxes[0].set_ylabel(\"Embedding value\", fontsize=16)\naxes[0].legend(loc=\"lower left\", framealpha=1.0, fontsize=14)\naxes[0].locator_params(axis=\"y\", nbins=5)\n\n## Frequency modulation\nfor freq_idx in range(L):\n    color = colors[freq_idx % len(colors)]\n    sin_idx, cos_idx = 2 * freq_idx + 1, 2 * freq_idx + 2\n    axes[1].plot(x, input_frequency[sin_idx], color=color, label=f\"Channel {freq_idx + 1}\")\n    axes[1].plot(x, input_frequency[cos_idx], color=color)\naxes[1].set_title(\"Frequency Modulation\", fontsize=18, pad=20)\naxes[1].set_ylabel(\"Embedding value\", fontsize=16)\naxes[1].set_xlabel(\"x\", fontsize=16)\naxes[1].locator_params(axis=\"y\", nbins=5)\n\nplt.suptitle(f\"Sinusoidal Embeddings with Parameter m = {m}\", y=0.98, fontsize=20)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 4em;\"></div>\n\n## Neural Operator SinusoidalEmbedding Class\nThe ``neuralop`` library provides a unified sinusoidal positional embedding class,\n``neuralop.layers.embeddings.SinusoidalEmbedding``, with the following embedding techniques:\n\n- ``transformer`` - Vaswani, A. et al (2017), \"Attention Is All You Need\".\n- ``nerf`` - Mildenhall, B. et al (2020), \"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\".\n\nThe `SinusoidalEmbedding` class expects inputs to be of shape\n\n             ``(batch_size, N, input_channels)`` or ``(N, input_channels)``\n\n\n.. raw:: html\n\n   <div style=\"margin-top: 2em;\"></div>\n\n### Embedding Variants\nLet $\\vec{x} \\in \\mathbb{R}^N$ denote a 1D input domain consisting of\n$N$ discretized points. The embedding function\n$g: \\mathbb{R}^N \\rightarrow \\mathbb{R}^{N \\times 2L}$ maps each input value\n$x_n$ to a $2L$-dimensional vector composed of sine and cosine terms evaluated\nat different frequencies. Each embedding type defines these frequencies differently,\nleading to distinct representations.\n\n.. raw:: html\n\n   <div style=\"margin-top: 2em;\"></div>\n\n**1. Transformer-style embedding:** For $0 \\leq k < L$:\n\n\\begin{align}g(\\vec{x})_{:, 2 k}=\\sin \\left(\\frac{\\vec{x}}{\\text { max_positions }^{k / L}}\\right), \\quad g(\\vec{x})_{:, 2 k+1}=\\cos \\left(\\frac{\\vec{x}}{\\text { max_positions }^{k / L}}\\right) .\\end{align}\n\nHere, $\\text{max_positions}$ controls the maximum position for the embedding.\n\n.. raw:: html\n\n   <div style=\"margin-top: 2em;\"></div>\n\n**2. NeRF-style embedding:** For $0 \\leq k < L$:\n\n\\begin{align}g(\\vec{x})_{:, 2 k}=\\sin \\left(2^k \\pi \\vec{x}\\right), \\quad g(\\vec{x})_{:, 2 k+1}=\\cos \\left(2^k \\pi \\vec{x}\\right) .\\end{align}\n\nIn order to ensure that the Nyquist-Criterion is satisfied, for the Transformer-style\nembedding, the embedding frequencies should satisfy: $f_{\\max} < f_{\\text{Nyquist}}$.\n\nFor the NeRF-style embedding:\n\n\\begin{align}2^{L-1} < \\frac{N}{2} \\ \\ \\implies \\ \\  L < 1 + \\log_2\\left(\\frac{N}{2}\\right).\\end{align}\n\n.. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\nBelow, we include examples of using the `SinusoidalEmbedding` class with both the\ntransformer- and NeRF-style embeddings.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define a spatial domain and the number of frequencies\nx = torch.linspace(0, 1, 1000)\nx_normalized = torch.linspace(0, 2 * torch.pi, len(x)).reshape(-1, 1)\nL = 3\n\n# Check if the number of frequencies satisfies the Nyquist-Criterion\nif L <= 1 + torch.log2(torch.tensor(len(x_normalized)/2 )):\n    print(f\"Nyquist-Shannon sampling theorem is satisfied for the given number of frequencies {L}.\")\nelse:\n    print(f\"Nyquist-Shannon sampling theorem is violated for the given number of frequencies {L}.\")\n\n# Define the transformer embedding\n# max_positions controls the frequency scaling in transformer-style embeddings\nmax_positions = 1000\ntransformer_embedder = SinusoidalEmbedding(\n    in_channels=1,\n    num_frequencies=L,\n    embedding_type=\"transformer\",\n    max_positions=max_positions,\n).to(device)\n\n# Apply transformer-style embedding\ntransformer_embedding = transformer_embedder(x_normalized).permute(1, 0)\n\n# Define the NeRF embedding\nnerf_embedder = SinusoidalEmbedding(\n    in_channels=1, num_frequencies=L, embedding_type=\"nerf\"\n).to(device)\n\n# Apply NeRF-style embedding\nnerf_embedding = nerf_embedder(x_normalized).permute(1, 0)\n\n# Plot both embeddings\ncolors = plt.cm.tab10.colors\nfig, axes = plt.subplots(2, 1, figsize=(10, 9), sharex=True)\n\n## Transformer embedding\nfor freq_idx in range(L):\n    color = colors[freq_idx % len(colors)]\n    sin_idx, cos_idx = 2 * freq_idx, 2 * freq_idx + 1\n\n    axes[0].plot(x, transformer_embedding[sin_idx], color=color, label=f\"Channel {freq_idx + 1}\")\n    axes[0].plot(x, transformer_embedding[cos_idx], color=color)\n\naxes[0].set_title(\"Transformer embedding\", fontsize=18, pad=20)\naxes[0].set_ylabel(\"Embedding value\", fontsize=16)\naxes[0].legend(loc=\"lower left\", framealpha=1.0, fontsize=14)\naxes[0].locator_params(axis=\"y\", nbins=5)\n\n## NeRF embedding\nfor freq_idx in range(L):\n    color = colors[freq_idx % len(colors)]\n    sin_idx, cos_idx = 2 * freq_idx, 2 * freq_idx + 1\n\n    axes[1].plot(x, nerf_embedding[sin_idx], color=color, label=f\"Channel {freq_idx + 1}\")\n    axes[1].plot(x, nerf_embedding[cos_idx], color=color)\n\naxes[1].set_title(\"NeRF embedding\", fontsize=18, pad=20)\naxes[1].set_xlabel(\"x\", fontsize=16)\naxes[1].set_ylabel(\"Embedding value\", fontsize=16)\naxes[1].locator_params(axis=\"y\", nbins=5)\n\nplt.suptitle(\"Sinusoidal Embeddings using transformer and NeRF embedding types\", y=0.98, fontsize=20)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 2em;\"></div>\n\n### Encoding Constant Parameters with NeuralOp Class\nSimilar to the earlier illustrative examples, we can also encode\na scalar parameter $m$ before passing it to a model. Once again, care must be taken to\nensure that the Nyquist criterion is satisfied.\n\nIn the Transformer-style embedding, to avoid aliasing, the embedding frequencies should still\nsatisfy\n\n\\begin{align}f_{\\max} < f_{\\text{Nyquist}}.\\end{align}\n\nFor the NeRF-style embedding, the modified constraint becomes:\n\n\\begin{align}2^{L-1}m < \\frac{N}{2} \\implies L < 1 + \\log_2\\left(\\frac{N}{2m}\\right).\\end{align}\n\n.. raw:: html\n\n   <div style=\"margin-top: 2em;\"></div>\n\nBelow, we demonstrate an example of encoding the parameter $m = 2.5$ through frequency\nmodulation of the NeRF-style embedding.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define a spatial domain and the number of frequencies\nx = torch.linspace(0, 1, 1000)\nx_normalized = torch.linspace(0, 2 * torch.pi, len(x)).reshape(-1, 1)\nL = 3\n\n# Define the parameter to encode\nm = 2.5\nm_tensor = torch.tensor([m])\n\n# Check if the number of frequencies and parameter satisfies the Nyquist-Criterion\nif L <= 1 + torch.log2(torch.tensor(len(x_normalized)/2 * m)):\n    print(f\"Nyquist-Shannon sampling theorem is satisfied for the given parameter {m} and number of frequencies {L}.\")\nelse:\n    print(f\"Nyquist-Shannon sampling theorem is violated for the given parameter {m} and number of frequencies {L}.\")\n\n# Define the NeRF embedding\nnerf_embedder = SinusoidalEmbedding(\n    in_channels=1, num_frequencies=L, embedding_type=\"nerf\"\n).to(device)\n\n# Apply frequency modulation: multiply input by parameter before embedding\n# This scales all frequencies by the parameter m\nnerf_embedding = nerf_embedder(x_normalized * m_tensor).permute(1, 0)\n\n# Plot the embedding\ncolors = plt.cm.tab10.colors\n\nplt.figure(figsize=(10, 5))\nfor freq_idx in range(L):\n    color = colors[freq_idx % len(colors)]\n    sin_idx = 2 * freq_idx\n    cos_idx = 2 * freq_idx + 1\n\n    plt.plot(x, nerf_embedding[sin_idx], color=color, label=f\"Channel {freq_idx}\")\n    plt.plot(x, nerf_embedding[cos_idx], color=color)\n\nplt.xlabel(\"x\", fontsize=16)\nplt.ylabel(\"Embedding\", fontsize=16)\nplt.title(\"NeRF-style embedding with modulated frequency\", fontsize=18, pad=20)\nplt.legend(loc=\"lower left\", framealpha=1.0, fontsize=14)\nplt.locator_params(axis=\"y\", nbins=5)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly, we can encode the parameter $m = 2.5$ through amplitude modulation, where we show\nan example using the NeRF-style embedding below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define a spatial domain and the number of frequencies\nx = torch.linspace(0, 1, 1000)\nx_normalized = torch.linspace(0, 2 * torch.pi, len(x)).reshape(-1, 1)\nL = 3\n\n# Define the parameter to encode\nm = 2.5\nm_tensor = torch.tensor([m])\n\n# Check if the number of frequencies and parameter satisfies the Nyquist-Criterion\nif L <= 1 + torch.log2(torch.tensor(len(x_normalized)/2)):\n    print(f\"Nyquist-Shannon sampling theorem is satisfied for the given number of frequencies {L}.\")\nelse:\n    print(f\"Nyquist-Shannon sampling theorem is violated for the given number of frequencies {L}.\")\n\n# Define the embedding\nnerf_embedder = SinusoidalEmbedding(\n    in_channels=1, num_frequencies=L, embedding_type=\"nerf\"\n).to(device)\n\n# Apply amplitude modulation: multiply embedding by parameter after computation\n# This scales all embedding components by the parameter m\nnerf_embedding = nerf_embedder(x_normalized).permute(1, 0) * m_tensor\n\n# Plot the embedding\ncolors = plt.cm.tab10.colors\n\nplt.figure(figsize=(10, 5))\nfor freq_idx in range(L):\n    color = colors[freq_idx % len(colors)]\n    sin_idx = 2 * freq_idx\n    cos_idx = 2 * freq_idx + 1\n\n    plt.plot(x, nerf_embedding[sin_idx], color=color, label=f\"Channel {freq_idx}\")\n    plt.plot(x, nerf_embedding[cos_idx], color=color)\n\nplt.xlabel(\"x\", fontsize=16)\nplt.ylabel(\"Embedding\", fontsize=16)\nplt.title(\"NeRF-style embedding with amplitude modulation\", fontsize=18, pad=20)\nplt.legend(loc=\"lower left\", framealpha=1.0, fontsize=14)\nplt.locator_params(axis=\"y\", nbins=5)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 4em;\"></div>\n\n## Application to Fourier Neural Operators (FNOs)\nFourier Neural Operators (FNOs) learn mappings between functions by operating in the frequency domain.\nThey use the Fourier transform to express data as combinations of sine and cosine components,\nenabling them to capture complex, multi-scale interactions across frequencies.\nGiven that sinusoidal embeddings also lift low-dimensional data into a richer spectral\nrepresentation, they complement FNOs naturally.\nThis synergy makes sinusoidal embeddings particularly effective for neural operator architectures.\n\nIn the general setting for neural operators, we strongly recommend choosing the number of frequencies\n$L$ such that the Nyquist-Criterion is not violated. This can be done by following the guidelines\nwe provided earlier for selecting $L$ in both transformer-style and NeRF-style embeddings.\n\nWhen dealing with FNOs with a specified number of Fourier modes, $\\text{n_modes}$, the\nhighest embedded frequency should ideally also remain below $\\text{n_modes}$,\nas higher frequencies will be zeroed out and not acted upon by the spectral convolution operation.\n\nFor the NeRF-style embedding, this condition leads to an explicit upper bound on $L$:\n\n\\begin{align}2^{L-1} < \\text{n_modes} \\ \\ \\implies \\ \\  L < 1 + \\log_2\\left(\\text{n_modes}\\right).\\end{align}\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 4em;\"></div>\n\n## Setup in Higher Dimensions\nLet $X \\in \\mathbb{R}^{d \\times N}$ denote a $d$-dimensional input domain consisting of\n$N$ discretised points, where each row $\\vec{x}_{i} \\in \\mathbb{R}^N$ corresponds to\nthe sampled coordinates along the $i$-th spatial or temporal dimension. Thus, each column\nof $X$ represents a single point $\\vec{x}_{:,j} \\in \\mathbb{R}^d$ in the\n$d$-dimensional domain.\n\nBuilding on the 1D embedding function $g$ introduced earlier, we define the\nmulti-dimensional embedding\n\n\\begin{align}h: \\mathbb{R}^{d \\times N} \\ \\  \\rightarrow \\ \\ \\mathbb{R}^{N \\times 2 L d}, \\quad h(X)=\\operatorname{concat}\\left(g\\left(\\vec{x}_1\\right), g\\left(\\vec{x}_2\\right), \\ldots, g\\left(\\vec{x}_d\\right)\\right),\\end{align}\n\nwhere each $\\vec{x}_i$ denotes the sampled domain along the $i$-th input dimension.\n\nThe multi-dimensional embedding function $h$ applies the 1D embedding function $g$\nindependently to each coordinate dimension and concatenates the resulting embeddings\nalong the feature axis. This approach allows the model to capture\nfrequency patterns along each dimension separately while maintaining the overall structure.\n\n.. raw:: html\n\n   <div style=\"margin-top: 2em;\"></div>\n\nBelow, we include an example of using the `SinusoidalEmbedding` class to construct NeRF-style\nembeddings for a 3D input.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define a 1D spatial domain and construct 3D input by repeating the normalized 1D domain\ndim = 3\nx_1d = torch.linspace(0, 1, 1000)\n# Normalize to [0, 2\u03c0] and add channel dimension\nx_normalized_1d = torch.linspace(0, 2 * torch.pi, x_1d.size(0), device=x_1d.device).unsqueeze(1)\n# Repeat for 3D input: shape (N, 3)\nx_normalized = x_normalized_1d.repeat(1, dim)\n\n# Define the number of frequencies\nL = 3\n\n# Check if the number of frequencies satisfies the Nyquist-Criterion\n# For multi-dimensional inputs, the constraint applies to each dimension independently\nif L <= 1 + torch.log2(torch.tensor(len(x_normalized)/2)):\n    print(f\"Nyquist-Shannon sampling theorem is satisfied for the given number of frequencies {L}.\")\nelse:\n    print(f\"Nyquist-Shannon sampling theorem is violated for the given number of frequencies {L}.\")\n\n# Define the transformer embedding\nmax_positions = 1000\ntransformer_embedder = SinusoidalEmbedding(\n    in_channels=3,\n    num_frequencies=L,\n    embedding_type=\"transformer\",\n    max_positions=max_positions,\n).to(device)\n\n# Apply transformer-style embedding\ntransformer_embedding = transformer_embedder(x_normalized).permute(1, 0)\n\n# Define the NeRF embedding\nnerf_embedder = SinusoidalEmbedding(\n    in_channels=dim, num_frequencies=L, embedding_type=\"nerf\"\n).to(device)\n\n# Apply NeRF-style embedding\nnerf_embedding = nerf_embedder(x_normalized).permute(1, 0)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}