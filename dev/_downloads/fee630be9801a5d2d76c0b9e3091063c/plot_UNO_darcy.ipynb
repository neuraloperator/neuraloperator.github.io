{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# U-NO on Darcy-Flow\n\nTraining a U-shaped Neural Operator (U-NO) on the small Darcy-Flow example we ship with the package.\n\nThis tutorial demonstrates the U-NO architecture, which combines the resolution invariance\nof neural operators with the multi-scale feature extraction of U-Net architectures.\nThe U-NO uses skip connections and multi-resolution processing to capture both local\nand global features in the data, making it particularly effective for complex PDE problems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## Import dependencies\nWe import the necessary modules for working with the UNO model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport matplotlib.pyplot as plt\nimport sys\nfrom neuralop.models import UNO\nfrom neuralop import Trainer\nfrom neuralop.training import AdamW\nfrom neuralop.data.datasets import load_darcy_flow_small\nfrom neuralop.utils import count_model_params\nfrom neuralop import LpLoss, H1Loss\n\ndevice = \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## Loading the Darcy-Flow dataset\nWe load the Darcy-Flow dataset for training and testing.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_loader, test_loaders, data_processor = load_darcy_flow_small(\n    n_train=1000,\n    batch_size=32,\n    n_tests=[100, 50],\n    test_resolutions=[16, 32],\n    test_batch_sizes=[32, 32],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## Creating the U-NO model\nWe create a U-shaped Neural Operator with the following architecture:\n\n- in_channels: Number of input channels\n- out_channels: Number of output channels\n- hidden_channels: Width of the hidden layers\n- uno_out_channels: Channel dimensions for each layer in the U-Net structure\n- uno_n_modes: Fourier modes for each layer (decreasing then increasing)\n- uno_scalings: Scaling factors for each layer\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = UNO(\n    in_channels=1,\n    out_channels=1,\n    hidden_channels=64,\n    projection_channels=64,\n    uno_out_channels=[32, 64, 64, 64, 32],\n    uno_n_modes=[[8, 8], [8, 8], [4, 4], [8, 8], [8, 8]],\n    uno_scalings=[[1.0, 1.0], [0.5, 0.5], [1, 1], [2, 2], [1, 1]],\n    horizontal_skips_map=None,\n    channel_mlp_skip=\"linear\",\n    n_layers=5,\n)\n\nmodel = model.to(device)\n\n# Count and display the number of parameters\nn_params = count_model_params(model)\nprint(f\"\\nOur model has {n_params} parameters.\")\nsys.stdout.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## Creating the optimizer and scheduler\nWe use AdamW optimizer with weight decay for regularization\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=8e-3, weight_decay=1e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## Setting up loss functions\nWe use H1 loss for training and L2 loss for evaluation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "l2loss = LpLoss(d=2, p=2)\nh1loss = H1Loss(d=2)\n\ntrain_loss = h1loss\neval_losses = {\"h1\": h1loss, \"l2\": l2loss}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## Displaying configuration\nWe print the model architecture, optimizer, scheduler, and loss functions\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n### MODEL ###\\n\", model)\nprint(\"\\n### OPTIMIZER ###\\n\", optimizer)\nprint(\"\\n### SCHEDULER ###\\n\", scheduler)\nprint(\"\\n### LOSSES ###\")\nprint(f\"\\n * Train: {train_loss}\")\nprint(f\"\\n * Test: {eval_losses}\")\nsys.stdout.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## Creating the trainer\nWe create a Trainer object that handles the training loop for the U-NO\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n    model=model,\n    n_epochs=30,\n    device=device,\n    data_processor=data_processor,\n    wandb_log=False,  # Disable Weights & Biases logging\n    eval_interval=5,  # Evaluate every 5 epochs\n    use_distributed=False,  # Single GPU/CPU training\n    verbose=True,\n)  # Print training progress"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## Training the U-NO model\nWe train the model on our Darcy-Flow dataset. The trainer will:\n\n1. Run the forward pass through the U-NO\n2. Compute the H1 loss\n3. Backpropagate and update weights\n4. Evaluate on test data every 5 epochs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer.train(\n    train_loader=train_loader,\n    test_loaders=test_loaders,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    regularizer=False,\n    training_loss=train_loss,\n    eval_losses=eval_losses,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## Visualizing U-NO predictions\nWe visualize the model's predictions on the Darcy-Flow dataset.\nNote that we trained on a very small resolution for a very small number of epochs.\nIn practice, we would train at larger resolution on many more samples.\n\nHowever, for practicality, we created a minimal example that:\ni) fits in just a few MB of memory\nii) can be trained quickly on CPU\n\nIn practice we would train a Neural Operator on one or multiple GPUs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "test_samples = test_loaders[32].dataset\n\nfig = plt.figure(figsize=(7, 7))\nfor index in range(3):\n    data = test_samples[index]\n    data = data_processor.preprocess(data, batched=False)\n    # Input x\n    x = data[\"x\"]\n    # Ground-truth\n    y = data[\"y\"]\n    # Model prediction: U-NO output\n    out = model(x.unsqueeze(0).to(device)).cpu()\n\n    # Plot input x\n    ax = fig.add_subplot(3, 3, index * 3 + 1)\n    ax.imshow(x[0], cmap=\"gray\")\n    if index == 0:\n        ax.set_title(\"Input x\")\n    plt.xticks([], [])\n    plt.yticks([], [])\n\n    # Plot ground-truth y\n    ax = fig.add_subplot(3, 3, index * 3 + 2)\n    ax.imshow(y.squeeze())\n    if index == 0:\n        ax.set_title(\"Ground-truth y\")\n    plt.xticks([], [])\n    plt.yticks([], [])\n\n    # Plot model prediction\n    ax = fig.add_subplot(3, 3, index * 3 + 3)\n    ax.imshow(out.squeeze().detach().numpy())\n    if index == 0:\n        ax.set_title(\"U-NO prediction\")\n    plt.xticks([], [])\n    plt.yticks([], [])\n\nfig.suptitle(\"U-NO predictions on 32x32 Darcy-Flow data\", y=0.98)\nplt.tight_layout()\nfig.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}