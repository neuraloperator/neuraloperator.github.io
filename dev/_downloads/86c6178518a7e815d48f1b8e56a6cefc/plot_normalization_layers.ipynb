{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Normalization Layers\n\nWe provide several normalization layers that can be used to stabilize and accelerate model training. These layers are designed to be dimension-agnostic, making them flexible for use in various neural operator models. \n\nIn this tutorial, we explore `InstanceNorm`, `BatchNorm`, and `AdaIN`, and visualize their effects on 1D data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first import the neuralop library and required dependencies.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom neuralop.layers.normalization_layers import InstanceNorm, BatchNorm, AdaIN\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Normalization with 1D Functions\nTo clearly see how these layers work, we create a synthetic dataset of 1D functions.\nOur batch will consist of 10 functions. The first 5 will be sine waves with high amplitude and positive vertical shift, while the next 5 will be cosine waves with low amplitude and negative vertical shift. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_samples = 10\nn_points = 100\nx = torch.linspace(0, 2 * torch.pi, n_points)\ndata = torch.zeros((n_samples, 1, n_points))\n\nfor i in range(n_samples):\n    if i < 5:\n        # Group 1: High amplitude, positive shift\n        amplitude = np.random.uniform(2.0, 3.0)\n        shift = np.random.uniform(1.0, 2.0)\n        frequency = np.random.uniform(2.0, 3.0)\n        data[i, 0, :] = amplitude * torch.sin(frequency * x) + shift\n    else:\n        # Group 2: Low amplitude, negative shift\n        amplitude = np.random.uniform(0.5, 1.0)\n        shift = np.random.uniform(-2.0, -1.0)\n        frequency = np.random.uniform(1.0, 2.0)\n        data[i, 0, :] = amplitude * torch.cos(frequency * x) + shift"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's visualize the original data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\nplt.title(\"Original Data\", fontsize=20)\nfor i in range(n_samples):\n    if i < 5:\n        plt.plot(x, data[i, 0, :], 'b-', label='High Amplitude Functions with Positive Shift' if i == 0 else \"\")\n    else:\n        plt.plot(x, data[i, 0, :], 'r-', label='Low Amplitude Functions with Negative Shift' if i == 5 else \"\")\nplt.legend(fontsize=20, loc='lower center', bbox_to_anchor=(0.5, -0.5), ncol=1)\nplt.xlabel(\"$x$\", fontsize=18)\nplt.ylabel(\"$f(x)$\", fontsize=18)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.locator_params(axis='y', nbins=5)\nplt.grid(True, axis='y')\nplt.subplots_adjust(bottom=0.2, top=0.9)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## InstanceNorm\n`InstanceNorm` normalizes each sample in the batch **independently**. This means it rescales each of the 10 functions to have a mean of 0 and a standard deviation of 1, regardless of the other functions in the batch. This is useful when the statistical properties of each sample are distinct and should be treated separately.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "instance_norm = InstanceNorm()\ndata_in = instance_norm(data)\n\nplt.figure(figsize=(12, 6))\nplt.title(\"After InstanceNorm\", fontsize=20)\nfor i in range(n_samples):\n    y_plot = data_in[i, 0, :].detach().numpy()\n    if i < 5:\n        plt.plot(x, y_plot, 'b-', label='High Amplitude Functions with Positive Shift' if i == 0 else \"\")\n    else:\n        plt.plot(x, y_plot, 'r-', label='Low Amplitude Functions with Negative Shift' if i == 5 else \"\")\nplt.legend(fontsize=20, loc='lower center', bbox_to_anchor=(0.5, -0.5), ncol=1)\nplt.xlabel(\"$x$\", fontsize=18)\nplt.ylabel(\"$f(x)$\", fontsize=18)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.locator_params(axis='y', nbins=5)\nplt.grid(True, axis='y')\nplt.subplots_adjust(bottom=0.2, top=0.9)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice how all functions are now perfectly scaled to the same range, centered around zero. The original differences in amplitude and shift between the functions have been completely removed by instance-wise normalization.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## BatchNorm\n`BatchNorm` normalizes the data **across the entire batch**. It computes a single mean and standard deviation for all 10 functions combined and uses these values to normalize all the data. This is the most common form of normalization and is effective when batch statistics are a good approximation of the overall data distribution.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We need to specify the number of dimensions and features for BatchNorm\nbatch_norm = BatchNorm(n_dim=1, num_features=1)\ndata_bn = batch_norm(data)\n\nplt.figure(figsize=(12, 6))\nplt.title(\"After BatchNorm\", fontsize=20)\nfor i in range(n_samples):\n    y_plot = data_bn[i, 0, :].detach().numpy()\n    if i < 5:\n        plt.plot(x, y_plot, 'b-', label='High Amplitude Functions with Positive Shift' if i == 0 else \"\")\n    else:\n        plt.plot(x, y_plot, 'r-', label='Low Amplitude Functions with Negative Shift' if i == 5 else \"\")\nplt.legend(fontsize=20, loc='lower center', bbox_to_anchor=(0.5, -0.5), ncol=1)\nplt.xlabel(\"$x$\", fontsize=18)\nplt.ylabel(\"$f(x)$\", fontsize=18)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.locator_params(axis='y', nbins=5)\nplt.grid(True, axis='y')\nplt.subplots_adjust(bottom=0.2, top=0.9)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With `BatchNorm`, the relative differences between the two groups of functions are preserved. The high-amplitude functions are still visibly distinct from the low-amplitude ones, but the entire batch is now centered around a mean of zero with a standard deviation of one.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## AdaIN (Adaptive Instance Normalization)\n`AdaIN` is a more advanced normalization that allows for \"style transfer.\" It first normalizes an input (like `InstanceNorm`) and then applies a new style (a scaling `weight` and a shifting `bias`) derived from an external embedding vector. This is powerful for models where we want to control output characteristics based on a conditioning signal.\n\nTo guarantee a clear and deterministic result for this tutorial, we define our own simple MLP. This MLP maps our chosen style embeddings directly to a desired `weight` and `bias`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "content_function = torch.sin(2 * x).unsqueeze(0).unsqueeze(0)  # (1, 1, 100)\n\n# A simple, predictable MLP that maps an embedding directly to a (weight, bias) pair\nclass ToyMLP(nn.Module):\n    def forward(self, embedding):\n        return embedding\n\n# Style 1: A simple change in amplitude and mean (weight=2.0, bias=1.0)\nstyle_embedding_1 = torch.tensor([2.0, 1.0])\n# Style 2: A more complex change: inverting phase, shrinking, and shifting down (weight=-0.7, bias=-0.5)\nstyle_embedding_2 = torch.tensor([-0.7, -0.5])\n\n# The AdaIN layer needs to know the embedding dimension and number of input channels.\n# We pass our toy MLP to have full control.\nadain = AdaIN(embed_dim=2, in_channels=1, mlp=ToyMLP())\n\n# Apply the first style\nadain.set_embedding(style_embedding_1)\noutput_1 = adain(content_function)\n\n# Apply the second style\nadain.set_embedding(style_embedding_2)\noutput_2 = adain(content_function)\n\nplt.figure(figsize=(12, 6))\nplt.title(\"AdaIN for Style Transfer\", fontsize=20)\nplt.plot(x, content_function.squeeze().numpy(), 'k--', label='Content Function')\nplt.plot(x, output_1.squeeze().detach().numpy(), 'g-', label='Style 1: Increased Amplitude, Positive Shift')\nplt.plot(x, output_2.squeeze().detach().numpy(), 'm-', label='Style 2: Inverted, Shrunk, Negative Shift')\nplt.legend(fontsize=20, loc='lower center', bbox_to_anchor=(0.5, -0.7), ncol=1)\nplt.xlabel(\"$x$\", fontsize=18)\nplt.ylabel(\"$f(x)$\", fontsize=18)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.locator_params(axis='y', nbins=5)\nplt.grid(True, axis='y')\nplt.subplots_adjust(bottom=0.35, top=0.9)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, the same content function is transformed into two very different outputs. **Style 1** produces a simple sinusoidal wave with larger amplitude and positive shift. **Style 2** produces a more complex transformation: the function is inverted (phase-shifted), its amplitude is reduced, and it's shifted downwards. This demonstrates how AdaIN can modulate network layer output in diverse ways based on the style embedding.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}