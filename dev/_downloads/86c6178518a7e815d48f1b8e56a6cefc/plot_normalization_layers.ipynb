{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Normalization Layers\n\nThis tutorial demonstrates normalization layers designed for neural operators.\nNormalization is crucial for:\n\n- Stabilizing training dynamics\n- Accelerating convergence\n- Improving generalization\n- Handling different data distributions\n\nThe tutorial covers `InstanceNorm`, `BatchNorm`, and `AdaIN` layers, which are\ndimension-agnostic and flexible for various neural operator architectures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## Import dependencies\nWe import the necessary modules for working with normalization layers\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom neuralop.layers.normalization_layers import InstanceNorm, BatchNorm, AdaIN\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## Understanding Normalization with 1D Functions\nTo clearly see how these layers work, we create a synthetic dataset of 1D functions.\nOur batch will consist of 10 functions. The first 5 will be sine waves with high amplitude and positive vertical shift, while the next 5 will be cosine waves with low amplitude and negative vertical shift.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_samples = 10\nn_points = 100\nx = torch.linspace(0, 2 * torch.pi, n_points)\ndata = torch.zeros((n_samples, 1, n_points))\n\n# Generate two groups of functions with different characteristics\nfor i in range(n_samples):\n    if i < 5:\n        # Group 1: High amplitude sine waves with positive vertical shift\n        amplitude = np.random.uniform(2.0, 3.0)\n        shift = np.random.uniform(1.0, 2.0)\n        frequency = np.random.uniform(2.0, 3.0)\n        data[i, 0, :] = amplitude * torch.sin(frequency * x) + shift\n    else:\n        # Group 2: Low amplitude cosine waves with negative vertical shift\n        amplitude = np.random.uniform(0.5, 1.0)\n        shift = np.random.uniform(-2.0, -1.0)\n        frequency = np.random.uniform(1.0, 2.0)\n        data[i, 0, :] = amplitude * torch.cos(frequency * x) + shift"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## Visualizing the original data\nWe plot the synthetic functions to see their different statistical properties.\nThe blue functions have high amplitude and positive shift, while the red functions\nhave low amplitude and negative shift.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\nplt.title(\"Original Data\", fontsize=20)\nfor i in range(n_samples):\n    if i < 5:\n        plt.plot(x, data[i, 0, :], \"b-\", label=\"High Amplitude Functions with Positive Shift\" if i == 0 else \"\")\n    else:\n        plt.plot(x, data[i, 0, :], \"r-\", label=\"Low Amplitude Functions with Negative Shift\" if i == 5 else \"\")\n\nplt.legend(fontsize=20, loc=\"lower center\", bbox_to_anchor=(0.5, -0.5), ncol=1)\nplt.xlabel(\"$x$\", fontsize=18)\nplt.ylabel(\"$f(x)$\", fontsize=18)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.locator_params(axis=\"y\", nbins=5)\nplt.grid(True, axis=\"y\")\nplt.subplots_adjust(bottom=0.2, top=0.9)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## InstanceNorm\n`InstanceNorm` normalizes each sample in the batch **independently**. This means it rescales each of the 10 functions to have a mean of 0 and a standard deviation of 1, regardless of the other functions in the batch. This is useful when the statistical properties of each sample are distinct and should be treated separately.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Apply instance normalization\ninstance_norm = InstanceNorm()\ndata_in = instance_norm(data)\n\nplt.figure(figsize=(12, 6))\nplt.title(\"After InstanceNorm\", fontsize=20)\nfor i in range(n_samples):\n    y_plot = data_in[i, 0, :].detach().numpy()\n    if i < 5:\n        plt.plot(x, y_plot, \"b-\", label=\"High Amplitude Functions with Positive Shift\" if i == 0 else \"\")\n    else:\n        plt.plot(x, y_plot, \"r-\", label=\"Low Amplitude Functions with Negative Shift\" if i == 5 else \"\")\nplt.legend(fontsize=20, loc=\"lower center\", bbox_to_anchor=(0.5, -0.5), ncol=1)\nplt.xlabel(\"$x$\", fontsize=18)\nplt.ylabel(\"$f(x)$\", fontsize=18)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.locator_params(axis=\"y\", nbins=5)\nplt.grid(True, axis=\"y\")\nplt.subplots_adjust(bottom=0.2, top=0.9)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\nNotice how all functions are now perfectly scaled to the same range, centered around zero. The original differences in amplitude and shift between the functions have been completely removed by instance-wise normalization.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## BatchNorm\n`BatchNorm` normalizes the data **across the entire batch**. It computes a single mean and standard deviation for all 10 functions combined and uses these values to normalize all the data. This is the most common form of normalization and is effective when batch statistics are a good approximation of the overall data distribution.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We need to specify the number of dimensions and features for BatchNorm\nbatch_norm = BatchNorm(n_dim=1, num_features=1)\ndata_bn = batch_norm(data)\n\nplt.figure(figsize=(12, 6))\nplt.title(\"After BatchNorm\", fontsize=20)\nfor i in range(n_samples):\n    y_plot = data_bn[i, 0, :].detach().numpy()\n    if i < 5:\n        plt.plot(x, y_plot, \"b-\", label=\"High Amplitude Functions with Positive Shift\" if i == 0 else \"\")\n    else:\n        plt.plot(x, y_plot, \"r-\", label=\"Low Amplitude Functions with Negative Shift\" if i == 5 else \"\")\nplt.legend(fontsize=20, loc=\"lower center\", bbox_to_anchor=(0.5, -0.5), ncol=1)\nplt.xlabel(\"$x$\", fontsize=18)\nplt.ylabel(\"$f(x)$\", fontsize=18)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.locator_params(axis=\"y\", nbins=5)\nplt.grid(True, axis=\"y\")\nplt.subplots_adjust(bottom=0.2, top=0.9)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With `BatchNorm`, the relative differences between the two groups of functions are preserved. The high-amplitude functions are still visibly distinct from the low-amplitude ones, but the entire batch is now centered around a mean of zero with a standard deviation of one.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\n## AdaIN (Adaptive Instance Normalization)\n`AdaIN` is a more advanced normalization that allows for \"style transfer.\" It first normalizes an input (like `InstanceNorm`) and then applies a new style (a scaling `weight` and a shifting `bias`) derived from an external embedding vector. This is powerful for models where we want to control output characteristics based on a conditioning signal.\n\nTo guarantee a clear and deterministic result for this tutorial, we define our own simple MLP. This MLP maps our chosen style embeddings directly to a desired `weight` and `bias`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "content_function = torch.sin(2 * x).unsqueeze(0).unsqueeze(0)  # (1, 1, 100)\n\n\n# A simple, predictable MLP that maps an embedding directly to a (weight, bias) pair\nclass ToyMLP(nn.Module):\n    def forward(self, embedding):\n        return embedding\n\n\n# Style 1: A simple change in amplitude and mean (weight=2.0, bias=1.0)\nstyle_embedding_1 = torch.tensor([2.0, 1.0])\n# Style 2: A more complex change: inverting phase, shrinking, and shifting down (weight=-0.7, bias=-0.5)\nstyle_embedding_2 = torch.tensor([-0.7, -0.5])\n\n# The AdaIN layer needs to know the embedding dimension and number of input channels.\n# We pass our toy MLP to have full control.\nadain = AdaIN(embed_dim=2, in_channels=1, mlp=ToyMLP())\n\n# Apply the first style\nadain.set_embedding(style_embedding_1)\noutput_1 = adain(content_function)\n\n# Apply the second style\nadain.set_embedding(style_embedding_2)\noutput_2 = adain(content_function)\n\nplt.figure(figsize=(12, 6))\nplt.title(\"AdaIN for Style Transfer\", fontsize=20)\nplt.plot(x, content_function.squeeze().numpy(), \"k--\", label=\"Content Function\")\nplt.plot(x, output_1.squeeze().detach().numpy(), \"g-\", label=\"Style 1: Increased Amplitude, Positive Shift\")\nplt.plot(x, output_2.squeeze().detach().numpy(), \"m-\", label=\"Style 2: Inverted, Shrunk, Negative Shift\")\nplt.legend(fontsize=20, loc=\"lower center\", bbox_to_anchor=(0.5, -0.7), ncol=1)\nplt.xlabel(\"$x$\", fontsize=18)\nplt.ylabel(\"$f(x)$\", fontsize=18)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.locator_params(axis=\"y\", nbins=5)\nplt.grid(True, axis=\"y\")\nplt.subplots_adjust(bottom=0.35, top=0.9)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n\n   <div style=\"margin-top: 3em;\"></div>\n\nAs you can see, the same content function is transformed into two very different outputs. **Style 1** produces a simple sinusoidal wave with larger amplitude and positive shift. **Style 2** produces a more complex transformation: the function is inverted (phase-shifted), its amplitude is reduced, and it's shifted downwards. This demonstrates how AdaIN can modulate network layer output in diverse ways based on the style embedding.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}