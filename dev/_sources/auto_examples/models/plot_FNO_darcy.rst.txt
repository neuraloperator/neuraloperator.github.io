
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/models/plot_FNO_darcy.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_models_plot_FNO_darcy.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_models_plot_FNO_darcy.py:


Training an FNO on Darcy-Flow
=============================

We train a Fourier Neural Operator (FNO) on our small :ref:`Darcy-Flow example <sphx_glr_auto_examples_data_plot_darcy_flow.py>`.

This tutorial demonstrates the complete workflow of training a neural operator:
1. Loading and preprocessing the Darcy-Flow dataset
2. Creating an FNO model architecture
3. Setting up training components (optimizer, scheduler, losses)
4. Training the model
5. Evaluating predictions and zero-shot super-resolution

Note that this dataset is much smaller than one we would use in practice. The small Darcy-flow is an example built to
be trained on a CPU in a few seconds, whereas normally we would train on one or multiple GPUs. 

The FNO's key advantage is its resolution invariance - it can make predictions at different resolutions
without retraining, which we will demonstrate in the zero-shot super-resolution section.

.. GENERATED FROM PYTHON SOURCE LINES 22-29

.. raw:: html

   <div style="margin-top: 3em;"></div>

Import dependencies
-------------------
We import the necessary modules from `neuralop` for training a Fourier Neural Operator

.. GENERATED FROM PYTHON SOURCE LINES 29-42

.. code-block:: Python


    import torch
    import matplotlib.pyplot as plt
    import sys
    from neuralop.models import FNO
    from neuralop import Trainer
    from neuralop.training import AdamW
    from neuralop.data.datasets import load_darcy_flow_small
    from neuralop.utils import count_model_params
    from neuralop import LpLoss, H1Loss

    device = 'cpu'








.. GENERATED FROM PYTHON SOURCE LINES 43-51

.. raw:: html

   <div style="margin-top: 3em;"></div>

Loading the Darcy-Flow dataset
------------------------------
We load the small Darcy-Flow dataset with multiple resolutions for training and testing.
The dataset contains permeability fields (input) and pressure fields (output).

.. GENERATED FROM PYTHON SOURCE LINES 51-60

.. code-block:: Python


    train_loader, test_loaders, data_processor = load_darcy_flow_small(
            n_train=1000, batch_size=32, 
            test_resolutions=[16, 32], n_tests=[100, 50],
            test_batch_sizes=[32, 32],
    )
    data_processor = data_processor.to(device)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Loading test db for resolution 16 with 100 samples 
    Loading test db for resolution 32 with 50 samples 




.. GENERATED FROM PYTHON SOURCE LINES 61-67

.. raw:: html

   <div style="margin-top: 3em;"></div>

Creating the FNO model
----------------------

.. GENERATED FROM PYTHON SOURCE LINES 67-81

.. code-block:: Python


    model = FNO(n_modes=(8, 8),
                 in_channels=1, 
                 out_channels=1,
                 hidden_channels=32, 
                 projection_channel_ratio=2)
    model = model.to(device)

    # Count and display the number of parameters
    n_params = count_model_params(model)
    print(f'\nOur model has {n_params} parameters.')
    sys.stdout.flush()






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    Our model has 340833 parameters.




.. GENERATED FROM PYTHON SOURCE LINES 82-89

.. raw:: html

   <div style="margin-top: 3em;"></div>

Creating the optimizer and scheduler
------------------------------------
We use AdamW optimizer with weight decay for regularization

.. GENERATED FROM PYTHON SOURCE LINES 89-94

.. code-block:: Python

    optimizer = AdamW(model.parameters(), 
                                    lr=8e-3, 
                                    weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)








.. GENERATED FROM PYTHON SOURCE LINES 95-103

.. raw:: html

   <div style="margin-top: 3em;"></div>

Setting up loss functions
-------------------------
We use H1 loss for training and L2 loss for evaluation
H1 loss is particularly good for PDE problems as it penalizes both function values and gradients

.. GENERATED FROM PYTHON SOURCE LINES 103-110

.. code-block:: Python

    l2loss = LpLoss(d=2, p=2)  # L2 loss for function values
    h1loss = H1Loss(d=2)       # H1 loss includes gradient information

    train_loss = h1loss
    eval_losses={'h1': h1loss, 'l2': l2loss}









.. GENERATED FROM PYTHON SOURCE LINES 111-118

.. raw:: html

   <div style="margin-top: 3em;"></div>

Training the model
---------------------
We display the training configuration and then train the model

.. GENERATED FROM PYTHON SOURCE LINES 118-127

.. code-block:: Python


    print('\n### MODEL ###\n', model)
    print('\n### OPTIMIZER ###\n', optimizer)
    print('\n### SCHEDULER ###\n', scheduler)
    print('\n### LOSSES ###')
    print(f'\n * Train: {train_loss}')
    print(f'\n * Test: {eval_losses}')
    sys.stdout.flush()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    ### MODEL ###
     FNO(
      (positional_embedding): GridEmbeddingND()
      (fno_blocks): FNOBlocks(
        (convs): ModuleList(
          (0-3): 4 x SpectralConv(
            (weight): DenseTensor(shape=torch.Size([32, 32, 8, 5]), rank=None)
          )
        )
        (fno_skips): ModuleList(
          (0-3): 4 x Flattened1dConv(
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
        )
        (channel_mlp): ModuleList(
          (0-3): 4 x ChannelMLP(
            (fcs): ModuleList(
              (0): Conv1d(32, 16, kernel_size=(1,), stride=(1,))
              (1): Conv1d(16, 32, kernel_size=(1,), stride=(1,))
            )
          )
        )
        (channel_mlp_skips): ModuleList(
          (0-3): 4 x SoftGating()
        )
      )
      (lifting): ChannelMLP(
        (fcs): ModuleList(
          (0): Conv1d(3, 64, kernel_size=(1,), stride=(1,))
          (1): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
        )
      )
      (projection): ChannelMLP(
        (fcs): ModuleList(
          (0): Conv1d(32, 64, kernel_size=(1,), stride=(1,))
          (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))
        )
      )
    )

    ### OPTIMIZER ###
     AdamW (
    Parameter Group 0
        betas: (0.9, 0.999)
        correct_bias: True
        eps: 1e-06
        initial_lr: 0.008
        lr: 0.008
        weight_decay: 0.0001
    )

    ### SCHEDULER ###
     <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7fdd77506d50>

    ### LOSSES ###

     * Train: <neuralop.losses.data_losses.H1Loss object at 0x7fdd77507b10>

     * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x7fdd77507b10>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x7fdd77507390>}




.. GENERATED FROM PYTHON SOURCE LINES 128-135

.. raw:: html

   <div style="margin-top: 3em;"></div>

Creating the trainer
--------------------
We create a Trainer object that handles the training loop, evaluation, and logging

.. GENERATED FROM PYTHON SOURCE LINES 135-143

.. code-block:: Python

    trainer = Trainer(model=model, n_epochs=20,
                      device=device,
                      data_processor=data_processor,
                      wandb_log=False,        # Disable Weights & Biases logging for this tutorial
                      eval_interval=3,       # Evaluate every 3 epochs
                      use_distributed=False,  # Single GPU/CPU training
                      verbose=True)          # Print training progress








.. GENERATED FROM PYTHON SOURCE LINES 144-155

.. raw:: html

   <div style="margin-top: 3em;"></div>

Training the model
------------------
We train the model on our Darcy-Flow dataset. The trainer will:
1. Run the forward pass through the FNO
2. Compute the H1 loss
3. Backpropagate and update weights
4. Evaluate on test data every 3 epochs

.. GENERATED FROM PYTHON SOURCE LINES 155-164

.. code-block:: Python


    trainer.train(train_loader=train_loader,
                  test_loaders=test_loaders,
                  optimizer=optimizer,
                  scheduler=scheduler, 
                  regularizer=False, 
                  training_loss=train_loss,
                  eval_losses=eval_losses)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Training on 1000 samples
    Testing on [50, 50] samples         on resolutions [16, 32].
    /opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.
      warnings.warn(warn_msg)
    /opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/torch/nn/modules/module.py:1786: UserWarning: FNO.forward() received unexpected keyword arguments: ['y']. These arguments will be ignored.
      return forward_call(*args, **kwargs)
    Raw outputs of shape torch.Size([32, 1, 16, 16])
    /home/runner/work/neuraloperator/neuraloperator/neuralop/training/trainer.py:536: UserWarning: H1Loss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
      loss += training_loss(out, **sample)
    [0] time=1.52, avg_loss=0.6187, train_err=19.3351
    /home/runner/work/neuraloperator/neuraloperator/neuralop/training/trainer.py:581: UserWarning: LpLoss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
      val_loss = loss(out, **sample)
    Eval: 16_h1=0.4499, 16_l2=0.2930, 32_h1=0.5270, 32_l2=0.3010
    [3] time=1.51, avg_loss=0.2391, train_err=7.4729
    Eval: 16_h1=0.2387, 16_l2=0.1591, 32_h1=0.3755, 32_l2=0.1767
    [6] time=1.50, avg_loss=0.1952, train_err=6.0990
    Eval: 16_h1=0.2325, 16_l2=0.1577, 32_h1=0.3526, 32_l2=0.1765
    [9] time=1.50, avg_loss=0.1784, train_err=5.5735
    Eval: 16_h1=0.1883, 16_l2=0.1127, 32_h1=0.3345, 32_l2=0.1466
    [12] time=1.54, avg_loss=0.1606, train_err=5.0201
    Eval: 16_h1=0.1828, 16_l2=0.1058, 32_h1=0.3339, 32_l2=0.1321
    [15] time=1.51, avg_loss=0.1578, train_err=4.9310
    Eval: 16_h1=0.1887, 16_l2=0.1100, 32_h1=0.3292, 32_l2=0.1341
    [18] time=1.52, avg_loss=0.1456, train_err=4.5506
    Eval: 16_h1=0.1774, 16_l2=0.0938, 32_h1=0.3155, 32_l2=0.1171

    {'train_err': 4.338394977152348, 'avg_loss': 0.13882863926887512, 'avg_lasso_loss': None, 'epoch_train_time': 1.529051332999984}



.. GENERATED FROM PYTHON SOURCE LINES 165-178

.. raw:: html

   <div style="margin-top: 3em;"></div>

.. _plot_preds :
Visualizing predictions
------------------------
Let's take a look at what our model's predicted outputs look like. 
We wll compare the inputs, ground-truth outputs, and model predictions side by side.

Note that in this example, we train on a very small resolution for
a very small number of epochs. In practice, we would train at a larger 
resolution on many more samples.

.. GENERATED FROM PYTHON SOURCE LINES 178-222

.. code-block:: Python


    test_samples = test_loaders[16].dataset

    fig = plt.figure(figsize=(7, 7))
    for index in range(3):
        data = test_samples[index]
        data = data_processor.preprocess(data, batched=False)
    
        # Input
        x = data['x']
        # Ground-truth output
        y = data['y']
        # Model prediction
        out = model(x.unsqueeze(0))

        # Plot input 
        ax = fig.add_subplot(3, 3, index*3 + 1)
        ax.imshow(x[0], cmap='gray')
        if index == 0: 
            ax.set_title('Input x')
        plt.xticks([], [])
        plt.yticks([], [])

        # Plot ground-truth output
        ax = fig.add_subplot(3, 3, index*3 + 2)
        ax.imshow(y.squeeze())
        if index == 0: 
            ax.set_title('Ground-truth output')
        plt.xticks([], [])
        plt.yticks([], [])

        # Plot model prediction
        ax = fig.add_subplot(3, 3, index*3 + 3)
        ax.imshow(out.squeeze().detach().numpy())
        if index == 0: 
            ax.set_title('Model prediction')
        plt.xticks([], [])
        plt.yticks([], [])

    fig.suptitle('FNO predictions on 16x16 Darcy-Flow data', y=0.98)
    plt.tight_layout()
    fig.show()





.. image-sg:: /auto_examples/models/images/sphx_glr_plot_FNO_darcy_001.png
   :alt: FNO predictions on 16x16 Darcy-Flow data, Input x, Ground-truth output, Model prediction
   :srcset: /auto_examples/models/images/sphx_glr_plot_FNO_darcy_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 223-234

.. raw:: html

   <div style="margin-top: 3em;"></div>

.. zero_shot :
Zero-shot super-resolution evaluation
-------------------------------------
One of the key advantages of neural operators is their resolution invariance.
The FNO's invariance to the discretization of input data means we can natively 
make predictions on higher-resolution inputs and get higher-resolution outputs
without retraining the model!

.. GENERATED FROM PYTHON SOURCE LINES 234-277

.. code-block:: Python


    test_samples = test_loaders[32].dataset

    fig = plt.figure(figsize=(7, 7))
    for index in range(3):
        data = test_samples[index]
        data = data_processor.preprocess(data, batched=False)
    
        # Input at higher-resolution
        x = data['x']
        # Ground-truth output at higher-resolution
        y = data['y']
        # Model prediction at higher-resolution
        out = model(x.unsqueeze(0))

        # Plot input at higher-resolution
        ax = fig.add_subplot(3, 3, index*3 + 1)
        ax.imshow(x[0], cmap='gray')
        if index == 0: 
            ax.set_title('Input at 32x32')
        plt.xticks([], [])
        plt.yticks([], [])

        # Plot ground-truth output at higher-resolution
        ax = fig.add_subplot(3, 3, index*3 + 2)
        ax.imshow(y.squeeze())
        if index == 0: 
            ax.set_title('Ground-truth at 32x32')
        plt.xticks([], [])
        plt.yticks([], [])

        # Plot model prediction at higher-resolution
        ax = fig.add_subplot(3, 3, index*3 + 3)
        ax.imshow(out.squeeze().detach().numpy())
        if index == 0: 
            ax.set_title('FNO prediction at 32x32')
        plt.xticks([], [])
        plt.yticks([], [])

    fig.suptitle('Zero-shot super-resolution: 16x16 → 32x32', y=0.98)
    plt.tight_layout()
    fig.show()




.. image-sg:: /auto_examples/models/images/sphx_glr_plot_FNO_darcy_002.png
   :alt: Zero-shot super-resolution: 16x16 → 32x32, Input at 32x32, Ground-truth at 32x32, FNO prediction at 32x32
   :srcset: /auto_examples/models/images/sphx_glr_plot_FNO_darcy_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 278-296

.. raw:: html

   <div style="margin-top: 3em;"></div>

Understanding zero-shot super-resolution
----------------------------------------
We only trained the model on data at a resolution of 16x16, and with no modifications 
or special prompting, we were able to perform inference on higher-resolution input data 
and get higher-resolution predictions! This is a powerful capability of neural operators.

In practice, we often want to evaluate neural operators at multiple resolutions to track 
a model's zero-shot super-resolution performance throughout training. That's why many of 
our datasets, including the small Darcy-flow we showcased, are parameterized with a list 
of `test_resolutions` to choose from. 

Note: These predictions may be noisier than we would expect for a model evaluated 
at the same resolution at which it was trained. This is because the model hasn't seen
the higher-frequency patterns present in the 32x32 data during training. However, this
demonstrates the fundamental resolution invariance of neural operators. 


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 34.367 seconds)


.. _sphx_glr_download_auto_examples_models_plot_FNO_darcy.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_FNO_darcy.ipynb <plot_FNO_darcy.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_FNO_darcy.py <plot_FNO_darcy.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_FNO_darcy.zip <plot_FNO_darcy.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
