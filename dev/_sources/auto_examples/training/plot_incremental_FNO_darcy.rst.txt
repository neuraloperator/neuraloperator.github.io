
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/training/plot_incremental_FNO_darcy.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_training_plot_incremental_FNO_darcy.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_training_plot_incremental_FNO_darcy.py:


Training an FNO with incremental meta-learning
===============================================
A demo of the Incremental FNO meta-learning algorithm on our small Darcy-Flow dataset.

This tutorial demonstrates incremental meta-learning for neural operators, which allows
the model to gradually increase its complexity during training. This approach can lead to:
- Better convergence properties
- More stable training dynamics
- Improved generalization
- Reduced computational requirements during early training

The incremental approach starts with a small number of Fourier modes and gradually
increases the model capacity as training progresses.

.. GENERATED FROM PYTHON SOURCE LINES 18-25

.. raw:: html

   <div style="margin-top: 3em;"></div>

Import dependencies
-------------------
We import the necessary modules for incremental FNO training

.. GENERATED FROM PYTHON SOURCE LINES 25-37

.. code-block:: Python


    import torch
    import matplotlib.pyplot as plt
    import sys
    from neuralop.models import FNO
    from neuralop.data.datasets import load_darcy_flow_small
    from neuralop.utils import count_model_params
    from neuralop.training import AdamW
    from neuralop.training.incremental import IncrementalFNOTrainer
    from neuralop.data.transforms.data_processors import IncrementalDataProcessor
    from neuralop import LpLoss, H1Loss








.. GENERATED FROM PYTHON SOURCE LINES 38-45

.. raw:: html

   <div style="margin-top: 3em;"></div>

Loading the Darcy-Flow dataset
------------------------------
We load the Darcy-Flow dataset with multiple resolutions for incremental training.

.. GENERATED FROM PYTHON SOURCE LINES 45-56

.. code-block:: Python


    train_loader, test_loaders, output_encoder = load_darcy_flow_small(
        n_train=100,
        batch_size=16,
        test_resolutions=[16, 32],
        n_tests=[100, 50],
        test_batch_sizes=[32, 32],
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Loading test db for resolution 16 with 100 samples 
    Loading test db for resolution 32 with 50 samples 




.. GENERATED FROM PYTHON SOURCE LINES 57-66

.. raw:: html

   <div style="margin-top: 3em;"></div>

Configuring incremental training
--------------------------------
We set up the incremental FNO model with a small starting number of modes.
The model will gradually increase its capacity during training.
We choose to update the modes using the incremental gradient explained algorithm

.. GENERATED FROM PYTHON SOURCE LINES 66-72

.. code-block:: Python

    incremental = True
    if incremental:
        starting_modes = (2, 2)  # Start with very few modes
    else:
        starting_modes = (16, 16)  # Standard number of modes








.. GENERATED FROM PYTHON SOURCE LINES 73-81

.. raw:: html

   <div style="margin-top: 3em;"></div>

Creating the incremental FNO model
----------------------------------
We create an FNO model with a maximum number of modes that can be reached
during incremental training. The model starts with fewer modes and grows.

.. GENERATED FROM PYTHON SOURCE LINES 81-91

.. code-block:: Python

    model = FNO(
        max_n_modes=(16, 16),    # Maximum modes the model can reach
        n_modes=starting_modes,  # Starting number of modes
        hidden_channels=32,
        in_channels=1,
        out_channels=1,
    )
    model = model.to(device)
    n_params = count_model_params(model)








.. GENERATED FROM PYTHON SOURCE LINES 92-99

.. raw:: html

   <div style="margin-top: 3em;"></div>

Setting up the optimizer and scheduler
-------------------------------------
We use AdamW optimizer with weight decay for regularization

.. GENERATED FROM PYTHON SOURCE LINES 99-102

.. code-block:: Python

    optimizer = AdamW(model.parameters(), lr=8e-3, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)








.. GENERATED FROM PYTHON SOURCE LINES 103-121

.. raw:: html

   <div style="margin-top: 3em;"></div>

Configuring incremental data processing
---------------------------------------
If one wants to use Incremental Resolution, one should use the IncrementalDataProcessor.
When passed to the trainer, the trainer will automatically update the resolution.

Key parameters for incremental resolution:
- incremental_resolution: bool, default is False
  if True, increase the resolution of the input incrementally
- incremental_res_gap: parameter for resolution updates
- subsampling_rates: a list of resolutions to use
- dataset_indices: a list of indices of the dataset to slice to regularize the input resolution
- dataset_resolution: the resolution of the input
- epoch_gap: the number of epochs to wait before increasing the resolution
- verbose: if True, print the resolution and the number of modes

.. GENERATED FROM PYTHON SOURCE LINES 121-134

.. code-block:: Python


    data_transform = IncrementalDataProcessor(
        in_normalizer=None,
        out_normalizer=None,
        device=device,
        subsampling_rates=[2, 1],      # Resolution scaling factors
        dataset_resolution=16,          # Base resolution
        dataset_indices=[2, 3],        # Dataset indices for regularization
        epoch_gap=10,                  # Epochs between resolution updates
        verbose=True,                  # Print progress information
    )

    data_transform = data_transform.to(device)




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Original Incre Res: change index to 0
    Original Incre Res: change sub to 2
    Original Incre Res: change res to 8




.. GENERATED FROM PYTHON SOURCE LINES 135-142

.. raw:: html

   <div style="margin-top: 3em;"></div>

Setting up loss functions
-------------------------
We use H1 loss for training and L2 loss for evaluation

.. GENERATED FROM PYTHON SOURCE LINES 142-147

.. code-block:: Python

    l2loss = LpLoss(d=2, p=2)  
    h1loss = H1Loss(d=2)       
    train_loss = h1loss
    eval_losses = {"h1": h1loss, "l2": l2loss}








.. GENERATED FROM PYTHON SOURCE LINES 148-156

.. raw:: html

   <div style="margin-top: 3em;"></div>

Displaying training configuration
---------------------------------
We display the model parameters, optimizer, scheduler, and loss functions
to verify our incremental training setup

.. GENERATED FROM PYTHON SOURCE LINES 156-165

.. code-block:: Python

    print("\n### N PARAMS ###\n", n_params)
    print("\n### OPTIMIZER ###\n", optimizer)
    print("\n### SCHEDULER ###\n", scheduler)
    print("\n### LOSSES ###")
    print("\n### INCREMENTAL RESOLUTION + GRADIENT EXPLAINED ###")
    print(f"\n * Train: {train_loss}")
    print(f"\n * Test: {eval_losses}")
    sys.stdout.flush()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    ### N PARAMS ###
     2110305

    ### OPTIMIZER ###
     AdamW (
    Parameter Group 0
        betas: (0.9, 0.999)
        correct_bias: True
        eps: 1e-06
        initial_lr: 0.008
        lr: 0.008
        weight_decay: 0.0001
    )

    ### SCHEDULER ###
     <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f10aac8c9d0>

    ### LOSSES ###

    ### INCREMENTAL RESOLUTION + GRADIENT EXPLAINED ###

     * Train: <neuralop.losses.data_losses.H1Loss object at 0x7f1084cfca50>

     * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x7f1084cfca50>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x7f10aac8cd60>}




.. GENERATED FROM PYTHON SOURCE LINES 166-189

.. raw:: html

   <div style="margin-top: 3em;"></div>

Configuring the IncrementalFNOTrainer
--------------------------------------
We set up the IncrementalFNOTrainer with various incremental learning options.
Other options include setting incremental_loss_gap = True.
If one wants to use incremental resolution, set it to True.
In this example we only update the modes and not the resolution.
When using incremental resolution, keep in mind that the number of modes 
initially set should be strictly less than the resolution.

Key parameters for incremental training:
- incremental_grad: bool, default is False
  if True, use the base incremental algorithm based on gradient variance
  - incremental_grad_eps: threshold for gradient variance
  - incremental_buffer: number of buffer modes to calculate gradient variance
  - incremental_max_iter: initial number of iterations
  - incremental_grad_max_iter: maximum iterations to accumulate gradients
- incremental_loss_gap: bool, default is False
  if True, use the incremental algorithm based on loss gap
  - incremental_loss_eps: threshold for loss gap

.. GENERATED FROM PYTHON SOURCE LINES 189-206

.. code-block:: Python


    # Create the IncrementalFNOTrainer with our configuration
    trainer = IncrementalFNOTrainer(
        model=model,
        n_epochs=20,
        data_processor=data_transform,
        device=device,
        verbose=True,
        incremental_loss_gap=False,     # Use gradient-based incremental learning
        incremental_grad=True,          # Enable gradient-based mode updates
        incremental_grad_eps=0.9999,    # Gradient variance threshold
        incremental_loss_eps=0.001,     # Loss gap threshold
        incremental_buffer=5,           # Buffer modes for gradient calculation
        incremental_max_iter=1,         # Initial iterations
        incremental_grad_max_iter=2,    # Maximum gradient accumulation iterations
    )








.. GENERATED FROM PYTHON SOURCE LINES 207-218

.. raw:: html

   <div style="margin-top: 3em;"></div>

Training the incremental FNO model
----------------------------------
We train the model using incremental meta-learning. The trainer will:
1. Start with a small number of Fourier modes
2. Gradually increase the model capacity based on gradient variance
3. Monitor the incremental learning progress
4. Evaluate on test data throughout training

.. GENERATED FROM PYTHON SOURCE LINES 218-229

.. code-block:: Python


    trainer.train(
        train_loader,
        test_loaders,
        optimizer,
        scheduler,
        regularizer=False,
        training_loss=train_loss,
        eval_losses=eval_losses,
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Training on 100 samples
    Testing on [50, 50] samples         on resolutions [16, 32].
    /opt/hostedtoolcache/Python/3.13.8/x64/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.
      warnings.warn(warn_msg)
    /opt/hostedtoolcache/Python/3.13.8/x64/lib/python3.13/site-packages/torch/nn/modules/module.py:1786: UserWarning: FNO.forward() received unexpected keyword arguments: ['y']. These arguments will be ignored.
      return forward_call(*args, **kwargs)
    Raw outputs of shape torch.Size([16, 1, 8, 8])
    /home/runner/work/neuraloperator/neuraloperator/neuralop/training/trainer.py:507: UserWarning: H1Loss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
      loss += training_loss(out, **sample)
    [0] time=0.33, avg_loss=0.9243, train_err=13.2048
    /home/runner/work/neuraloperator/neuraloperator/neuralop/training/trainer.py:557: UserWarning: LpLoss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
      val_loss = loss(out, **sample)
    Eval: 16_h1=0.9534, 16_l2=0.8806, 32_h1=1.1958, 32_l2=0.9061
    [1] time=0.31, avg_loss=0.7827, train_err=11.1808
    Eval: 16_h1=0.8564, 16_l2=0.6700, 32_h1=0.8839, 32_l2=0.6700
    [2] time=0.31, avg_loss=0.8049, train_err=11.4980
    /home/runner/work/neuraloperator/neuraloperator/neuralop/training/incremental.py:160: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
    Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)
      incremental_modes - self.incremental_buffer, torch.Tensor(strength_vector))
    Eval: 16_h1=0.9575, 16_l2=0.7642, 32_h1=1.2390, 32_l2=0.7657
    [3] time=0.31, avg_loss=0.7089, train_err=10.1266
    Eval: 16_h1=0.7859, 16_l2=0.6639, 32_h1=0.9917, 32_l2=0.6876
    [4] time=0.31, avg_loss=0.6143, train_err=8.7757
    Eval: 16_h1=0.8288, 16_l2=0.4462, 32_h1=1.1422, 32_l2=0.4906
    [5] time=0.32, avg_loss=0.5798, train_err=8.2822
    Eval: 16_h1=1.0814, 16_l2=0.6752, 32_h1=1.3831, 32_l2=0.6993
    [6] time=0.32, avg_loss=0.6898, train_err=9.8549
    Eval: 16_h1=0.7418, 16_l2=0.4494, 32_h1=0.8665, 32_l2=0.4705
    [7] time=0.32, avg_loss=0.5636, train_err=8.0517
    Eval: 16_h1=0.6759, 16_l2=0.3575, 32_h1=0.9154, 32_l2=0.3875
    [8] time=0.32, avg_loss=0.4536, train_err=6.4801
    Eval: 16_h1=0.7670, 16_l2=0.4425, 32_h1=1.0364, 32_l2=0.4801
    [9] time=0.32, avg_loss=0.5155, train_err=7.3637
    Eval: 16_h1=0.6233, 16_l2=0.3333, 32_h1=0.9005, 32_l2=0.3704
    Incre Res Update: change index to 1
    Incre Res Update: change sub to 1
    Incre Res Update: change res to 16
    [10] time=0.40, avg_loss=0.5669, train_err=8.0993
    Eval: 16_h1=0.5299, 16_l2=0.3396, 32_h1=0.6401, 32_l2=0.3353
    [11] time=0.39, avg_loss=0.4987, train_err=7.1238
    Eval: 16_h1=0.5249, 16_l2=0.3015, 32_h1=0.6580, 32_l2=0.3139
    [12] time=0.39, avg_loss=0.4790, train_err=6.8430
    Eval: 16_h1=0.4389, 16_l2=0.2627, 32_h1=0.5762, 32_l2=0.2702
    [13] time=0.39, avg_loss=0.4104, train_err=5.8630
    Eval: 16_h1=0.4197, 16_l2=0.2524, 32_h1=0.5390, 32_l2=0.2599
    [14] time=0.39, avg_loss=0.3792, train_err=5.4168
    Eval: 16_h1=0.4713, 16_l2=0.2681, 32_h1=0.5947, 32_l2=0.2830
    [15] time=0.39, avg_loss=0.4189, train_err=5.9842
    Eval: 16_h1=0.3960, 16_l2=0.2346, 32_h1=0.5202, 32_l2=0.2376
    [16] time=0.40, avg_loss=0.3652, train_err=5.2176
    Eval: 16_h1=0.3709, 16_l2=0.2278, 32_h1=0.5125, 32_l2=0.2413
    [17] time=0.39, avg_loss=0.3380, train_err=4.8286
    Eval: 16_h1=0.4387, 16_l2=0.2623, 32_h1=0.5748, 32_l2=0.2667
    [18] time=0.39, avg_loss=0.3902, train_err=5.5743
    Eval: 16_h1=0.3759, 16_l2=0.2355, 32_h1=0.5155, 32_l2=0.2516
    [19] time=0.39, avg_loss=0.3373, train_err=4.8183
    Eval: 16_h1=0.3395, 16_l2=0.2149, 32_h1=0.4755, 32_l2=0.2298

    {'train_err': 4.818341153008597, 'avg_loss': 0.3372838807106018, 'avg_lasso_loss': None, 'epoch_train_time': 0.3891734550001047, '16_h1': tensor(0.3395), '16_l2': tensor(0.2149), '32_h1': tensor(0.4755), '32_l2': tensor(0.2298)}



.. GENERATED FROM PYTHON SOURCE LINES 230-245

.. raw:: html

   <div style="margin-top: 3em;"></div>

Visualizing incremental FNO predictions
----------------------------------------
We visualize the model's predictions after incremental training.
Note that we trained on a very small resolution for a very small number of epochs.
In practice, we would train at larger resolution on many more samples.

However, for practicality, we created a minimal example that:
i) fits in just a few MB of memory
ii) can be trained quickly on CPU

In practice we would train a Neural Operator on one or multiple GPUs

.. GENERATED FROM PYTHON SOURCE LINES 245-287

.. code-block:: Python


    test_samples = test_loaders[32].dataset

    fig = plt.figure(figsize=(7, 7))
    for index in range(3):
        data = test_samples[index]
        # Input x
        x = data["x"].to(device)
        # Ground-truth
        y = data["y"].to(device)
        # Model prediction: incremental FNO output
        out = model(x.unsqueeze(0))
    
        # Plot input x
        ax = fig.add_subplot(3, 3, index * 3 + 1)
        x = x.cpu().squeeze().detach().numpy()
        y = y.cpu().squeeze().detach().numpy()
        ax.imshow(x, cmap="gray")
        if index == 0:
            ax.set_title("Input x")
        plt.xticks([], [])
        plt.yticks([], [])

        # Plot ground-truth y
        ax = fig.add_subplot(3, 3, index * 3 + 2)
        ax.imshow(y.squeeze())
        if index == 0:
            ax.set_title("Ground-truth y")
        plt.xticks([], [])
        plt.yticks([], [])

        # Plot model prediction
        ax = fig.add_subplot(3, 3, index * 3 + 3)
        ax.imshow(out.cpu().squeeze().detach().numpy())
        if index == 0:
            ax.set_title("Incremental FNO prediction")
        plt.xticks([], [])
        plt.yticks([], [])

    fig.suptitle("Incremental FNO predictions on Darcy-Flow data", y=0.98)
    plt.tight_layout()
    fig.show()



.. image-sg:: /auto_examples/training/images/sphx_glr_plot_incremental_FNO_darcy_001.png
   :alt: Incremental FNO predictions on Darcy-Flow data, Input x, Ground-truth y, Incremental FNO prediction
   :srcset: /auto_examples/training/images/sphx_glr_plot_incremental_FNO_darcy_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 12.945 seconds)


.. _sphx_glr_download_auto_examples_training_plot_incremental_FNO_darcy.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_incremental_FNO_darcy.ipynb <plot_incremental_FNO_darcy.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_incremental_FNO_darcy.py <plot_incremental_FNO_darcy.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_incremental_FNO_darcy.zip <plot_incremental_FNO_darcy.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
