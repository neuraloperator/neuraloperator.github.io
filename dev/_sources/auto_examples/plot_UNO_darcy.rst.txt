
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_UNO_darcy.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_UNO_darcy.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_UNO_darcy.py:


U-NO on Darcy-Flow
==================

In this example, we demonstrate how to train a U-shaped Neural Operator on 
the small Darcy-Flow example we ship with the package

.. GENERATED FROM PYTHON SOURCE LINES 11-26

.. code-block:: Python



    import torch
    import matplotlib.pyplot as plt
    import sys
    from neuralop.models import UNO
    from neuralop import Trainer
    from neuralop.training import AdamW
    from neuralop.data.datasets import load_darcy_flow_small
    from neuralop.utils import count_model_params
    from neuralop import LpLoss, H1Loss

    device = 'cpu'









.. GENERATED FROM PYTHON SOURCE LINES 27-28

Loading the Darcy Flow dataset

.. GENERATED FROM PYTHON SOURCE LINES 28-53

.. code-block:: Python

    train_loader, test_loaders, data_processor = load_darcy_flow_small(
            n_train=1000, batch_size=32, 
            test_resolutions=[16, 32], n_tests=[100, 50],
            test_batch_sizes=[32, 32],
    )

    model = UNO(in_channels=1, 
                out_channels=1, 
                hidden_channels=64, 
                projection_channels=64,
                uno_out_channels=[32,64,64,64,32],
                uno_n_modes=[[16,16],[8,8],[8,8],[8,8],[16,16]],
                uno_scalings=[[1.0,1.0],[0.5,0.5],[1,1],[2,2],[1,1]],
                horizontal_skips_map=None,
                channel_mlp_skip="linear",
                n_layers = 5,
                domain_padding=0.2)

    model = model.to(device)

    n_params = count_model_params(model)
    print(f'\nOur model has {n_params} parameters.')
    sys.stdout.flush()






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/runner/work/neuraloperator/neuraloperator/neuralop/data/datasets/pt_dataset.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
      data = torch.load(
    Loading test db for resolution 16 with 100 samples 
    /home/runner/work/neuraloperator/neuraloperator/neuralop/data/datasets/pt_dataset.py:172: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
      data = torch.load(Path(root_dir).joinpath(f"{dataset_name}_test_{res}.pt").as_posix())
    Loading test db for resolution 32 with 50 samples 
    fno_skip='linear'
    channel_mlp_skip='linear'
    fno_skip='linear'
    channel_mlp_skip='linear'
    fno_skip='linear'
    channel_mlp_skip='linear'
    fno_skip='linear'
    channel_mlp_skip='linear'
    fno_skip='linear'
    channel_mlp_skip='linear'

    Our model has 2700097 parameters.




.. GENERATED FROM PYTHON SOURCE LINES 54-55

Create the optimizer

.. GENERATED FROM PYTHON SOURCE LINES 55-61

.. code-block:: Python

    optimizer = AdamW(model.parameters(), 
                                    lr=8e-3, 
                                    weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)









.. GENERATED FROM PYTHON SOURCE LINES 62-63

Creating the losses

.. GENERATED FROM PYTHON SOURCE LINES 63-70

.. code-block:: Python

    l2loss = LpLoss(d=2, p=2)
    h1loss = H1Loss(d=2)

    train_loss = h1loss
    eval_losses={'h1': h1loss, 'l2': l2loss}









.. GENERATED FROM PYTHON SOURCE LINES 71-82

.. code-block:: Python



    print('\n### MODEL ###\n', model)
    print('\n### OPTIMIZER ###\n', optimizer)
    print('\n### SCHEDULER ###\n', scheduler)
    print('\n### LOSSES ###')
    print(f'\n * Train: {train_loss}')
    print(f'\n * Test: {eval_losses}')
    sys.stdout.flush()






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    ### MODEL ###
     UNO(
      (positional_embedding): GridEmbeddingND()
      (domain_padding): DomainPadding()
      (lifting): ChannelMLP(
        (fcs): ModuleList(
          (0): Conv1d(3, 256, kernel_size=(1,), stride=(1,))
          (1): Conv1d(256, 64, kernel_size=(1,), stride=(1,))
        )
      )
      (fno_blocks): ModuleList(
        (0): FNOBlocks(
          (convs): ModuleList(
            (0): SpectralConv(
              (weight): DenseTensor(shape=torch.Size([64, 32, 16, 9]), rank=None)
            )
          )
          (fno_skips): ModuleList(
            (0): Flattened1dConv(
              (conv): Conv1d(64, 32, kernel_size=(1,), stride=(1,), bias=False)
            )
          )
          (channel_mlp): ModuleList(
            (0): ChannelMLP(
              (fcs): ModuleList(
                (0): Conv1d(32, 16, kernel_size=(1,), stride=(1,))
                (1): Conv1d(16, 32, kernel_size=(1,), stride=(1,))
              )
            )
          )
          (channel_mlp_skips): ModuleList(
            (0): Flattened1dConv(
              (conv): Conv1d(64, 32, kernel_size=(1,), stride=(1,), bias=False)
            )
          )
        )
        (1): FNOBlocks(
          (convs): ModuleList(
            (0): SpectralConv(
              (weight): DenseTensor(shape=torch.Size([32, 64, 8, 5]), rank=None)
            )
          )
          (fno_skips): ModuleList(
            (0): Flattened1dConv(
              (conv): Conv1d(32, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
          )
          (channel_mlp): ModuleList(
            (0): ChannelMLP(
              (fcs): ModuleList(
                (0): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
                (1): Conv1d(32, 64, kernel_size=(1,), stride=(1,))
              )
            )
          )
          (channel_mlp_skips): ModuleList(
            (0): Flattened1dConv(
              (conv): Conv1d(32, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
          )
        )
        (2): FNOBlocks(
          (convs): ModuleList(
            (0): SpectralConv(
              (weight): DenseTensor(shape=torch.Size([64, 64, 8, 5]), rank=None)
            )
          )
          (fno_skips): ModuleList(
            (0): Flattened1dConv(
              (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
          )
          (channel_mlp): ModuleList(
            (0): ChannelMLP(
              (fcs): ModuleList(
                (0): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
                (1): Conv1d(32, 64, kernel_size=(1,), stride=(1,))
              )
            )
          )
          (channel_mlp_skips): ModuleList(
            (0): Flattened1dConv(
              (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
          )
        )
        (3): FNOBlocks(
          (convs): ModuleList(
            (0): SpectralConv(
              (weight): DenseTensor(shape=torch.Size([128, 64, 8, 5]), rank=None)
            )
          )
          (fno_skips): ModuleList(
            (0): Flattened1dConv(
              (conv): Conv1d(128, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
          )
          (channel_mlp): ModuleList(
            (0): ChannelMLP(
              (fcs): ModuleList(
                (0): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
                (1): Conv1d(32, 64, kernel_size=(1,), stride=(1,))
              )
            )
          )
          (channel_mlp_skips): ModuleList(
            (0): Flattened1dConv(
              (conv): Conv1d(128, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
          )
        )
        (4): FNOBlocks(
          (convs): ModuleList(
            (0): SpectralConv(
              (weight): DenseTensor(shape=torch.Size([96, 32, 16, 9]), rank=None)
            )
          )
          (fno_skips): ModuleList(
            (0): Flattened1dConv(
              (conv): Conv1d(96, 32, kernel_size=(1,), stride=(1,), bias=False)
            )
          )
          (channel_mlp): ModuleList(
            (0): ChannelMLP(
              (fcs): ModuleList(
                (0): Conv1d(32, 16, kernel_size=(1,), stride=(1,))
                (1): Conv1d(16, 32, kernel_size=(1,), stride=(1,))
              )
            )
          )
          (channel_mlp_skips): ModuleList(
            (0): Flattened1dConv(
              (conv): Conv1d(96, 32, kernel_size=(1,), stride=(1,), bias=False)
            )
          )
        )
      )
      (horizontal_skips): ModuleDict(
        (0): Flattened1dConv(
          (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
        )
        (1): Flattened1dConv(
          (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (projection): ChannelMLP(
        (fcs): ModuleList(
          (0): Conv1d(32, 64, kernel_size=(1,), stride=(1,))
          (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))
        )
      )
    )

    ### OPTIMIZER ###
     AdamW (
    Parameter Group 0
        betas: (0.9, 0.999)
        correct_bias: True
        eps: 1e-06
        initial_lr: 0.008
        lr: 0.008
        weight_decay: 0.0001
    )

    ### SCHEDULER ###
     <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f0da937ae10>

    ### LOSSES ###

     * Train: <neuralop.losses.data_losses.H1Loss object at 0x7f0da9502690>

     * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x7f0da9502690>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x7f0da9501250>}




.. GENERATED FROM PYTHON SOURCE LINES 83-84

Create the trainer

.. GENERATED FROM PYTHON SOURCE LINES 84-94

.. code-block:: Python

    trainer = Trainer(model=model,
                       n_epochs=20,
                      device=device,
                      data_processor=data_processor,
                      wandb_log=False,
                      eval_interval=3,
                      use_distributed=False,
                      verbose=True)









.. GENERATED FROM PYTHON SOURCE LINES 95-96

Actually train the model on our small Darcy-Flow dataset

.. GENERATED FROM PYTHON SOURCE LINES 96-106

.. code-block:: Python


    trainer.train(train_loader=train_loader,
                  test_loaders=test_loaders,
                  optimizer=optimizer,
                  scheduler=scheduler, 
                  regularizer=False, 
                  training_loss=train_loss,
                  eval_losses=eval_losses)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Training on 1000 samples
    Testing on [50, 50] samples         on resolutions [16, 32].
    Raw outputs of shape torch.Size([32, 1, 16, 16])
    [0] time=10.36, avg_loss=0.6517, train_err=20.3653
    Eval: 16_h1=0.4303, 16_l2=0.2800, 32_h1=0.9207, 32_l2=0.5716
    [3] time=10.18, avg_loss=0.2396, train_err=7.4866
    Eval: 16_h1=0.2667, 16_l2=0.1748, 32_h1=0.8210, 32_l2=0.6055
    [6] time=10.25, avg_loss=0.2391, train_err=7.4718
    Eval: 16_h1=0.3624, 16_l2=0.2583, 32_h1=0.7937, 32_l2=0.5680
    [9] time=10.25, avg_loss=0.1887, train_err=5.8984
    Eval: 16_h1=0.2744, 16_l2=0.1737, 32_h1=0.7806, 32_l2=0.5113
    [12] time=10.30, avg_loss=0.1818, train_err=5.6809
    Eval: 16_h1=0.2776, 16_l2=0.1787, 32_h1=0.7827, 32_l2=0.5253
    [15] time=10.21, avg_loss=0.1681, train_err=5.2536
    Eval: 16_h1=0.2401, 16_l2=0.1450, 32_h1=0.7742, 32_l2=0.4820
    [18] time=10.22, avg_loss=0.1580, train_err=4.9365
    Eval: 16_h1=0.2594, 16_l2=0.1664, 32_h1=0.7755, 32_l2=0.4988

    {'train_err': 4.288203448057175, 'avg_loss': 0.1372225103378296, 'avg_lasso_loss': None, 'epoch_train_time': 10.219220160999953}



.. GENERATED FROM PYTHON SOURCE LINES 107-117

Plot the prediction, and compare with the ground-truth 
Note that we trained on a very small resolution for
a very small number of epochs
In practice, we would train at larger resolution, on many more samples.

However, for practicity, we created a minimal example that
i) fits in just a few Mb of memory
ii) can be trained quickly on CPU

In practice we would train a Neural Operator on one or multiple GPUs

.. GENERATED FROM PYTHON SOURCE LINES 117-155

.. code-block:: Python


    test_samples = test_loaders[32].dataset

    fig = plt.figure(figsize=(7, 7))
    for index in range(3):
        data = test_samples[index]
        data = data_processor.preprocess(data, batched=False)
        # Input x
        x = data['x']
        # Ground-truth
        y = data['y']
        # Model prediction
        out = model(x.unsqueeze(0).to(device)).cpu()

        ax = fig.add_subplot(3, 3, index*3 + 1)
        ax.imshow(x[0], cmap='gray')
        if index == 0: 
            ax.set_title('Input x')
        plt.xticks([], [])
        plt.yticks([], [])

        ax = fig.add_subplot(3, 3, index*3 + 2)
        ax.imshow(y.squeeze())
        if index == 0: 
            ax.set_title('Ground-truth y')
        plt.xticks([], [])
        plt.yticks([], [])

        ax = fig.add_subplot(3, 3, index*3 + 3)
        ax.imshow(out.squeeze().detach().numpy())
        if index == 0: 
            ax.set_title('Model prediction')
        plt.xticks([], [])
        plt.yticks([], [])

    fig.suptitle('Inputs, ground-truth output and prediction.', y=0.98)
    plt.tight_layout()
    fig.show()



.. image-sg:: /auto_examples/images/sphx_glr_plot_UNO_darcy_001.png
   :alt: Inputs, ground-truth output and prediction., Input x, Ground-truth y, Model prediction
   :srcset: /auto_examples/images/sphx_glr_plot_UNO_darcy_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (3 minutes 28.077 seconds)


.. _sphx_glr_download_auto_examples_plot_UNO_darcy.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_UNO_darcy.ipynb <plot_UNO_darcy.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_UNO_darcy.py <plot_UNO_darcy.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_UNO_darcy.zip <plot_UNO_darcy.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
