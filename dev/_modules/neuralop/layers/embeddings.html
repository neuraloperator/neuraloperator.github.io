<!DOCTYPE html>

<html lang="en" data-content_root="../../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>neuralop.layers.embeddings &#8212; neuraloperator 1.0.2 documentation</title> 
<link rel="stylesheet" href="../../../_static/tensorly_style.css">
<link rel="apple-touch-icon" sizes="180x180" href="../../../_static/favicon/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../../_static/favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="../../../_static/favicon/favicon-16x16.png">
<link rel="manifest" href="../../../_static/favicon/site.webmanifest">
<link rel="mask-icon" href="../../../_static/favicon/safari-pinned-tab.svg" color="#5bbad5">
<link rel="shortcut icon" href="../../../_static/favicon/favicon.ico">
<meta name="theme-color" content="#ffffff">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tensorly_style.css?v=a02e9698" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />

  
    <script src="../../../_static/documentation_options.js?v=1ed6394b"></script>
    <script src="../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
 <script src="../../../_static/navbar_burger.js"></script>
 <script defer src="https://use.fontawesome.com/releases/v5.14.0/js/all.js"></script>
 
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

  </head>
<body  class="has-navbar-fixed-top">

  <header>
    <navbar>
      <nav class="navbar top-navbar is-fixed-top has-shadow is-flex-wrap-wrap" role="navigation" aria-label="main top navigation">
        <div class="navbar-brand">
        

          <a class="navbar-item" href="../../../index.html">
            <img src="../../../_static/neuraloperator_logo.png" height="28">
          </a>
          <a class="navbar-item is-hidden-desktop" href="https://github.com/neuraloperator/neuraloperator" target="_blank">
              <span class="icon"><i class="fab fa-github"></i></span>
          </a>

          <a role="button" class="navbar-burger" data-target="top-nav-menu" aria-label="menu" aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
          </a>

        </div>
        
        <div class="navbar-menu" id="top-nav-menu">
        

          <div class="navbar-start">
            
              <a class="navbar-item" href="../../../install.html">
              Install
            </a>
              <a class="navbar-item" href="../../../theory_guide/index.html">
              Theory Guide
            </a>
              <a class="navbar-item" href="../../../user_guide/index.html">
              User Guide
            </a>
              <a class="navbar-item" href="../../../modules/api.html">
              API
            </a>
              <a class="navbar-item" href="../../../auto_examples/index.html">
              Examples
            </a>
              <a class="navbar-item" href="../../../dev_guide/index.html">
              Developer's Guide
            </a>
          </div>
        
          <div class="navbar-end">
            <div class="navbar-item">
            
            <a class="button is-hidden-touch is-dark" href="https://github.com/neuraloperator/neuraloperator" target="_blank">
              <span class="icon-text">
                <span class="icon is-large">
                  <i class="fab fa-github"></i>
                </span>
                <span>Github</span>
              </span>
            </a>

            </div> 
          </div> 
        </div> 

      </nav>
      
    </navbar>
  </header>


  <div id="column-container">
  <div class="columns is-mobile is-centered">
	
  

  <div class="column main-column">

    
    <div class="main-section">

      
      

      <div class="container content main-content">
        
  <h1>Source code for neuralop.layers.embeddings</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">abc</span><span class="w"> </span><span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">List</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>


<span class="k">class</span><span class="w"> </span><span class="nc">Embedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">ABC</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="nd">@property</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">out_channels</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>


<div class="viewcode-block" id="GridEmbedding2D">
<a class="viewcode-back" href="../../../modules/generated/neuralop.layers.embeddings.GridEmbedding2D.html#neuralop.layers.embeddings.GridEmbedding2D">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">GridEmbedding2D</span><span class="p">(</span><span class="n">Embedding</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    GridEmbedding2D applies a simple positional</span>
<span class="sd">    embedding as a regular 2D grid. Expects inputs of shape</span>
<span class="sd">    (batch, channels, d_1, d_2)</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    in_channels : int</span>
<span class="sd">        number of channels in input. Fixed for output channel interface</span>
<span class="sd">    grid_boundaries : list, optional</span>
<span class="sd">        coordinate boundaries of input grid, by default [[0, 1], [0, 1]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">grid_boundaries</span><span class="o">=</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">in_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grid_boundaries</span> <span class="o">=</span> <span class="n">grid_boundaries</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_grid</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_res</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">out_channels</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">+</span> <span class="mi">2</span>

<div class="viewcode-block" id="GridEmbedding2D.grid">
<a class="viewcode-back" href="../../../modules/generated/neuralop.layers.embeddings.GridEmbedding2D.html#neuralop.layers.embeddings.GridEmbedding2D.grid">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">grid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spatial_dims</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;grid generates 2D grid needed for pos encoding</span>
<span class="sd">        and caches the grid associated with MRU resolution</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        spatial_dims : torch.size</span>
<span class="sd">             sizes of spatial resolution</span>
<span class="sd">        device : literal &#39;cpu&#39; or &#39;cuda:*&#39;</span>
<span class="sd">            where to load data</span>
<span class="sd">        dtype : str</span>
<span class="sd">            dtype to encode data</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.tensor</span>
<span class="sd">            output grids to concatenate</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># handle case of multiple train resolutions</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grid</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_res</span> <span class="o">!=</span> <span class="n">spatial_dims</span><span class="p">:</span>
            <span class="n">grid_x</span><span class="p">,</span> <span class="n">grid_y</span> <span class="o">=</span> <span class="n">regular_grid_2d</span><span class="p">(</span>
                <span class="n">spatial_dims</span><span class="p">,</span> <span class="n">grid_boundaries</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">grid_boundaries</span>
            <span class="p">)</span>
            <span class="n">grid_x</span> <span class="o">=</span> <span class="n">grid_x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">grid_y</span> <span class="o">=</span> <span class="n">grid_y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_grid</span> <span class="o">=</span> <span class="n">grid_x</span><span class="p">,</span> <span class="n">grid_y</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_res</span> <span class="o">=</span> <span class="n">spatial_dims</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grid</span></div>


<div class="viewcode-block" id="GridEmbedding2D.forward">
<a class="viewcode-back" href="../../../modules/generated/neuralop.layers.embeddings.GridEmbedding2D.html#neuralop.layers.embeddings.GridEmbedding2D.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">batched</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:],</span> <span class="n">data</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
            <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span>
            <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># in the unbatched case, the dataloader will stack N</span>
        <span class="c1"># examples with no batch dim to create one</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">batched</span> <span class="ow">and</span> <span class="n">batch_size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">out</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">out</span></div>
</div>



<div class="viewcode-block" id="GridEmbeddingND">
<a class="viewcode-back" href="../../../modules/generated/neuralop.layers.embeddings.GridEmbeddingND.html#neuralop.layers.embeddings.GridEmbeddingND">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">GridEmbeddingND</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    GridEmbeddingND applies a simple positional</span>
<span class="sd">    embedding as a regular ND grid. Expects inputs of shape</span>
<span class="sd">    (batch, channels, d_1, ..., d_n).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    in_channels : int</span>
<span class="sd">        number of channels in input</span>
<span class="sd">    dim : int</span>
<span class="sd">        dimensions of positional encoding to apply</span>
<span class="sd">    grid_boundaries : list, optional</span>
<span class="sd">        coordinate boundaries of input grid along each dim, by default [[0, 1], [0, 1]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">grid_boundaries</span><span class="o">=</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">in_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span>
            <span class="n">grid_boundaries</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Error: expected grid_boundaries to be an iterable of length </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="si">}</span><span class="s2">, received </span><span class="si">{</span><span class="n">grid_boundaries</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grid_boundaries</span> <span class="o">=</span> <span class="n">grid_boundaries</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_grid</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_res</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">out_channels</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span>

<div class="viewcode-block" id="GridEmbeddingND.grid">
<a class="viewcode-back" href="../../../modules/generated/neuralop.layers.embeddings.GridEmbeddingND.html#neuralop.layers.embeddings.GridEmbeddingND.grid">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">grid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spatial_dims</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;grid generates ND grid needed for pos encoding</span>
<span class="sd">        and caches the grid associated with MRU resolution</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        spatial_dims : torch.Size</span>
<span class="sd">             sizes of spatial resolution</span>
<span class="sd">        device : literal &#39;cpu&#39; or &#39;cuda:*&#39;</span>
<span class="sd">            where to load data</span>
<span class="sd">        dtype : str</span>
<span class="sd">            dtype to encode data</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.tensor</span>
<span class="sd">            output grids to concatenate</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># handle case of multiple train resolutions</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grid</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_res</span> <span class="o">!=</span> <span class="n">spatial_dims</span><span class="p">:</span>
            <span class="n">grids_by_dim</span> <span class="o">=</span> <span class="n">regular_grid_nd</span><span class="p">(</span><span class="n">spatial_dims</span><span class="p">,</span> <span class="n">grid_boundaries</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">grid_boundaries</span><span class="p">)</span>
            <span class="c1"># add batch, channel dims</span>
            <span class="n">grids_by_dim</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">grids_by_dim</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_grid</span> <span class="o">=</span> <span class="n">grids_by_dim</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_res</span> <span class="o">=</span> <span class="n">spatial_dims</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grid</span></div>


<div class="viewcode-block" id="GridEmbeddingND.forward">
<a class="viewcode-back" href="../../../modules/generated/neuralop.layers.embeddings.GridEmbeddingND.html#neuralop.layers.embeddings.GridEmbeddingND.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Params</span>
<span class="sd">        --------</span>
<span class="sd">        data: torch.Tensor</span>
<span class="sd">            assumes shape (batch (optional), channels, x_1, x_2, ...x_n)</span>
<span class="sd">        batched: bool</span>
<span class="sd">            whether data has a batch dim</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># add batch dim if it doesn&#39;t exist</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">batched</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">grids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">spatial_dims</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:],</span> <span class="n">device</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">grids</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">*</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">grids</span><span class="p">]</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="n">grids</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span></div>
</div>



<div class="viewcode-block" id="SinusoidalEmbedding">
<a class="viewcode-back" href="../../../modules/generated/neuralop.layers.embeddings.SinusoidalEmbedding.html#neuralop.layers.embeddings.SinusoidalEmbedding">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">SinusoidalEmbedding</span><span class="p">(</span><span class="n">Embedding</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sinusoidal positional embedding for enriching coordinate inputs with spectral information [1]_, [2]_.</span>

<span class="sd">    This class provides sinusoidal positional embeddings in two styles: Transformer-style</span>
<span class="sd">    and NeRF-style. It lifts low-dimensional coordinates into a richer spectral representation</span>
<span class="sd">    by encoding them as periodic functions (sines and cosines) at multiple frequencies.</span>

<span class="sd">    The embedding enhances a model&#39;s ability to capture fine-scale variations and high-frequency</span>
<span class="sd">    dynamics by providing a hierarchy of frequency components alongside the original coordinates.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    in_channels : int</span>
<span class="sd">        Number of input channels to embed (dimensionality of input coordinates)</span>
<span class="sd">    num_freqs : int, optional</span>
<span class="sd">        Number of frequency levels L in the embedding. Each level contributes</span>
<span class="sd">        a sine and cosine pair, resulting in 2L output channels per input channel.</span>
<span class="sd">        By default, set to the number of input channels.</span>
<span class="sd">    embedding_type : {&#39;transformer&#39;, &#39;nerf&#39;}, optional</span>
<span class="sd">        Type of embedding to apply, by default &#39;transformer&#39;</span>

<span class="sd">        Transformer-style [1]_:</span>
<span class="sd">        For each input coordinate p and frequency level k (0 ≤ k &lt; L):</span>
<span class="sd">        - g(p)_{2k} = sin(p / max_positions^{k/L})</span>
<span class="sd">        - g(p)_{2k+1} = cos(p / max_positions^{k/L})</span>

<span class="sd">        NeRF-style [2]_:</span>
<span class="sd">        For each input coordinate p and frequency level k (0 ≤ k &lt; L):</span>
<span class="sd">        - g(p)_{2k} = sin(2^k * π * p)</span>
<span class="sd">        - g(p)_{2k+1} = cos(2^k * π * p)</span>

<span class="sd">    max_positions : int, optional</span>
<span class="sd">        Maximum number of positions for transformer-style encoding, by default 10000.</span>
<span class="sd">        Only used when embedding_type=&#39;transformer&#39;.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    - Input shape: (batch, n_in, in_channels) or (n_in, in_channels)</span>
<span class="sd">    - Output shape: (batch, n_in, 2*num_freqs*in_channels) or (n_in, 2*num_freqs*in_channels)</span>
<span class="sd">    - Ensure the highest frequency satisfies the Nyquist criterion:</span>
<span class="sd">      - Transformer: f_max &lt; N/2 where N is the number of sampling points</span>
<span class="sd">      - NeRF: 2^{L-1} &lt; N/2, i.e., L &lt; 1 + log₂(N/2)</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    See `examples/layers/plot_sinusoidal_embeddings.py` for comprehensive visualizations</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] Vaswani, A. et al. &quot;Attention Is All You Need&quot;.</span>
<span class="sd">           NeurIPS 2017, https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</span>

<span class="sd">    .. [2] Mildenhall, B. et al. &quot;NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis&quot;.</span>
<span class="sd">           ArXiv 2020, https://arxiv.org/pdf/2003.08934</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_frequencies</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">embedding_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;transformer&quot;</span><span class="p">,</span>
        <span class="n">max_positions</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">in_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_frequencies</span> <span class="o">=</span> <span class="n">num_frequencies</span>

        <span class="c1"># verify embedding type</span>
        <span class="n">allowed_embeddings</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;nerf&quot;</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">]</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">embedding_type</span> <span class="ow">in</span> <span class="n">allowed_embeddings</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Error: embedding_type expected one of </span><span class="si">{</span><span class="n">allowed_embeddings</span><span class="si">}</span><span class="s2">, received </span><span class="si">{</span><span class="n">embedding_type</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_type</span> <span class="o">=</span> <span class="n">embedding_type</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_type</span> <span class="o">==</span> <span class="s2">&quot;transformer&quot;</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">max_positions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="p">),</span> <span class="s2">&quot;Error: max_positions must have an int value for </span><span class="se">\</span>
<span class="s2">                transformer embedding.&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_positions</span> <span class="o">=</span> <span class="n">max_positions</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">out_channels</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        required property for linking/composing model layers</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_frequencies</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span>

<div class="viewcode-block" id="SinusoidalEmbedding.forward">
<a class="viewcode-back" href="../../../modules/generated/neuralop.layers.embeddings.SinusoidalEmbedding.html#neuralop.layers.embeddings.SinusoidalEmbedding.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        -----------</span>
<span class="sd">        x: torch.Tensor</span>
<span class="sd">            shape (n_in, self.in_channels) or (batch, n_in, self.in_channels)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;Error: expected inputs of shape (batch, n_in, </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span><span class="si">}</span><span class="s2">)</span><span class="se">\</span>
<span class="s2">            or (n_in, channels), got inputs with ndim=</span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">, shape=</span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">batched</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batched</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_in</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_type</span> <span class="o">==</span> <span class="s2">&quot;nerf&quot;</span><span class="p">:</span>
            <span class="n">freqs</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_frequencies</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">pi</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_type</span> <span class="o">==</span> <span class="s2">&quot;transformer&quot;</span><span class="p">:</span>
            <span class="n">freqs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_frequencies</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_frequencies</span> <span class="o">*</span> <span class="mi">2</span>
            <span class="n">freqs</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_positions</span><span class="p">)</span> <span class="o">**</span> <span class="n">freqs</span>

        <span class="c1"># outer product of wavenumbers and position coordinates</span>
        <span class="c1"># shape b, n_in * channels, len(freqs)</span>
        <span class="n">freqs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bij, k -&gt; bijk&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">freqs</span><span class="p">)</span>

        <span class="c1"># shape len(x), 2, len(freqs)</span>
        <span class="n">freqs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">freqs</span><span class="o">.</span><span class="n">sin</span><span class="p">(),</span> <span class="n">freqs</span><span class="o">.</span><span class="n">cos</span><span class="p">()),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># transpose the inner per-entry matrix and ravel to interleave sin and cos</span>
        <span class="n">freqs</span> <span class="o">=</span> <span class="n">freqs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_in</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">batched</span><span class="p">:</span>
            <span class="n">freqs</span> <span class="o">=</span> <span class="n">freqs</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">freqs</span></div>
</div>



<span class="k">class</span><span class="w"> </span><span class="nc">RotaryEmbedding2D</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">min_freq</span><span class="o">=</span><span class="mi">1</span> <span class="o">/</span> <span class="mi">64</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Applying rotary positional embedding (https://arxiv.org/abs/2104.09864) to the input feature tensor.</span>
<span class="sd">        The crux is the dot product of two rotation matrices R(theta1) and R(theta2) is equal to R(theta2 - theta1).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">inv_freq</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">10000</span> <span class="o">**</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">/</span> <span class="n">dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_freq</span> <span class="o">=</span> <span class="n">min_freq</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;inv_freq&quot;</span><span class="p">,</span> <span class="n">inv_freq</span><span class="p">,</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="mi">2</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">coordinates</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;coordinates is tensor of [batch_size, num_points]&quot;&quot;&quot;</span>
        <span class="n">coordinates</span> <span class="o">=</span> <span class="n">coordinates</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_freq</span><span class="p">)</span>
        <span class="n">freqs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;... i , j -&gt; ... i j&quot;</span><span class="p">,</span> <span class="n">coordinates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inv_freq</span><span class="p">)</span>  <span class="c1"># [b, n, d//2]</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">freqs</span><span class="p">,</span> <span class="n">freqs</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [b, n, d]</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">apply_1d_rotary_pos_emb</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">freqs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">apply_rotary_pos_emb</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">freqs</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">apply_2d_rotary_pos_emb</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">freqs_x</span><span class="p">,</span> <span class="n">freqs_y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Split the last dimension of features into two equal halves</span>
<span class="sd">        and apply 1d rotary positional embedding to each half.&quot;&quot;&quot;</span>
        <span class="n">d</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">t_x</span><span class="p">,</span> <span class="n">t_y</span> <span class="o">=</span> <span class="n">t</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span> <span class="n">d</span> <span class="o">//</span> <span class="mi">2</span><span class="p">],</span> <span class="n">t</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">d</span> <span class="o">//</span> <span class="mi">2</span> <span class="p">:]</span>

        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
            <span class="p">(</span><span class="n">apply_rotary_pos_emb</span><span class="p">(</span><span class="n">t_x</span><span class="p">,</span> <span class="n">freqs_x</span><span class="p">),</span> <span class="n">apply_rotary_pos_emb</span><span class="p">(</span><span class="n">t_y</span><span class="p">,</span> <span class="n">freqs_y</span><span class="p">)),</span>
            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>


<span class="c1"># Utility functions for GridEmbedding</span>
<span class="k">def</span><span class="w"> </span><span class="nf">regular_grid_2d</span><span class="p">(</span><span class="n">spatial_dims</span><span class="p">,</span> <span class="n">grid_boundaries</span><span class="o">=</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a 2 x height x width stack of positional encodings A, where</span>
<span class="sd">    A[:,i,j] = [[x,y]] at coordinate (i,j) on a (height, width) grid.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">spatial_dims</span>

    <span class="n">xt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">grid_boundaries</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">grid_boundaries</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">height</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">yt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">grid_boundaries</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">grid_boundaries</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">width</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">grid_x</span><span class="p">,</span> <span class="n">grid_y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">yt</span><span class="p">,</span> <span class="n">indexing</span><span class="o">=</span><span class="s2">&quot;ij&quot;</span><span class="p">)</span>

    <span class="n">grid_x</span> <span class="o">=</span> <span class="n">grid_x</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">grid_y</span> <span class="o">=</span> <span class="n">grid_y</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">grid_x</span><span class="p">,</span> <span class="n">grid_y</span>


<span class="k">def</span><span class="w"> </span><span class="nf">regular_grid_nd</span><span class="p">(</span>
    <span class="n">resolutions</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">grid_boundaries</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span> <span class="o">*</span> <span class="mi">2</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;regular_grid_nd generates a tensor of coordinate points that</span>
<span class="sd">    describe a bounded regular grid.</span>

<span class="sd">    Creates a dim x res_d1 x ... x res_dn stack of positional encodings A, where</span>
<span class="sd">    A[:,c1,c2,...] = [[d1,d2,...dn]] at coordinate (c1,c2,...cn) on a (res_d1, ...res_dn) grid.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    resolutions : List[int]</span>
<span class="sd">        resolution of the output grid along each dimension</span>
<span class="sd">    grid_boundaries : List[List[int]], optional</span>
<span class="sd">        List of pairs [start, end] of the boundaries of the</span>
<span class="sd">        regular grid. Must correspond 1-to-1 with resolutions default [[0,1], [0,1]]</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    grid: tuple(Tensor)</span>
<span class="sd">    list of tensors describing positional encoding</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">resolutions</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span>
        <span class="n">grid_boundaries</span>
    <span class="p">),</span> <span class="s2">&quot;Error: inputs must have same number of dimensions&quot;</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">resolutions</span><span class="p">)</span>

    <span class="n">meshgrid_inputs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">res</span><span class="p">,</span> <span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">stop</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">resolutions</span><span class="p">,</span> <span class="n">grid_boundaries</span><span class="p">):</span>
        <span class="n">meshgrid_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">stop</span><span class="p">,</span> <span class="n">res</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="o">*</span><span class="n">meshgrid_inputs</span><span class="p">,</span> <span class="n">indexing</span><span class="o">=</span><span class="s2">&quot;ij&quot;</span><span class="p">)</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">repeat</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">dim</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">grid</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">grid</span>


<span class="c1"># Utility fucntions for Rotary embedding</span>
<span class="c1"># modified from https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py</span>
<span class="k">def</span><span class="w"> </span><span class="nf">rotate_half</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Split x&#39;s channels into two equal halves.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># split the last dimension of x into two equal halves</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="o">-</span><span class="n">x2</span><span class="p">,</span> <span class="n">x1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">apply_rotary_pos_emb</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">freqs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Apply rotation matrix computed based on freqs to rotate t.</span>
<span class="sd">    t: tensor of shape [batch_size, num_points, dim]</span>
<span class="sd">    freqs: tensor of shape [batch_size, num_points, 1]</span>

<span class="sd">    Formula: see equation (34) in https://arxiv.org/pdf/2104.09864.pdf</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="n">freqs</span><span class="o">.</span><span class="n">cos</span><span class="p">())</span> <span class="o">+</span> <span class="p">(</span><span class="n">rotate_half</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="n">freqs</span><span class="o">.</span><span class="n">sin</span><span class="p">())</span>
</pre></div>

      </div>

      

        <footer class="footer">
    <div class="content has-text-centered">
        <div class="block">
          &copy; Copyright 2025, Jean Kossaifi, David Pitt, Nikola Kovachki, Zongyi Li and Anima Anandkumar.<br/>
        </div>
    </div>
  </footer>

    </div>

  </div>  

	

  </div>  
  </div> 

  
  <script>
    function toggle_sidebar() {
        var element = document.getElementById("sidebar");
        var container = document.getElementById("column-container");
        var localtoccolumn = document.getElementById("localtoc-column");
        element.classList.toggle("hide-tablet");
        element.classList.toggle("is-hidden-mobile");
        container.classList.toggle("sidemenu-hidden");
        localtoccolumn.classList.toggle("is-one-fifth-widescreen");
        localtoccolumn.classList.toggle("is-2-desktop");
        localtoccolumn.classList.toggle("is-3-desktop");
    }
  </script> 



  </body>
</html>