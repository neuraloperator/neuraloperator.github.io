<!DOCTYPE html>

<html lang="en" data-content_root="../../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>neuralop.models.codano &#8212; neuraloperator 1.0.2 documentation</title> 
<link rel="stylesheet" href="../../../_static/tensorly_style.css">
<link rel="apple-touch-icon" sizes="180x180" href="../../../_static/favicon/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../../_static/favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="../../../_static/favicon/favicon-16x16.png">
<link rel="manifest" href="../../../_static/favicon/site.webmanifest">
<link rel="mask-icon" href="../../../_static/favicon/safari-pinned-tab.svg" color="#5bbad5">
<link rel="shortcut icon" href="../../../_static/favicon/favicon.ico">
<meta name="theme-color" content="#ffffff">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tensorly_style.css?v=a02e9698" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />

  
    <script src="../../../_static/documentation_options.js?v=1ed6394b"></script>
    <script src="../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
 <script src="../../../_static/navbar_burger.js"></script>
 <script defer src="https://use.fontawesome.com/releases/v5.14.0/js/all.js"></script>
 
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

  </head>
<body  class="has-navbar-fixed-top">

  <header>
    <navbar>
      <nav class="navbar top-navbar is-fixed-top has-shadow is-flex-wrap-wrap" role="navigation" aria-label="main top navigation">
        <div class="navbar-brand">
        

          <a class="navbar-item" href="../../../index.html">
            <img src="../../../_static/neuraloperator_logo.png" height="28">
          </a>
          <a class="navbar-item is-hidden-desktop" href="https://github.com/neuraloperator/neuraloperator" target="_blank">
              <span class="icon"><i class="fab fa-github"></i></span>
          </a>

          <a role="button" class="navbar-burger" data-target="top-nav-menu" aria-label="menu" aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
          </a>

        </div>
        
        <div class="navbar-menu" id="top-nav-menu">
        

          <div class="navbar-start">
            
              <a class="navbar-item" href="../../../install.html">
              Install
            </a>
              <a class="navbar-item" href="../../../theory_guide/index.html">
              Theory Guide
            </a>
              <a class="navbar-item" href="../../../user_guide/index.html">
              User Guide
            </a>
              <a class="navbar-item" href="../../../modules/api.html">
              API
            </a>
              <a class="navbar-item" href="../../../auto_examples/index.html">
              Examples
            </a>
              <a class="navbar-item" href="../../../dev_guide/index.html">
              Developer's Guide
            </a>
          </div>
        
          <div class="navbar-end">
            <div class="navbar-item">
            
            <a class="button is-hidden-touch is-dark" href="https://github.com/neuraloperator/neuraloperator" target="_blank">
              <span class="icon-text">
                <span class="icon is-large">
                  <i class="fab fa-github"></i>
                </span>
                <span>Github</span>
              </span>
            </a>

            </div> 
          </div> 
        </div> 

      </nav>
      
    </navbar>
  </header>


  <div id="column-container">
  <div class="columns is-mobile is-centered">
	
  

  <div class="column main-column">

    
    <div class="main-section">

      
      

      <div class="container content main-content">
        
  <h1>Source code for neuralop.models.codano</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">..layers.channel_mlp</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChannelMLP</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">..layers.spectral_convolution</span><span class="w"> </span><span class="kn">import</span> <span class="n">SpectralConv</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">..layers.skip_connections</span><span class="w"> </span><span class="kn">import</span> <span class="n">skip_connection</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">..layers.padding</span><span class="w"> </span><span class="kn">import</span> <span class="n">DomainPadding</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">..layers.coda_layer</span><span class="w"> </span><span class="kn">import</span> <span class="n">CODALayer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">..layers.resample</span><span class="w"> </span><span class="kn">import</span> <span class="n">resample</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">..layers.embeddings</span><span class="w"> </span><span class="kn">import</span> <span class="n">GridEmbedding2D</span><span class="p">,</span> <span class="n">GridEmbeddingND</span>


<div class="viewcode-block" id="CODANO">
<a class="viewcode-back" href="../../../modules/generated/neuralop.models.CODANO.html#neuralop.models.CODANO">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">CODANO</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Codomain Attention Neural Operators (CoDA-NO) </span>
<span class="sd">    </span>
<span class="sd">    It uses a specialized attention mechanism in the codomain space for data in</span>
<span class="sd">    infinite dimensional spaces as described in [1]_. The model treats each input channel as a variable of the physical system</span>
<span class="sd">    and uses attention mechanism to model the interactions between the variables. The model uses lifting and projection modules</span>
<span class="sd">    to map the input variables to a higher-dimensional space and then back to the output space. The model also supports positional</span>
<span class="sd">    encoding and static channel information for additional context of the physical system such as external force or inlet condition.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_layers : int</span>
<span class="sd">        The number of codomain attention layers. Default: 4</span>
<span class="sd">    n_modes : list</span>
<span class="sd">        The number of Fourier modes to use in integral operators in the CoDA-NO block along each dimension.</span>
<span class="sd">        Example: For a 5-layer 2D CoDA-NO, n_modes=[[16, 16], [16, 16], [16, 16], [16, 16], [16, 16]]</span>

<span class="sd">    Other parameters</span>
<span class="sd">    ---------------</span>
<span class="sd">    output_variable_codimension : int, optional</span>
<span class="sd">        The number of output channels (or output codomain dimension) corresponding to each input variable (or input channel).</span>
<span class="sd">        Example: For an input with 3 variables (channels) and output_variable_codimension=2, the output will have 6 channels (3 variables Ã— 2 codimension). Default: 1</span>
<span class="sd">    lifting_channels : int, optional</span>
<span class="sd">        Number of intermediate channels in the lifting block.</span>
<span class="sd">        The lifting module projects each input variable (i.e., each input channel) into a</span>
<span class="sd">        higher-dimensional space determined by hidden_variable_codimension.</span>
<span class="sd">        If lifting_channels is None, lifting is not performed and the input channels are</span>
<span class="sd">        directly used as tokens for codomain attention. Default: 64</span>
<span class="sd">    hidden_variable_codimension : int, optional</span>
<span class="sd">        The number of hidden channels corresponding to each input variable (or channel). Each input channel is independently lifted</span>
<span class="sd">        to hidden_variable_codimension channels by the lifting block. Default: 32</span>
<span class="sd">    projection_channels : int, optional</span>
<span class="sd">        The number of intermediate channels in the projection block of the CODANO.</span>
<span class="sd">        If projection_channels=None, projection is not performed and the output of</span>
<span class="sd">        the last CoDA block is returned directly. Default: 64</span>
<span class="sd">    use_positional_encoding : bool, optional</span>
<span class="sd">        Indicates whether to use variable-specific positional encoding. If True, a learnable positional encoding is concatenated</span>
<span class="sd">        to each variable (each input channel) before the lifting operation.</span>
<span class="sd">        The positional encoding used here is a function space generalization of the learnable positional encoding</span>
<span class="sd">        used in BERT [2]. In CODANO, the positional encoding is a function on domain which is learned directly</span>
<span class="sd">        in the Fourier Space. Default: False</span>
<span class="sd">    positional_encoding_dim : int, optional</span>
<span class="sd">        The dimension (number of channels) of the positional encoding learned of each input variable</span>
<span class="sd">        (i.e., input channel). Default: 8</span>
<span class="sd">    positional_encoding_modes : list, optional</span>
<span class="sd">        Number of Fourier modes used in positional encoding along each dimension. The positional embeddings are functions and are directly learned</span>
<span class="sd">        in Fourier space. This parameter must be specified when use_positional_encoding=True.</span>
<span class="sd">        Example: For a 2D input, positional_encoding_modes could be [16, 16]. Default: None</span>
<span class="sd">    static_channel_dim : int, optional</span>
<span class="sd">        The number of channels for static information, such as boundary conditions in PDEs. These channels are concatenated with</span>
<span class="sd">        each variable before the lifting operation and used to provide additional information</span>
<span class="sd">        regarding the physical setup of the system.</span>
<span class="sd">        When static_channel_dim &gt; 0, additional information must be provided during the forward pass.</span>
<span class="sd">        For example, static_channel_dim=1 can be used to provide mask of the domain pointing</span>
<span class="sd">        a hole or obstacle in the domain. Default: 0</span>
<span class="sd">    variable_ids : list[str], optional</span>
<span class="sd">        The names of the variables in the dataset.</span>
<span class="sd">        This parameter is only required when use_positional_encoding=True to initialize learnable</span>
<span class="sd">        positional embeddings for each unique physical variable in the dataset.</span>

<span class="sd">        For example:</span>
<span class="sd">        If the dataset consists of only Navier Stokes equations, the variable_ids=[&#39;u_x&#39;, &#39;u_y&#39;, &#39;p&#39;], representing the velocity</span>
<span class="sd">        components in x and y directions and pressure, respectively. Please note that we consider each input channel as a physical</span>
<span class="sd">        variable of the PDE.</span>

<span class="sd">        Please note that the &#39;velocity&#39; variable is composed of two channels (codimension=2) and we have split the velocity field</span>
<span class="sd">        into two components, i.e., u_x and u_y. And this is to be done for all variable with codimension &gt; 1.</span>

<span class="sd">        If the dataset consists of multiple PDEs, such as Navier Stokes and Heat equation, the variable_ids=[&#39;u_x&#39;, &#39;u_y&#39;, &#39;p&#39;, &#39;T&#39;],</span>
<span class="sd">        where &#39;T&#39; represents the temperature variable for the Heat equation and &#39;u_x&#39;, &#39;u_y&#39;, &#39;p&#39; are the velocity components and pressure</span>
<span class="sd">        for the Navier Stokes equations. This is required when we aim to learn a single solver for multiple different PDEs.</span>

<span class="sd">        This parameter is not required when use_positional_encoding=False. Default: None</span>
<span class="sd">    per_layer_scaling_factors : list, optional</span>
<span class="sd">        The output scaling factor for each CoDANO_block along each dimension. The output of each of the CoDANO_block</span>
<span class="sd">        is resampled according to the scaling factor and then passed to the following CoDANO_blocks.</span>
<span class="sd">        Example: For a 2D input and n_layers=5, per_layer_scaling_factors=[[1, 1], [0.5, 0.5], [1, 1], [2, 2], [1, 1]],</span>
<span class="sd">        which downsamples the output of the second layer by a factor of 2 and upsamples the output of the fourth layer by a factor of 2.</span>
<span class="sd">        The resolution of the output of the CODANO model is determined by the product of the scaling factors of all the layers. Default: None</span>
<span class="sd">    n_heads : list, optional</span>
<span class="sd">        The number of attention heads for each layer.</span>
<span class="sd">        Example: For a 4-layer CoDA-NO, n_heads=[2, 2, 2, 2]. Default: None (single attention head for each codomain attention block)</span>
<span class="sd">    attention_scaling_factors : list, optional</span>
<span class="sd">        Scaling factors in the codomain attention mechanism to scale the key and query functions. These scaling factors are used to resample</span>
<span class="sd">        the key and query function before calculating the attention matrix. It does not have any effect on the value functions</span>
<span class="sd">        in the codomain attention mechanism, i.e., it does not change the output shape of the block.</span>
<span class="sd">        Example: For a 5-layer CoDA-NO, attention_scaling_factors=[0.5, 0.5, 0.5, 0.5, 0.5], which downsample the key and query functions,</span>
<span class="sd">        reducing the resolution by a factor of 2. Default: None (no scaling)</span>
<span class="sd">    conv_module : nn.Module, optional</span>
<span class="sd">        The convolution module to use in the CoDANO_block. Default: SpectralConv</span>
<span class="sd">    nonlinear_attention : bool, optional</span>
<span class="sd">        Indicates whether to use a non-linear attention mechanism, employing non-linear key, query, and value operators. Default: False</span>
<span class="sd">    non_linearity : callable, optional</span>
<span class="sd">        The non-linearity to use in the codomain attention block. Default: F.gelu</span>
<span class="sd">    attention_token_dim : int, optional</span>
<span class="sd">        The number of channels in each token function. attention_token_dim must divide hidden_variable_codimension. Default: 1</span>
<span class="sd">    per_channel_attention : bool, optional</span>
<span class="sd">        Indicates whether to use a per-channel attention mechanism in Codomain attention layer. Default: False</span>
<span class="sd">    enable_cls_token : bool, optional</span>
<span class="sd">        Indicates whether to use a learnable CLASS token during the attention mechanism. We use a function-space generalization of the</span>
<span class="sd">        learnable [class] token used in vision transformers such as ViT, which is learned directly in Fourier space.</span>
<span class="sd">        The [class] function is realized on the input grid by performing an inverse Fourier transform of the learned Fourier coefficients.</span>
<span class="sd">        Then, the [class] token function is added to the set of input token functions before passing to the codomain attention layer. It aggregates</span>
<span class="sd">        information from all the other tokens through the attention mechanism. The output token corresponding to the [class] token is discarded in the</span>
<span class="sd">        output of the last CoDA block. Default: False</span>
<span class="sd">    use_horizontal_skip_connection : bool, optional</span>
<span class="sd">        Indicates whether to use horizontal skip connections, similar to U-shaped architectures. Default: False</span>
<span class="sd">    horizontal_skips_map : dict, optional</span>
<span class="sd">        A mapping that specifies horizontal skip connections between layers. Only required when use_horizontal_skip_connection=True.</span>
<span class="sd">        Example: For a 5-layer architecture, horizontal_skips_map={4: 0, 3: 1} creates skip connections from layer 0 to layer 4 and layer 1 to layer 3. Default: None</span>
<span class="sd">    domain_padding : float, optional</span>
<span class="sd">        The padding factor for each input channel. It zero pads each of the channel. Default: 0.25</span>
<span class="sd">    layer_kwargs : dict, optional</span>
<span class="sd">        Additional arguments for the CoDA blocks. Default: {}</span>

<span class="sd">    References</span>
<span class="sd">    -----------</span>
<span class="sd">    .. [1] : Rahman, Md Ashiqur, et al. &quot;Pretraining codomain attention neural operators for solving multiphysics pdes.&quot; (2024).</span>
<span class="sd">    NeurIPS 2024. https://arxiv.org/pdf/2403.12553.</span>

<span class="sd">    .. [2] : Devlin, Jacob, et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">output_variable_codimension</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">lifting_channels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
        <span class="n">hidden_variable_codimension</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
        <span class="n">projection_channels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
        <span class="n">use_positional_encoding</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">positional_encoding_dim</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
        <span class="n">positional_encoding_modes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">static_channel_dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">variable_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">use_horizontal_skip_connection</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">horizontal_skips_map</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">n_layers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">n_modes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">per_layer_scaling_factors</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">n_heads</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_scaling_factors</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">conv_module</span><span class="o">=</span><span class="n">SpectralConv</span><span class="p">,</span>
        <span class="n">nonlinear_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">non_linearity</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">gelu</span><span class="p">,</span>
        <span class="n">attention_token_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">per_channel_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">layer_kwargs</span><span class="o">=</span><span class="p">{},</span>
        <span class="n">domain_padding</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
        <span class="n">enable_cls_token</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">=</span> <span class="n">n_layers</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">n_modes</span><span class="p">)</span> <span class="o">==</span> <span class="n">n_layers</span><span class="p">,</span> <span class="s2">&quot;number of modes for all layers are not given&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">n_heads</span><span class="p">)</span> <span class="o">==</span> <span class="n">n_layers</span> <span class="ow">or</span> <span class="n">n_heads</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;number of Attention head for all layers are not given&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">per_layer_scaling_factors</span><span class="p">)</span> <span class="o">==</span> <span class="n">n_layers</span>
            <span class="ow">or</span> <span class="n">per_layer_scaling_factors</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;scaling for all layers are not given&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">attention_scaling_factors</span><span class="p">)</span> <span class="o">==</span> <span class="n">n_layers</span>
            <span class="ow">or</span> <span class="n">attention_scaling_factors</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;attention scaling for all layers are not given&quot;</span>
        <span class="k">if</span> <span class="n">use_positional_encoding</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">positional_encoding_dim</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;positional encoding dim is not given&quot;</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">positional_encoding_modes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="p">),</span> <span class="s2">&quot;positional encoding modes are not given&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">positional_encoding_dim</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="n">attention_scaling_factors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_scaling_factors</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">n_layers</span>

        <span class="n">input_variable_codimension</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># each channel is a variable</span>
        <span class="k">if</span> <span class="n">lifting_channels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lifting</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lifting_variable_codimension</span> <span class="o">=</span> <span class="n">lifting_channels</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lifting</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">if</span> <span class="n">projection_channels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">projection_variable_codimension</span> <span class="o">=</span> <span class="n">projection_channels</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">extended_variable_codimemsion</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">input_variable_codimension</span> <span class="o">+</span> <span class="n">static_channel_dim</span> <span class="o">+</span> <span class="n">positional_encoding_dim</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">lifting</span><span class="p">:</span>
            <span class="n">hidden_variable_codimension</span> <span class="o">=</span> <span class="n">extended_variable_codimemsion</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">hidden_variable_codimension</span> <span class="o">%</span> <span class="n">attention_token_dim</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">),</span> <span class="s2">&quot;attention token dim should divide hidden variable codimension&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">n_modes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">n_heads</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">n_heads</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">n_layers</span>
        <span class="k">if</span> <span class="n">per_layer_scaling_factors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">per_layer_scaling_factors</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dim</span><span class="p">]</span> <span class="o">*</span> <span class="n">n_layers</span>
        <span class="k">if</span> <span class="n">attention_scaling_factors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_scaling_factors</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">n_layers</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">input_variable_codimension</span> <span class="o">=</span> <span class="n">input_variable_codimension</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_variable_codimension</span> <span class="o">=</span> <span class="n">hidden_variable_codimension</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_modes</span> <span class="o">=</span> <span class="n">n_modes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">per_layer_scale_factors</span> <span class="o">=</span> <span class="n">per_layer_scaling_factors</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">non_linearity</span> <span class="o">=</span> <span class="n">non_linearity</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enable_cls_token</span> <span class="o">=</span> <span class="n">enable_cls_token</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding_dim</span> <span class="o">=</span> <span class="n">positional_encoding_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">variable_ids</span> <span class="o">=</span> <span class="n">variable_ids</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_scalings</span> <span class="o">=</span> <span class="n">attention_scaling_factors</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding_modes</span> <span class="o">=</span> <span class="n">positional_encoding_modes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">static_channel_dim</span> <span class="o">=</span> <span class="n">static_channel_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_kwargs</span> <span class="o">=</span> <span class="n">layer_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_positional_encoding</span> <span class="o">=</span> <span class="n">use_positional_encoding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_horizontal_skip_connection</span> <span class="o">=</span> <span class="n">use_horizontal_skip_connection</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">horizontal_skips_map</span> <span class="o">=</span> <span class="n">horizontal_skips_map</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_variable_codimension</span> <span class="o">=</span> <span class="n">output_variable_codimension</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding_modes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding_modes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding_modes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span>

        <span class="c1"># calculating scaling</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">per_layer_scale_factors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">end_to_end_scaling</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">per_layer_scale_factors</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="c1"># multiplying scaling factors</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">per_layer_scale_factors</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">end_to_end_scaling</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">i</span> <span class="o">*</span> <span class="n">j</span> <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">end_to_end_scaling</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
                <span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">end_to_end_scaling</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dim</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span>

        <span class="c1"># Setting up domain padding for encoder and reconstructor</span>
        <span class="k">if</span> <span class="n">domain_padding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">domain_padding</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">domain_padding</span> <span class="o">=</span> <span class="n">DomainPadding</span><span class="p">(</span>
                <span class="n">domain_padding</span><span class="o">=</span><span class="n">domain_padding</span><span class="p">,</span>
                <span class="n">resolution_scaling_factor</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">end_to_end_scaling</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">domain_padding</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">extended_variable_codimemsion</span> <span class="o">=</span> <span class="n">extended_variable_codimemsion</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lifting</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lifting</span> <span class="o">=</span> <span class="n">ChannelMLP</span><span class="p">(</span>
                <span class="n">in_channels</span><span class="o">=</span><span class="n">extended_variable_codimemsion</span><span class="p">,</span>
                <span class="n">out_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_variable_codimension</span><span class="p">,</span>
                <span class="n">hidden_channels</span><span class="o">=</span><span class="n">lifting_variable_codimension</span><span class="p">,</span>
                <span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                <span class="n">n_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dim</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_variable_codimension</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">extended_variable_codimemsion</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">attention_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([])</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attention_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">CODALayer</span><span class="p">(</span>
                    <span class="n">n_modes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_modes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                    <span class="n">n_heads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                    <span class="n">scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">attention_scalings</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                    <span class="n">token_codimension</span><span class="o">=</span><span class="n">attention_token_dim</span><span class="p">,</span>
                    <span class="n">per_channel_attention</span><span class="o">=</span><span class="n">per_channel_attention</span><span class="p">,</span>
                    <span class="n">nonlinear_attention</span><span class="o">=</span><span class="n">nonlinear_attention</span><span class="p">,</span>
                    <span class="n">resolution_scaling_factor</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">per_layer_scale_factors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                    <span class="n">conv_module</span><span class="o">=</span><span class="n">conv_module</span><span class="p">,</span>
                    <span class="n">non_linearity</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">non_linearity</span><span class="p">,</span>
                    <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_kwargs</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_horizontal_skip_connection</span><span class="p">:</span>
            <span class="c1"># horizontal skip connections</span>
            <span class="c1"># linear projection of the concated tokens from skip connections</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">skip_map_module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">horizontal_skips_map</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">skip_map_module</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">k</span><span class="p">)]</span> <span class="o">=</span> <span class="n">ChannelMLP</span><span class="p">(</span>
                    <span class="n">in_channels</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_variable_codimension</span><span class="p">,</span>
                    <span class="n">out_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_variable_codimension</span><span class="p">,</span>
                    <span class="n">hidden_channels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">n_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="n">non_linearity</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
                    <span class="n">n_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dim</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">ChannelMLP</span><span class="p">(</span>
                <span class="n">in_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_variable_codimension</span><span class="p">,</span>
                <span class="n">out_channels</span><span class="o">=</span><span class="n">output_variable_codimension</span><span class="p">,</span>
                <span class="n">hidden_channels</span><span class="o">=</span><span class="n">projection_variable_codimension</span><span class="p">,</span>
                <span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                <span class="n">n_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dim</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">enable_cls_token</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cls_token</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span>
                    <span class="mi">1</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_variable_codimension</span><span class="p">,</span>
                    <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">n_modes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cfloat</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">use_positional_encoding</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ParameterDict</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">variable_ids</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span>
                        <span class="mi">1</span><span class="p">,</span>
                        <span class="n">positional_encoding_dim</span><span class="p">,</span>
                        <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding_modes</span><span class="p">,</span>
                        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cfloat</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_extend_positional_encoding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_var_ids</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Add variable specific positional encoding for new variables. This function is required</span>
<span class="sd">        while adapting a pre-trained model to a new dataset/PDE with additional new variables.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        new_var_ids : list[str]</span>
<span class="sd">            IDs of the new variables to add positional encoding.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">new_var_ids</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span>
                    <span class="mi">1</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding_dim</span><span class="p">,</span>
                    <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding_modes</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cfloat</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">variable_ids</span> <span class="o">+=</span> <span class="n">new_var_ids</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_positional_encoding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">input_variable_ids</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the positional encoding for the input variables.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : torch.Tensor</span>
<span class="sd">            input tensor of shape (batch_size, num_inp_var, H, W, ...)</span>
<span class="sd">        input_variable_ids : list[str]</span>
<span class="sd">            The names of the variables corresponding to the channels of input &#39;x&#39;.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">encoding_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">input_variable_ids</span><span class="p">:</span>
            <span class="n">encoding_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">irfftn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dim</span> <span class="p">:])</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">encoding_list</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_cls_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the learnable cls token for the input variables.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : torch.Tensor</span>
<span class="sd">            input tensor of shape (batch_size, num_inp_var, H, W, ...)</span>
<span class="sd">            This is used to determine the shape of the cls token.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">cls_token</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">irfftn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_token</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dim</span> <span class="p">:])</span>
        <span class="n">repeat_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">]</span>
        <span class="n">repeat_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">cls_token</span> <span class="o">=</span> <span class="n">cls_token</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="o">*</span><span class="n">repeat_shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">cls_token</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_extend_variables</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">static_channel</span><span class="p">,</span> <span class="n">input_variable_ids</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Extend the input variables by concatenating the static channel and positional encoding.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : torch.Tensor</span>
<span class="sd">            input tensor of shape (batch_size, num_inp_var, H, W, ...)</span>
<span class="sd">        static_channel : torch.Tensor</span>
<span class="sd">            static channel tensor of shape (batch_size, static_channel_dim, H, W, ...)</span>
<span class="sd">        input_variable_ids : list[str]</span>
<span class="sd">            The names of the variables corresponding to the channels of input &#39;x&#39;.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">static_channel</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">repeat_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">]</span>
            <span class="n">repeat_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">static_channel</span> <span class="o">=</span> <span class="n">static_channel</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="o">*</span><span class="n">repeat_shape</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">static_channel</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_positional_encoding</span><span class="p">:</span>
            <span class="n">positional_encoding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_positional_encoding</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">input_variable_ids</span><span class="p">)</span>
            <span class="n">repeat_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">]</span>
            <span class="n">repeat_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">positional_encoding</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="o">*</span><span class="n">repeat_shape</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<div class="viewcode-block" id="CODANO.forward">
<a class="viewcode-back" href="../../../modules/generated/neuralop.models.CODANO.html#neuralop.models.CODANO.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">static_channel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">input_variable_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : torch.Tensor</span>
<span class="sd">            input tensor of shape (batch_size, num_inp_var, H, W, ...)</span>
<span class="sd">        static_channel : torch.Tensor</span>
<span class="sd">            static channel tensor of shape (batch_size, static_channel_dim, H, W, ...)</span>
<span class="sd">            These channels provide additional information regarding the physical setup of the system.</span>
<span class="sd">            Must be provided when `static_channel_dim &gt; 0`.</span>
<span class="sd">        input_variable_ids : list[str]</span>
<span class="sd">            The names of the variables corresponding to the channels of input &#39;x&#39;.</span>
<span class="sd">            This parameter is required when `use_positional_encoding=True`.</span>

<span class="sd">            For example, if input x represents and snapshot of the velocity field of a fluid flow, the variable_ids=[&#39;u_x&#39;, &#39;u_y&#39;].</span>
<span class="sd">            The variable_ids must be in the same order as the channels in the input tensor &#39;x&#39;, i.e., variable_ids[0] corresponds to the</span>
<span class="sd">            first channel of &#39;x&#39;, i.e., x[:, 0, ...].</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            output tensor of shape (batch_size, output_variable_codimension*num_inp_var, H, W, ...)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">num_inp_var</span><span class="p">,</span> <span class="o">*</span><span class="n">spatial_shape</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        <span class="p">)</span>  <span class="c1"># num_inp_var is the number of channels in the input</span>

        <span class="c1"># input validation</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">static_channel_dim</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="ow">and</span> <span class="n">static_channel</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="n">static_channel</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">static_channel_dim</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Epected static channel dimension is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">static_channel_dim</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">static_channel</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_positional_encoding</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">input_variable_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="p">),</span> <span class="s2">&quot;variable_ids are not provided for the input&quot;</span>
            <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span>
                <span class="n">input_variable_ids</span>
            <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Expected number of variables in input is </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">input_variable_ids</span><span class="p">)</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>

        <span class="c1"># position encoding and static channels are concatenated with the input</span>
        <span class="c1"># variables</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extend_variables</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">static_channel</span><span class="p">,</span> <span class="n">input_variable_ids</span><span class="p">)</span>

        <span class="c1"># input variables are lifted to a higher-dimensional space</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lifting</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">batch</span> <span class="o">*</span> <span class="n">num_inp_var</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">extended_variable_codimemsion</span><span class="p">,</span> <span class="o">*</span><span class="n">spatial_shape</span>
            <span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lifting</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">batch</span><span class="p">,</span> <span class="n">num_inp_var</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_variable_codimension</span><span class="p">,</span> <span class="o">*</span><span class="n">spatial_shape</span>
        <span class="p">)</span>

        <span class="c1"># getting the learnable CLASS token</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_cls_token</span><span class="p">:</span>
            <span class="n">cls_token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_cls_token</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="n">cls_token</span><span class="p">,</span>
                    <span class="n">x</span><span class="p">,</span>
                <span class="p">],</span>
                <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">num_inp_var</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># zero padding the domain of the input</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">domain_padding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">domain_padding</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># calculating the output shape</span>
        <span class="n">output_shape</span> <span class="o">=</span> <span class="p">[</span>
            <span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">j</span><span class="p">))</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dim</span> <span class="p">:],</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_to_end_scaling</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="c1"># forward pass through the Codomain Attention layers</span>
        <span class="n">skip_outputs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">layer_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span><span class="p">):</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">horizontal_skips_map</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="n">layer_idx</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">horizontal_skips_map</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
            <span class="p">):</span>
                <span class="c1"># `horizontal skip connections`</span>
                <span class="c1"># tokens from skip connections are concatenated with the</span>
                <span class="c1"># current token and then linearly projected</span>
                <span class="c1"># to the `hidden_variable_codimension`</span>
                <span class="n">skip_val</span> <span class="o">=</span> <span class="n">skip_outputs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">horizontal_skips_map</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">]]</span>
                <span class="n">resolution_scaling_factors</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">m</span> <span class="o">/</span> <span class="n">n</span> <span class="k">for</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">skip_val</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                <span class="p">]</span>
                <span class="n">resolution_scaling_factors</span> <span class="o">=</span> <span class="n">resolution_scaling_factors</span><span class="p">[</span>
                    <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dim</span> <span class="p">:</span>
                <span class="p">]</span>
                <span class="n">t</span> <span class="o">=</span> <span class="n">resample</span><span class="p">(</span>
                    <span class="n">skip_val</span><span class="p">,</span>
                    <span class="n">resolution_scaling_factors</span><span class="p">,</span>
                    <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dim</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
                    <span class="n">output_shape</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dim</span> <span class="p">:],</span>
                <span class="p">)</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                    <span class="n">batch</span> <span class="o">*</span> <span class="n">num_inp_var</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_variable_codimension</span><span class="p">,</span>
                    <span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dim</span> <span class="p">:],</span>
                <span class="p">)</span>
                <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                    <span class="n">batch</span> <span class="o">*</span> <span class="n">num_inp_var</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_variable_codimension</span><span class="p">,</span>
                    <span class="o">*</span><span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dim</span> <span class="p">:],</span>
                <span class="p">)</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">skip_map_module</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">layer_idx</span><span class="p">)](</span><span class="n">x</span><span class="p">)</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                    <span class="n">batch</span><span class="p">,</span>
                    <span class="n">num_inp_var</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_variable_codimension</span><span class="p">,</span>
                    <span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dim</span> <span class="p">:],</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="n">layer_idx</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">cur_output_shape</span> <span class="o">=</span> <span class="n">output_shape</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">cur_output_shape</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_layers</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="n">output_shape</span><span class="o">=</span><span class="n">cur_output_shape</span><span class="p">)</span>

            <span class="c1"># storing the outputs for skip connections</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">horizontal_skips_map</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="n">layer_idx</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">horizontal_skips_map</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
            <span class="p">):</span>
                <span class="n">skip_outputs</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

        <span class="c1"># removing the padding</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">domain_padding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">domain_padding</span><span class="o">.</span><span class="n">unpad</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># projecting the hidden variables to the output variables</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">batch</span> <span class="o">*</span> <span class="n">num_inp_var</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">hidden_variable_codimension</span><span class="p">,</span>
                <span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dim</span> <span class="p">:],</span>
            <span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">batch</span><span class="p">,</span>
                <span class="n">num_inp_var</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_variable_codimension</span><span class="p">,</span>
                <span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dim</span> <span class="p">:],</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span>

        <span class="c1"># discarding the CLASS token</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_cls_token</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_variable_codimension</span> <span class="p">:,</span> <span class="o">...</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">x</span></div>
</div>

</pre></div>

      </div>

      

        <footer class="footer">
    <div class="content has-text-centered">
        <div class="block">
          &copy; Copyright 2025, Jean Kossaifi, David Pitt, Nikola Kovachki, Zongyi Li and Anima Anandkumar.<br/>
        </div>
    </div>
  </footer>

    </div>

  </div>  

	

  </div>  
  </div> 

  
  <script>
    function toggle_sidebar() {
        var element = document.getElementById("sidebar");
        var container = document.getElementById("column-container");
        var localtoccolumn = document.getElementById("localtoc-column");
        element.classList.toggle("hide-tablet");
        element.classList.toggle("is-hidden-mobile");
        container.classList.toggle("sidemenu-hidden");
        localtoccolumn.classList.toggle("is-one-fifth-widescreen");
        localtoccolumn.classList.toggle("is-2-desktop");
        localtoccolumn.classList.toggle("is-3-desktop");
    }
  </script> 



  </body>
</html>