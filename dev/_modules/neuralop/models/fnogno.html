<!DOCTYPE html>

<html lang="en" data-content_root="../../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>neuralop.models.fnogno &#8212; neuraloperator 1.0.2 documentation</title> 
<link rel="stylesheet" href="../../../_static/tensorly_style.css">
<link rel="apple-touch-icon" sizes="180x180" href="../../../_static/favicon/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../../_static/favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="../../../_static/favicon/favicon-16x16.png">
<link rel="manifest" href="../../../_static/favicon/site.webmanifest">
<link rel="mask-icon" href="../../../_static/favicon/safari-pinned-tab.svg" color="#5bbad5">
<link rel="shortcut icon" href="../../../_static/favicon/favicon.ico">
<meta name="theme-color" content="#ffffff">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tensorly_style.css?v=a02e9698" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />

  
    <script src="../../../_static/documentation_options.js?v=1ed6394b"></script>
    <script src="../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
 <script src="../../../_static/navbar_burger.js"></script>
 <script defer src="https://use.fontawesome.com/releases/v5.14.0/js/all.js"></script>
 
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

  </head>
<body  class="has-navbar-fixed-top">

  <header>
    <navbar>
      <nav class="navbar top-navbar is-fixed-top has-shadow is-flex-wrap-wrap" role="navigation" aria-label="main top navigation">
        <div class="navbar-brand">
        

          <a class="navbar-item" href="../../../index.html">
            <img src="../../../_static/neuraloperator_logo.png" height="28">
          </a>
          <a class="navbar-item is-hidden-desktop" href="https://github.com/neuraloperator/neuraloperator" target="_blank">
              <span class="icon"><i class="fab fa-github"></i></span>
          </a>

          <a role="button" class="navbar-burger" data-target="top-nav-menu" aria-label="menu" aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
          </a>

        </div>
        
        <div class="navbar-menu" id="top-nav-menu">
        

          <div class="navbar-start">
            
              <a class="navbar-item" href="../../../install.html">
              Install
            </a>
              <a class="navbar-item" href="../../../theory_guide/index.html">
              Theory Guide
            </a>
              <a class="navbar-item" href="../../../user_guide/index.html">
              User Guide
            </a>
              <a class="navbar-item" href="../../../modules/api.html">
              API
            </a>
              <a class="navbar-item" href="../../../auto_examples/index.html">
              Examples
            </a>
              <a class="navbar-item" href="../../../dev_guide/index.html">
              Developer's Guide
            </a>
          </div>
        
          <div class="navbar-end">
            <div class="navbar-item">
            
            <a class="button is-hidden-touch is-dark" href="https://github.com/neuraloperator/neuraloperator" target="_blank">
              <span class="icon-text">
                <span class="icon is-large">
                  <i class="fab fa-github"></i>
                </span>
                <span>Github</span>
              </span>
            </a>

            </div> 
          </div> 
        </div> 

      </nav>
      
    </navbar>
  </header>


  <div id="column-container">
  <div class="columns is-mobile is-centered">
	
  

  <div class="column main-column">

    
    <div class="main-section">

      
      

      <div class="container content main-content">
        
  <h1>Source code for neuralop.models.fnogno</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>

<span class="c1"># Set warning filter to show each warning only once</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>

<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;once&quot;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">UserWarning</span><span class="p">)</span>


<span class="kn">from</span><span class="w"> </span><span class="nn">.base_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">..layers.channel_mlp</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChannelMLP</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">..layers.embeddings</span><span class="w"> </span><span class="kn">import</span> <span class="n">SinusoidalEmbedding</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">..layers.fno_block</span><span class="w"> </span><span class="kn">import</span> <span class="n">FNOBlocks</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">..layers.spectral_convolution</span><span class="w"> </span><span class="kn">import</span> <span class="n">SpectralConv</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">..layers.gno_block</span><span class="w"> </span><span class="kn">import</span> <span class="n">GNOBlock</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">..layers.gno_weighting_functions</span><span class="w"> </span><span class="kn">import</span> <span class="n">dispatch_weighting_fn</span>


<div class="viewcode-block" id="FNOGNO">
<a class="viewcode-back" href="../../../modules/generated/neuralop.models.FNOGNO.html#neuralop.models.FNOGNO">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">FNOGNO</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;FNOGNO&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;FNOGNO: Fourier/Geometry Neural Operator - maps from a regular N-d grid to an arbitrary query point cloud.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    in_channels : int</span>
<span class="sd">        Number of input channels. Determined by the problem.</span>
<span class="sd">    out_channels : int</span>
<span class="sd">        Number of output channels. Determined by the problem.</span>
<span class="sd">    fno_n_modes : tuple, optional</span>
<span class="sd">        Number of modes to keep along each spectral dimension of FNO block.</span>
<span class="sd">        Must be larger enough but smaller than max_resolution//2 (Nyquist frequency). Default: (16, 16, 16)</span>
<span class="sd">    fno_hidden_channels : int, optional</span>
<span class="sd">        Number of hidden channels of FNO block. Default: 64</span>
<span class="sd">    fno_n_layers : int, optional</span>
<span class="sd">        Number of FNO layers in the block. Default: 4</span>

<span class="sd">    projection_channel_ratio : int, optional</span>
<span class="sd">        Ratio of pointwise projection channels in the final ChannelMLP to fno_hidden_channels.</span>
<span class="sd">        The number of projection channels in the final ChannelMLP is computed by</span>
<span class="sd">        projection_channel_ratio * fno_hidden_channels (i.e. default 256). Default: 4</span>
<span class="sd">    gno_coord_dim : int, optional</span>
<span class="sd">        Dimension of coordinate space where GNO is computed. Determined by the problem. Default: 3</span>
<span class="sd">    gno_pos_embed_type : Literal[&quot;transformer&quot;, &quot;nerf&quot;], optional</span>
<span class="sd">        Type of optional sinusoidal positional embedding to use in GNOBlock. Default: &quot;transformer&quot;</span>
<span class="sd">    gno_radius : float, optional</span>
<span class="sd">        Radius parameter to construct graph. Default: 0.033</span>
<span class="sd">        Larger radius means more neighboors so more global interactions, but larger computational cost.</span>
<span class="sd">    gno_transform_type : str, optional</span>
<span class="sd">        Type of kernel integral transform to apply in GNO.</span>
<span class="sd">        Kernel k(x,y): parameterized as ChannelMLP MLP integrated over a neighborhood of x.</span>

<span class="sd">        Options:</span>
<span class="sd">        - &quot;linear_kernelonly&quot;: Integrand is k(x, y)</span>
<span class="sd">        - &quot;linear&quot;: Integrand is k(x, y) * f(y)</span>
<span class="sd">        - &quot;nonlinear_kernelonly&quot;: Integrand is k(x, y, f(y))</span>
<span class="sd">        - &quot;nonlinear&quot;: Integrand is k(x, y, f(y)) * f(y)</span>
<span class="sd">        Default: &quot;linear&quot;</span>
<span class="sd">    gno_weighting_function : Literal[&quot;half_cos&quot;, &quot;bump&quot;, &quot;quartic&quot;, &quot;quadr&quot;, &quot;octic&quot;], optional</span>
<span class="sd">        Choice of weighting function to use in the output GNO for Mollified Graph Neural Operator-based models.</span>
<span class="sd">        See neuralop.layers.gno_weighting_functions for more details. Default: None</span>
<span class="sd">    gno_weight_function_scale : float, optional</span>
<span class="sd">        Factor by which to scale weights from GNO weighting function. If gno_weighting_function is None, this is not used. Default: 1</span>
<span class="sd">    gno_embed_channels : int, optional</span>
<span class="sd">        Dimension of optional per-channel embedding to use in GNOBlock. Default: 32</span>
<span class="sd">    gno_embed_max_positions : int, optional</span>
<span class="sd">        Max positions of optional per-channel embedding to use in GNOBlock. If gno_pos_embed_type != &#39;transformer&#39;, value is unused. Default: 10000</span>
<span class="sd">    gno_channel_mlp_hidden_layers : list, optional</span>
<span class="sd">        Dimension of hidden ChannelMLP layers of GNO. Default: [512, 256]</span>
<span class="sd">    gno_channel_mlp_non_linearity : nn.Module, optional</span>
<span class="sd">        Nonlinear activation function between layers. Default: F.gelu</span>
<span class="sd">    gno_use_open3d : bool, optional</span>
<span class="sd">        Whether to use Open3D functionality. If False, uses simple fallback neighbor search. Default: True</span>
<span class="sd">    gno_use_torch_scatter : bool, optional</span>
<span class="sd">        Whether to use torch-scatter to perform grouped reductions in the IntegralTransform.</span>
<span class="sd">        If False, uses native Python reduction in neuralop.layers.segment_csr.</span>

<span class="sd">        .. warning::</span>
<span class="sd">            torch-scatter is an optional dependency that conflicts with the newest versions of PyTorch,</span>
<span class="sd">            so you must handle the conflict explicitly in your environment. See :ref:`torch_scatter_dependency`</span>
<span class="sd">            for more information.</span>

<span class="sd">        Default: True</span>
<span class="sd">    gno_batched : bool, optional</span>
<span class="sd">        Whether to use IntegralTransform/GNO layer in &quot;batched&quot; mode. If False, sets batched=False. Default: False</span>
<span class="sd">    fno_lifting_channel_ratio : int, optional</span>
<span class="sd">        Ratio of lifting channels to FNO hidden channels. Default: 4</span>
<span class="sd">    fno_resolution_scaling_factor : float, optional</span>
<span class="sd">        Factor by which to rescale output predictions in the original domain. Default: None</span>
<span class="sd">    fno_block_precision : str, optional</span>
<span class="sd">        Data precision to compute within FNO block. Options: &quot;full&quot;, &quot;half&quot;, &quot;mixed&quot;. Default: &quot;full&quot;</span>
<span class="sd">    fno_use_channel_mlp : bool, optional</span>
<span class="sd">        Whether to use a ChannelMLP layer after each FNO block. Default: True</span>
<span class="sd">    fno_channel_mlp_dropout : float, optional</span>
<span class="sd">        Dropout parameter of above ChannelMLP. Default: 0</span>
<span class="sd">    fno_channel_mlp_expansion : float, optional</span>
<span class="sd">        Expansion parameter of above ChannelMLP. Default: 0.5</span>
<span class="sd">    fno_non_linearity : nn.Module, optional</span>
<span class="sd">        Nonlinear activation function between each FNO layer. Default: F.gelu</span>
<span class="sd">    fno_stabilizer : nn.Module, optional</span>
<span class="sd">        By default None, otherwise tanh is used before FFT in the FNO block. Default: None</span>
<span class="sd">    fno_norm : str, optional</span>
<span class="sd">        Normalization layer to use in FNO. Options: &quot;ada_in&quot;, &quot;group_norm&quot;, &quot;instance_norm&quot;, None. Default: None</span>
<span class="sd">    fno_ada_in_features : int, optional</span>
<span class="sd">        If an adaptive mesh is used, number of channels of its positional embedding. Default: None</span>
<span class="sd">    fno_ada_in_dim : int, optional</span>
<span class="sd">        Dimensions of above FNO adaptive mesh. Default: 1</span>
<span class="sd">    fno_preactivation : bool, optional</span>
<span class="sd">        Whether to use ResNet-style preactivation. Default: False</span>
<span class="sd">    fno_skip : str, optional</span>
<span class="sd">        Type of skip connection to use. Options: &quot;linear&quot;, &quot;identity&quot;, &quot;soft-gating&quot;, None. Default: &quot;linear&quot;</span>
<span class="sd">    fno_channel_mlp_skip : str, optional</span>
<span class="sd">        Type of skip connection to use in the FNO.</span>

<span class="sd">        Options:</span>
<span class="sd">        - &quot;linear&quot;: Conv layer</span>
<span class="sd">        - &quot;soft-gating&quot;: Weights the channels of the input</span>
<span class="sd">        - &quot;identity&quot;: nn.Identity</span>
<span class="sd">        - None: No skip connection</span>

<span class="sd">        Default: &quot;soft-gating&quot;</span>
<span class="sd">    fno_separable : bool, optional</span>
<span class="sd">        Whether to use a depthwise separable spectral convolution. Default: False</span>
<span class="sd">    fno_factorization : str, optional</span>
<span class="sd">        Tensor factorization of the parameters weight to use. Options: &quot;tucker&quot;, &quot;tt&quot;, &quot;cp&quot;, None. Default: None</span>
<span class="sd">    fno_rank : float, optional</span>
<span class="sd">        Rank of the tensor factorization of the Fourier weights. Default: 1.0</span>
<span class="sd">    fno_fixed_rank_modes : bool, optional</span>
<span class="sd">        Whether to not factorize certain modes. Default: False</span>
<span class="sd">    fno_implementation : str, optional</span>
<span class="sd">        If factorization is not None, forward mode to use.</span>

<span class="sd">        Options:</span>
<span class="sd">        - &quot;reconstructed&quot;: The full weight tensor is reconstructed from the factorization and used for the forward pass</span>
<span class="sd">        - &quot;factorized&quot;: The input is directly contracted with the factors of the decomposition</span>

<span class="sd">        Default: &quot;factorized&quot;</span>
<span class="sd">    fno_decomposition_kwargs : dict, optional</span>
<span class="sd">        Additional parameters to pass to the tensor decomposition. Default: {}</span>
<span class="sd">    fno_conv_module : nn.Module, optional</span>
<span class="sd">        Spectral convolution module to use. Default: SpectralConv</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">,</span>
        <span class="n">projection_channel_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">gno_coord_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">gno_pos_embed_type</span><span class="o">=</span><span class="s2">&quot;transformer&quot;</span><span class="p">,</span>
        <span class="n">gno_transform_type</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span>
        <span class="n">fno_n_modes</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span>
        <span class="n">fno_hidden_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
        <span class="n">fno_lifting_channel_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">fno_n_layers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="c1"># Other GNO params</span>
        <span class="n">gno_embed_channels</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
        <span class="n">gno_embed_max_positions</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
        <span class="n">gno_radius</span><span class="o">=</span><span class="mf">0.033</span><span class="p">,</span>
        <span class="n">gno_weighting_function</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">gno_weight_function_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">gno_channel_mlp_hidden_layers</span><span class="o">=</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">],</span>
        <span class="n">gno_channel_mlp_non_linearity</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">gelu</span><span class="p">,</span>
        <span class="n">gno_use_open3d</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">gno_use_torch_scatter</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">gno_batched</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="c1"># Other FNO params</span>
        <span class="n">fno_resolution_scaling_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">fno_block_precision</span><span class="o">=</span><span class="s2">&quot;full&quot;</span><span class="p">,</span>
        <span class="n">fno_use_channel_mlp</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">fno_channel_mlp_dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">fno_channel_mlp_expansion</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">fno_non_linearity</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">gelu</span><span class="p">,</span>
        <span class="n">fno_stabilizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">fno_norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">fno_ada_in_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">fno_ada_in_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">fno_preactivation</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">fno_skip</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span>
        <span class="n">fno_channel_mlp_skip</span><span class="o">=</span><span class="s2">&quot;soft-gating&quot;</span><span class="p">,</span>
        <span class="n">fno_separable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">fno_factorization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">fno_rank</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">fno_fixed_rank_modes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">fno_implementation</span><span class="o">=</span><span class="s2">&quot;factorized&quot;</span><span class="p">,</span>
        <span class="n">fno_decomposition_kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(),</span>
        <span class="n">fno_conv_module</span><span class="o">=</span><span class="n">SpectralConv</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gno_coord_dim</span> <span class="o">=</span> <span class="n">gno_coord_dim</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gno_coord_dim</span> <span class="o">!=</span> <span class="mi">3</span> <span class="ow">and</span> <span class="n">gno_use_open3d</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Warning: GNO expects </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">gno_coord_dim</span><span class="si">}</span><span class="s2">-d data but Open3d expects 3-d data&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">in_coord_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">fno_n_modes</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_coord_dim</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gno_coord_dim</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Warning: FNO expects </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">in_coord_dim</span><span class="si">}</span><span class="s2">-d data while GNO expects </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">gno_coord_dim</span><span class="si">}</span><span class="s2">-d data&quot;</span>
            <span class="p">)</span>

        <span class="c1"># these lists contain the interior dimensions of the input</span>
        <span class="c1"># in order to reshape without explicitly providing dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_coord_dim_forward_order</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_coord_dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_coord_dim_reverse_order</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_coord_dim_forward_order</span>
        <span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gno_batched</span> <span class="o">=</span> <span class="n">gno_batched</span>  <span class="c1"># used in forward call to GNO</span>

        <span class="c1"># if batched, we must account for the extra batch dim</span>
        <span class="c1"># which causes previous dims to be incremented by 1</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gno_batched</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">in_coord_dim_forward_order</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_coord_dim_forward_order</span>
            <span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">in_coord_dim_reverse_order</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_coord_dim_reverse_order</span>
            <span class="p">]</span>

        <span class="k">if</span> <span class="n">fno_norm</span> <span class="o">==</span> <span class="s2">&quot;ada_in&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">fno_ada_in_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">adain_pos_embed</span> <span class="o">=</span> <span class="n">SinusoidalEmbedding</span><span class="p">(</span>
                    <span class="n">in_channels</span><span class="o">=</span><span class="n">fno_ada_in_dim</span><span class="p">,</span>
                    <span class="n">num_frequencies</span><span class="o">=</span><span class="n">fno_ada_in_features</span><span class="p">,</span>
                    <span class="n">embedding_type</span><span class="o">=</span><span class="s2">&quot;transformer&quot;</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="c1"># if ada_in positional embedding is provided, set the input dimension</span>
                <span class="c1"># of the ada_in norm to the output channels of positional embedding</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ada_in_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adain_pos_embed</span><span class="o">.</span><span class="n">out_channels</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ada_in_dim</span> <span class="o">=</span> <span class="n">fno_ada_in_dim</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">adain_pos_embed</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ada_in_dim</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Create lifting for FNOBlock separately</span>
        <span class="n">fno_lifting_channels</span> <span class="o">=</span> <span class="n">fno_lifting_channel_ratio</span> <span class="o">*</span> <span class="n">fno_hidden_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lifting</span> <span class="o">=</span> <span class="n">ChannelMLP</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_coord_dim</span><span class="p">,</span>
            <span class="n">hidden_channels</span><span class="o">=</span><span class="n">fno_lifting_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="n">fno_hidden_channels</span><span class="p">,</span>
            <span class="n">n_layers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">fno_hidden_channels</span> <span class="o">=</span> <span class="n">fno_hidden_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fno_blocks</span> <span class="o">=</span> <span class="n">FNOBlocks</span><span class="p">(</span>
            <span class="n">n_modes</span><span class="o">=</span><span class="n">fno_n_modes</span><span class="p">,</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">fno_hidden_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="n">fno_hidden_channels</span><span class="p">,</span>
            <span class="n">n_layers</span><span class="o">=</span><span class="n">fno_n_layers</span><span class="p">,</span>
            <span class="n">resolution_scaling_factor</span><span class="o">=</span><span class="n">fno_resolution_scaling_factor</span><span class="p">,</span>
            <span class="n">fno_block_precision</span><span class="o">=</span><span class="n">fno_block_precision</span><span class="p">,</span>
            <span class="n">use_channel_mlp</span><span class="o">=</span><span class="n">fno_use_channel_mlp</span><span class="p">,</span>
            <span class="n">channel_mlp_expansion</span><span class="o">=</span><span class="n">fno_channel_mlp_expansion</span><span class="p">,</span>
            <span class="n">channel_mlp_dropout</span><span class="o">=</span><span class="n">fno_channel_mlp_dropout</span><span class="p">,</span>
            <span class="n">non_linearity</span><span class="o">=</span><span class="n">fno_non_linearity</span><span class="p">,</span>
            <span class="n">stabilizer</span><span class="o">=</span><span class="n">fno_stabilizer</span><span class="p">,</span>
            <span class="n">norm</span><span class="o">=</span><span class="n">fno_norm</span><span class="p">,</span>
            <span class="n">ada_in_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ada_in_dim</span><span class="p">,</span>
            <span class="n">preactivation</span><span class="o">=</span><span class="n">fno_preactivation</span><span class="p">,</span>
            <span class="n">fno_skip</span><span class="o">=</span><span class="n">fno_skip</span><span class="p">,</span>
            <span class="n">channel_mlp_skip</span><span class="o">=</span><span class="n">fno_channel_mlp_skip</span><span class="p">,</span>
            <span class="n">separable</span><span class="o">=</span><span class="n">fno_separable</span><span class="p">,</span>
            <span class="n">factorization</span><span class="o">=</span><span class="n">fno_factorization</span><span class="p">,</span>
            <span class="n">rank</span><span class="o">=</span><span class="n">fno_rank</span><span class="p">,</span>
            <span class="n">fixed_rank_modes</span><span class="o">=</span><span class="n">fno_fixed_rank_modes</span><span class="p">,</span>
            <span class="n">implementation</span><span class="o">=</span><span class="n">fno_implementation</span><span class="p">,</span>
            <span class="n">decomposition_kwargs</span><span class="o">=</span><span class="n">fno_decomposition_kwargs</span><span class="p">,</span>
            <span class="n">conv_module</span><span class="o">=</span><span class="n">fno_conv_module</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gno_radius</span> <span class="o">=</span> <span class="n">gno_radius</span>

        <span class="k">if</span> <span class="n">gno_weighting_function</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">weight_fn</span> <span class="o">=</span> <span class="n">dispatch_weighting_fn</span><span class="p">(</span>
                <span class="n">gno_weighting_function</span><span class="p">,</span>
                <span class="n">sq_radius</span><span class="o">=</span><span class="n">gno_radius</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>
                <span class="n">scale</span><span class="o">=</span><span class="n">gno_weight_function_scale</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">weight_fn</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gno</span> <span class="o">=</span> <span class="n">GNOBlock</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">fno_hidden_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="n">fno_hidden_channels</span><span class="p">,</span>
            <span class="n">radius</span><span class="o">=</span><span class="n">gno_radius</span><span class="p">,</span>
            <span class="n">weighting_fn</span><span class="o">=</span><span class="n">weight_fn</span><span class="p">,</span>
            <span class="n">coord_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gno_coord_dim</span><span class="p">,</span>
            <span class="n">pos_embedding_type</span><span class="o">=</span><span class="n">gno_pos_embed_type</span><span class="p">,</span>
            <span class="n">pos_embedding_channels</span><span class="o">=</span><span class="n">gno_embed_channels</span><span class="p">,</span>
            <span class="n">pos_embedding_max_positions</span><span class="o">=</span><span class="n">gno_embed_max_positions</span><span class="p">,</span>
            <span class="n">channel_mlp_layers</span><span class="o">=</span><span class="n">gno_channel_mlp_hidden_layers</span><span class="p">,</span>
            <span class="n">channel_mlp_non_linearity</span><span class="o">=</span><span class="n">gno_channel_mlp_non_linearity</span><span class="p">,</span>
            <span class="n">transform_type</span><span class="o">=</span><span class="n">gno_transform_type</span><span class="p">,</span>
            <span class="n">use_open3d_neighbor_search</span><span class="o">=</span><span class="n">gno_use_open3d</span><span class="p">,</span>
            <span class="n">use_torch_scatter_reduce</span><span class="o">=</span><span class="n">gno_use_torch_scatter</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">projection_channels</span> <span class="o">=</span> <span class="n">projection_channel_ratio</span> <span class="o">*</span> <span class="n">fno_hidden_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">ChannelMLP</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">fno_hidden_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="n">out_channels</span><span class="p">,</span>
            <span class="n">hidden_channels</span><span class="o">=</span><span class="n">projection_channels</span><span class="p">,</span>
            <span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="n">n_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">non_linearity</span><span class="o">=</span><span class="n">fno_non_linearity</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># out_p : (n_out, gno_coord_dim)</span>
    <span class="c1"># in_p : (n_1, n_2, ..., n_k, k)</span>
    <span class="c1"># if batched shape is the same because this is just geometry</span>
    <span class="c1"># that remains constant across the entire batch</span>
    <span class="c1"># f : (n_1, n_2, ..., n_k,  in_channels)</span>
    <span class="c1"># if batched, (b, n_1, n_2, ..., n_k,  in_channels)</span>
    <span class="c1"># ada_in : (fno_ada_in_dim, )</span>

    <span class="c1"># returns: (fno_hidden_channels, n_1, n_2, ...)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">latent_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_p</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">ada_in</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gno_batched</span><span class="p">:</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="c1"># repeat in_p along the batch dimension for latent embedding</span>
            <span class="n">in_p</span> <span class="o">=</span> <span class="n">in_p</span><span class="o">.</span><span class="n">repeat</span><span class="p">([</span><span class="n">batch_size</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">in_p</span><span class="o">.</span><span class="n">ndim</span><span class="p">))</span>
        <span class="n">in_p</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">f</span><span class="p">,</span> <span class="n">in_p</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gno_batched</span><span class="p">:</span>
            <span class="c1"># shape: (b, k, n_1, n_2, ... n_k)</span>
            <span class="n">in_p</span> <span class="o">=</span> <span class="n">in_p</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">in_coord_dim_forward_order</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">in_p</span> <span class="o">=</span> <span class="n">in_p</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">in_coord_dim_forward_order</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Update Ada IN embedding</span>
        <span class="k">if</span> <span class="n">ada_in</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adain_pos_embed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">ada_in_embed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adain_pos_embed</span><span class="p">(</span><span class="n">ada_in</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">ada_in_embed</span> <span class="o">=</span> <span class="n">ada_in</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">fno_blocks</span><span class="o">.</span><span class="n">set_ada_in_embeddings</span><span class="p">(</span><span class="n">ada_in_embed</span><span class="p">)</span>

        <span class="c1"># Apply FNO blocks</span>

        <span class="n">in_p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lifting</span><span class="p">(</span><span class="n">in_p</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fno_blocks</span><span class="o">.</span><span class="n">n_layers</span><span class="p">):</span>
            <span class="n">in_p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fno_blocks</span><span class="p">(</span><span class="n">in_p</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gno_batched</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">in_p</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">in_p</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<div class="viewcode-block" id="FNOGNO.integrate_latent">
<a class="viewcode-back" href="../../../modules/generated/neuralop.models.FNOGNO.html#neuralop.models.FNOGNO.integrate_latent">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">integrate_latent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_p</span><span class="p">,</span> <span class="n">out_p</span><span class="p">,</span> <span class="n">latent_embed</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute integration region for each output point</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># (n_1*n_2*..., fno_hidden_channels)</span>
        <span class="c1"># if batched, (b, n1*n2*..., fno_hidden_channels)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gno_batched</span><span class="p">:</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">latent_embed</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">latent_embed</span> <span class="o">=</span> <span class="n">latent_embed</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span>
                <span class="mi">0</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">in_coord_dim_reverse_order</span><span class="p">,</span> <span class="mi">1</span>
            <span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">fno_hidden_channels</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">latent_embed</span> <span class="o">=</span> <span class="n">latent_embed</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span>
                <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">in_coord_dim_reverse_order</span><span class="p">,</span> <span class="mi">0</span>
            <span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">fno_hidden_channels</span><span class="p">))</span>

        <span class="c1"># (n_out, fno_hidden_channels)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gno</span><span class="p">(</span>
            <span class="n">y</span><span class="o">=</span><span class="n">in_p</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">in_p</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
            <span class="n">x</span><span class="o">=</span><span class="n">out_p</span><span class="p">,</span>
            <span class="n">f_y</span><span class="o">=</span><span class="n">latent_embed</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># if self.gno is variable and not batched</span>
        <span class="k">if</span> <span class="n">out</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># b, c, n_out</span>

        <span class="c1"># Project pointwise to out channels</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gno_batched</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="FNOGNO.forward">
<a class="viewcode-back" href="../../../modules/generated/neuralop.models.FNOGNO.html#neuralop.models.FNOGNO.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_p</span><span class="p">,</span> <span class="n">out_p</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">ada_in</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;FNOGNO.forward() received unexpected keyword arguments: </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">. &quot;</span>
                <span class="s2">&quot;These arguments will be ignored.&quot;</span><span class="p">,</span>
                <span class="ne">UserWarning</span><span class="p">,</span>
                <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Compute latent space embedding</span>
        <span class="n">latent_embed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_embedding</span><span class="p">(</span><span class="n">in_p</span><span class="o">=</span><span class="n">in_p</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">f</span><span class="p">,</span> <span class="n">ada_in</span><span class="o">=</span><span class="n">ada_in</span><span class="p">)</span>
        <span class="c1"># Integrate latent space</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">integrate_latent</span><span class="p">(</span><span class="n">in_p</span><span class="o">=</span><span class="n">in_p</span><span class="p">,</span> <span class="n">out_p</span><span class="o">=</span><span class="n">out_p</span><span class="p">,</span> <span class="n">latent_embed</span><span class="o">=</span><span class="n">latent_embed</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span></div>
</div>

</pre></div>

      </div>

      

        <footer class="footer">
    <div class="content has-text-centered">
        <div class="block">
          &copy; Copyright 2025, Jean Kossaifi, David Pitt, Nikola Kovachki, Zongyi Li and Anima Anandkumar.<br/>
        </div>
    </div>
  </footer>

    </div>

  </div>  

	

  </div>  
  </div> 

  
  <script>
    function toggle_sidebar() {
        var element = document.getElementById("sidebar");
        var container = document.getElementById("column-container");
        var localtoccolumn = document.getElementById("localtoc-column");
        element.classList.toggle("hide-tablet");
        element.classList.toggle("is-hidden-mobile");
        container.classList.toggle("sidemenu-hidden");
        localtoccolumn.classList.toggle("is-one-fifth-widescreen");
        localtoccolumn.classList.toggle("is-2-desktop");
        localtoccolumn.classList.toggle("is-3-desktop");
    }
  </script> 



  </body>
</html>