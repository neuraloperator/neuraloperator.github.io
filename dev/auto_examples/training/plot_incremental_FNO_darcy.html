<!DOCTYPE html>

<html lang="en" data-content_root="../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Training an FNO with incremental meta-learning &#8212; neuraloperator 2.0.0 documentation</title> 
<link rel="stylesheet" href="../../_static/tensorly_style.css">
<link rel="apple-touch-icon" sizes="180x180" href="../../_static/favicon/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../_static/favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="../../_static/favicon/favicon-16x16.png">
<link rel="manifest" href="../../_static/favicon/site.webmanifest">
<link rel="mask-icon" href="../../_static/favicon/safari-pinned-tab.svg" color="#5bbad5">
<link rel="shortcut icon" href="../../_static/favicon/favicon.ico">
<meta name="theme-color" content="#ffffff">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../_static/tensorly_style.css?v=a02e9698" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css?v=5126dfd5" />

  
    <script src="../../_static/documentation_options.js?v=51b770b3"></script>
    <script src="../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
 <script src="../../_static/navbar_burger.js"></script>
 <script defer src="https://use.fontawesome.com/releases/v5.14.0/js/all.js"></script>
 
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Development guide" href="../../dev_guide/index.html" />
    <link rel="prev" title="Checkpointing and loading training states" href="checkpoint_FNO_darcy.html" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

  </head>
<body  class="has-navbar-fixed-top">

  <header>
    <navbar>
      <nav class="navbar top-navbar is-fixed-top has-shadow is-flex-wrap-wrap" role="navigation" aria-label="main top navigation">
        <div class="navbar-brand">
        

          <a class="navbar-item" href="../../index.html">
            <img src="../../_static/neuraloperator_logo.png" height="28">
          </a>
          <a class="navbar-item is-hidden-desktop" href="https://github.com/neuraloperator/neuraloperator" target="_blank">
              <span class="icon"><i class="fab fa-github"></i></span>
          </a>

          <a role="button" class="navbar-burger" data-target="top-nav-menu" aria-label="menu" aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
          </a>

        </div>
        
        <div class="navbar-menu" id="top-nav-menu">
        

          <div class="navbar-start">
            
              <a class="navbar-item" href="../../install.html">
              Install
            </a>
              <a class="navbar-item" href="../../theory_guide/index.html">
              Theory Guide
            </a>
              <a class="navbar-item" href="../../user_guide/index.html">
              User Guide
            </a>
              <a class="navbar-item" href="../../modules/api.html">
              API
            </a>
              <a class="navbar-item" href="../index.html">
              Examples
            </a>
              <a class="navbar-item" href="../../dev_guide/index.html">
              Developer's Guide
            </a>
          </div>
        
          <div class="navbar-end">
            <div class="navbar-item">
            
            <a class="button is-hidden-touch is-dark" href="https://github.com/neuraloperator/neuraloperator" target="_blank">
              <span class="icon-text">
                <span class="icon is-large">
                  <i class="fab fa-github"></i>
                </span>
                <span>Github</span>
              </span>
            </a>

            </div> 
          </div> 
        </div> 

      </nav>
      
    </navbar>
  </header>


  <div id="column-container">
  <div class="columns is-mobile is-centered">
	
  
      <div class="column is-10-mobile is-one-third-tablet is-3-desktop is-hidden-mobile" id="sidebar">
    
    <aside class="sticky-nav sidebar-menu">
<div class="sidebar-search">
  <form class="field" id="searchbox" role="search" action="../../search.html" method="get">
    <!-- <label class="label" id="searchlabel">Quick search</label> -->
    <div class="field has-addons">
      <div class="control is-expanded">
        <input class="input" type="text" placeholder="Search the doc" name="q" aria-labelledby="searchlabel autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      </div>
      <div class="control">
        <input class="button is-info" type="submit" value="Go" />
      </div>
    </div>
  </form>
  <script>document.getElementById('searchbox').style.display = "block"</script>

</div>
      
      <div class="sidebar-menu-toc">
      <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../install.html">Installing NeuralOperator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../theory_guide/index.html">Theory Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user_guide/index.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../modules/api.html">API reference</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Examples</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../index.html#data">Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#data-generation">Data Generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#layers">Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#models">Models</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html#training-and-meta-algorithms">Training and Meta-Algorithms</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../dev_guide/index.html">Development guide</a></li>
</ul>
 
      </div>
    </aside>
  </div>
  

  <div class="column main-column">

    
    <div class="main-section">

      
      
      <div class="side-menu-toggle">
        <button class="button" id="toggle-sidebar" onclick="toggle_sidebar()">
          <span class="icon"><i class="fa fa-bars" aria-hidden="true"></i></span>
          <span>menu</span> 
        </button>
      </div>
      

      <div class="container content main-content">
        
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-auto-examples-training-plot-incremental-fno-darcy-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="training-an-fno-with-incremental-meta-learning">
<span id="sphx-glr-auto-examples-training-plot-incremental-fno-darcy-py"></span><h1>Training an FNO with incremental meta-learning<a class="headerlink" href="#training-an-fno-with-incremental-meta-learning" title="Link to this heading"></a></h1>
<p>A demo of the Incremental FNO meta-learning algorithm on our small Darcy-Flow dataset.</p>
<p>This tutorial demonstrates incremental meta-learning for neural operators, which allows
the model to gradually increase its complexity during training. This approach can lead to:</p>
<ul class="simple">
<li><p>Better convergence properties</p></li>
<li><p>More stable training dynamics</p></li>
<li><p>Improved generalization</p></li>
<li><p>Reduced computational requirements during early training</p></li>
</ul>
<p>The incremental approach starts with a small number of Fourier modes and gradually
increases the model capacity as training progresses.</p>
<div style="margin-top: 3em;"></div><section id="import-dependencies">
<h2>Import dependencies<a class="headerlink" href="#import-dependencies" title="Link to this heading"></a></h2>
<p>We import the necessary modules for incremental FNO training</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">neuralop.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">FNO</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">neuralop.data.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_darcy_flow_small</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">neuralop.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">count_model_params</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">neuralop.training</span><span class="w"> </span><span class="kn">import</span> <span class="n">AdamW</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">neuralop.training.incremental</span><span class="w"> </span><span class="kn">import</span> <span class="n">IncrementalFNOTrainer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">neuralop.data.transforms.data_processors</span><span class="w"> </span><span class="kn">import</span> <span class="n">IncrementalDataProcessor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">neuralop</span><span class="w"> </span><span class="kn">import</span> <span class="n">LpLoss</span><span class="p">,</span> <span class="n">H1Loss</span>
</pre></div>
</div>
<div style="margin-top: 3em;"></div></section>
<section id="loading-the-darcy-flow-dataset">
<h2>Loading the Darcy-Flow dataset<a class="headerlink" href="#loading-the-darcy-flow-dataset" title="Link to this heading"></a></h2>
<p>We load the Darcy-Flow dataset with multiple resolutions for incremental training.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">train_loader</span><span class="p">,</span> <span class="n">test_loaders</span><span class="p">,</span> <span class="n">output_encoder</span> <span class="o">=</span> <span class="n">load_darcy_flow_small</span><span class="p">(</span>
    <span class="n">n_train</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">test_resolutions</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">],</span>
    <span class="n">n_tests</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span>
    <span class="n">test_batch_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">],</span>
<span class="p">)</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Loading test db for resolution 16 with 100 samples
Loading test db for resolution 32 with 50 samples
</pre></div>
</div>
<div style="margin-top: 3em;"></div></section>
<section id="configuring-incremental-training">
<h2>Configuring incremental training<a class="headerlink" href="#configuring-incremental-training" title="Link to this heading"></a></h2>
<p>We set up the incremental FNO model with a small starting number of modes.
The model will gradually increase its capacity during training.
We choose to update the modes using the incremental gradient explained algorithm</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">incremental</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">if</span> <span class="n">incremental</span><span class="p">:</span>
    <span class="n">starting_modes</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># Start with very few modes</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">starting_modes</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>  <span class="c1"># Standard number of modes</span>
</pre></div>
</div>
<div style="margin-top: 3em;"></div></section>
<section id="creating-the-incremental-fno-model">
<h2>Creating the incremental FNO model<a class="headerlink" href="#creating-the-incremental-fno-model" title="Link to this heading"></a></h2>
<p>We create an FNO model with a maximum number of modes that can be reached
during incremental training. The model starts with fewer modes and grows.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">FNO</span><span class="p">(</span>
    <span class="n">max_n_modes</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>  <span class="c1"># Maximum modes the model can reach</span>
    <span class="n">n_modes</span><span class="o">=</span><span class="n">starting_modes</span><span class="p">,</span>  <span class="c1"># Starting number of modes</span>
    <span class="n">hidden_channels</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">in_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">out_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">n_params</span> <span class="o">=</span> <span class="n">count_model_params</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<div style="margin-top: 3em;"></div></section>
<section id="setting-up-the-optimizer-and-scheduler">
<h2>Setting up the optimizer and scheduler<a class="headerlink" href="#setting-up-the-optimizer-and-scheduler" title="Link to this heading"></a></h2>
<p>We use AdamW optimizer with weight decay for regularization</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">8e-3</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</pre></div>
</div>
<div style="margin-top: 3em;"></div></section>
<section id="configuring-incremental-data-processing">
<h2>Configuring incremental data processing<a class="headerlink" href="#configuring-incremental-data-processing" title="Link to this heading"></a></h2>
<p>If one wants to use Incremental Resolution, one should use the IncrementalDataProcessor.
When passed to the trainer, the trainer will automatically update the resolution.</p>
<p>Key parameters for incremental resolution:</p>
<ul class="simple">
<li><p>incremental_resolution: bool, default is False. If True, increase the resolution of the input incrementally</p></li>
<li><p>incremental_res_gap: parameter for resolution updates</p></li>
<li><p>subsampling_rates: a list of resolutions to use</p></li>
<li><p>dataset_indices: a list of indices of the dataset to slice to regularize the input resolution</p></li>
<li><p>dataset_resolution: the resolution of the input</p></li>
<li><p>epoch_gap: the number of epochs to wait before increasing the resolution</p></li>
<li><p>verbose: if True, print the resolution and the number of modes</p></li>
</ul>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">data_transform</span> <span class="o">=</span> <span class="n">IncrementalDataProcessor</span><span class="p">(</span>
    <span class="n">in_normalizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">out_normalizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
    <span class="n">subsampling_rates</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>  <span class="c1"># Resolution scaling factors</span>
    <span class="n">dataset_resolution</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>  <span class="c1"># Base resolution</span>
    <span class="n">dataset_indices</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>  <span class="c1"># Dataset indices for regularization</span>
    <span class="n">epoch_gap</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>  <span class="c1"># Epochs between resolution updates</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># Print progress information</span>
<span class="p">)</span>

<span class="n">data_transform</span> <span class="o">=</span> <span class="n">data_transform</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Original Incre Res: change index to 0
Original Incre Res: change sub to 2
Original Incre Res: change res to 8
</pre></div>
</div>
<div style="margin-top: 3em;"></div></section>
<section id="setting-up-loss-functions">
<h2>Setting up loss functions<a class="headerlink" href="#setting-up-loss-functions" title="Link to this heading"></a></h2>
<p>We use H1 loss for training and L2 loss for evaluation</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">l2loss</span> <span class="o">=</span> <span class="n">LpLoss</span><span class="p">(</span><span class="n">d</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">h1loss</span> <span class="o">=</span> <span class="n">H1Loss</span><span class="p">(</span><span class="n">d</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">train_loss</span> <span class="o">=</span> <span class="n">h1loss</span>
<span class="n">eval_losses</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;h1&quot;</span><span class="p">:</span> <span class="n">h1loss</span><span class="p">,</span> <span class="s2">&quot;l2&quot;</span><span class="p">:</span> <span class="n">l2loss</span><span class="p">}</span>
</pre></div>
</div>
<div style="margin-top: 3em;"></div></section>
<section id="displaying-training-configuration">
<h2>Displaying training configuration<a class="headerlink" href="#displaying-training-configuration" title="Link to this heading"></a></h2>
<p>We display the model parameters, optimizer, scheduler, and loss functions
to verify our incremental training setup</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">### N PARAMS ###</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">n_params</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">### OPTIMIZER ###</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">### SCHEDULER ###</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">### LOSSES ###&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">### INCREMENTAL RESOLUTION + GRADIENT EXPLAINED ###&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> * Train: </span><span class="si">{</span><span class="n">train_loss</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> * Test: </span><span class="si">{</span><span class="n">eval_losses</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>### N PARAMS ###
 537441

### OPTIMIZER ###
 AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    initial_lr: 0.008
    lr: 0.008
    weight_decay: 0.0001
)

### SCHEDULER ###
 &lt;torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7fd524cabbb0&gt;

### LOSSES ###

### INCREMENTAL RESOLUTION + GRADIENT EXPLAINED ###

 * Train: &lt;neuralop.losses.data_losses.H1Loss object at 0x7fd513a0dbd0&gt;

 * Test: {&#39;h1&#39;: &lt;neuralop.losses.data_losses.H1Loss object at 0x7fd513a0dbd0&gt;, &#39;l2&#39;: &lt;neuralop.losses.data_losses.LpLoss object at 0x7fd524caac40&gt;}
</pre></div>
</div>
<div style="margin-top: 3em;"></div></section>
<section id="configuring-the-incrementalfnotrainer">
<h2>Configuring the IncrementalFNOTrainer<a class="headerlink" href="#configuring-the-incrementalfnotrainer" title="Link to this heading"></a></h2>
<p>We set up the IncrementalFNOTrainer with various incremental learning options.
Other options include setting incremental_loss_gap = True.
If one wants to use incremental resolution, set it to True.
In this example we only update the modes and not the resolution.
When using incremental resolution, keep in mind that the number of modes
initially set should be strictly less than the resolution.</p>
<p>Key parameters for incremental training:</p>
<ul class="simple">
<li><p>incremental_grad: bool, default is False. If True, use the base incremental algorithm based on gradient variance</p></li>
<li><p>incremental_grad_eps: threshold for gradient variance</p></li>
<li><p>incremental_buffer: number of buffer modes to calculate gradient variance</p></li>
<li><p>incremental_max_iter: initial number of iterations</p></li>
<li><p>incremental_grad_max_iter: maximum iterations to accumulate gradients</p></li>
<li><p>incremental_loss_gap: bool, default is False. If True, use the incremental algorithm based on loss gap</p></li>
<li><p>incremental_loss_eps: threshold for loss gap</p></li>
</ul>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create the IncrementalFNOTrainer with our configuration</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">IncrementalFNOTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">n_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
    <span class="n">data_processor</span><span class="o">=</span><span class="n">data_transform</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">incremental_loss_gap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># Use gradient-based incremental learning</span>
    <span class="n">incremental_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># Enable gradient-based mode updates</span>
    <span class="n">incremental_grad_eps</span><span class="o">=</span><span class="mf">0.9999</span><span class="p">,</span>  <span class="c1"># Gradient variance threshold</span>
    <span class="n">incremental_loss_eps</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>  <span class="c1"># Loss gap threshold</span>
    <span class="n">incremental_buffer</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># Buffer modes for gradient calculation</span>
    <span class="n">incremental_max_iter</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="c1"># Initial iterations</span>
    <span class="n">incremental_grad_max_iter</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># Maximum gradient accumulation iterations</span>
<span class="p">)</span>
</pre></div>
</div>
<div style="margin-top: 3em;"></div></section>
<section id="training-the-incremental-fno-model">
<h2>Training the incremental FNO model<a class="headerlink" href="#training-the-incremental-fno-model" title="Link to this heading"></a></h2>
<p>We train the model using incremental meta-learning. The trainer will:
1. Start with a small number of Fourier modes
2. Gradually increase the model capacity based on gradient variance
3. Monitor the incremental learning progress
4. Evaluate on test data throughout training</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">train_loader</span><span class="p">,</span>
    <span class="n">test_loaders</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">scheduler</span><span class="p">,</span>
    <span class="n">regularizer</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">training_loss</span><span class="o">=</span><span class="n">train_loss</span><span class="p">,</span>
    <span class="n">eval_losses</span><span class="o">=</span><span class="n">eval_losses</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Training on 100 samples
Testing on [50, 50] samples         on resolutions [16, 32].
/opt/hostedtoolcache/Python/3.13.11/x64/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: &#39;pin_memory&#39; argument is set as true but no accelerator is found, then device pinned memory won&#39;t be used.
  warnings.warn(warn_msg)
/opt/hostedtoolcache/Python/3.13.11/x64/lib/python3.13/site-packages/torch/nn/modules/module.py:1786: UserWarning: FNO.forward() received unexpected keyword arguments: [&#39;y&#39;]. These arguments will be ignored.
  return forward_call(*args, **kwargs)
Raw outputs of shape torch.Size([16, 1, 8, 8])
/home/runner/work/neuraloperator/neuraloperator/neuralop/training/trainer.py:536: UserWarning: H1Loss.__call__() received unexpected keyword arguments: [&#39;x&#39;]. These arguments will be ignored.
  loss += training_loss(out, **sample)
[0] time=0.24, avg_loss=0.9379, train_err=13.3989
/home/runner/work/neuraloperator/neuraloperator/neuralop/training/trainer.py:581: UserWarning: LpLoss.__call__() received unexpected keyword arguments: [&#39;x&#39;]. These arguments will be ignored.
  val_loss = loss(out, **sample)
Eval: 16_h1=0.8951, 16_l2=0.5764, 32_h1=0.9397, 32_l2=0.5756
[1] time=0.21, avg_loss=0.8235, train_err=11.7637
Eval: 16_h1=0.8356, 16_l2=0.4704, 32_h1=1.0422, 32_l2=0.4983
[2] time=0.22, avg_loss=0.6751, train_err=9.6441
/home/runner/work/neuraloperator/neuraloperator/neuralop/training/incremental.py:244: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)
  torch.Tensor(strength_vector),
Eval: 16_h1=0.8407, 16_l2=0.4864, 32_h1=1.0523, 32_l2=0.5230
[3] time=0.21, avg_loss=0.6172, train_err=8.8165
Eval: 16_h1=0.7064, 16_l2=0.3904, 32_h1=0.9171, 32_l2=0.4216
[4] time=0.21, avg_loss=0.5630, train_err=8.0428
Eval: 16_h1=0.7321, 16_l2=0.4232, 32_h1=0.9270, 32_l2=0.4512
[5] time=0.21, avg_loss=0.5213, train_err=7.4477
Eval: 16_h1=0.8455, 16_l2=0.4952, 32_h1=1.0422, 32_l2=0.5151
[6] time=0.22, avg_loss=0.5604, train_err=8.0060
Eval: 16_h1=0.8722, 16_l2=0.4394, 32_h1=1.2960, 32_l2=0.4788
[7] time=0.22, avg_loss=0.5087, train_err=7.2676
Eval: 16_h1=0.6518, 16_l2=0.3533, 32_h1=0.9138, 32_l2=0.3902
[8] time=0.21, avg_loss=0.4225, train_err=6.0350
Eval: 16_h1=0.7022, 16_l2=0.3866, 32_h1=0.9870, 32_l2=0.4312
[9] time=0.22, avg_loss=0.4486, train_err=6.4087
Eval: 16_h1=0.6325, 16_l2=0.3323, 32_h1=0.9428, 32_l2=0.3703
Incre Res Update: change index to 1
Incre Res Update: change sub to 1
Incre Res Update: change res to 16
[10] time=0.30, avg_loss=0.5505, train_err=7.8647
Eval: 16_h1=0.5113, 16_l2=0.2967, 32_h1=0.6253, 32_l2=0.2858
[11] time=0.28, avg_loss=0.4654, train_err=6.6486
Eval: 16_h1=0.5035, 16_l2=0.3235, 32_h1=0.6616, 32_l2=0.3382
[12] time=0.29, avg_loss=0.4690, train_err=6.7004
Eval: 16_h1=0.4196, 16_l2=0.2600, 32_h1=0.5457, 32_l2=0.2662
[13] time=0.28, avg_loss=0.3872, train_err=5.5309
Eval: 16_h1=0.3993, 16_l2=0.2503, 32_h1=0.5213, 32_l2=0.2664
[14] time=0.28, avg_loss=0.3615, train_err=5.1648
Eval: 16_h1=0.4214, 16_l2=0.2614, 32_h1=0.5556, 32_l2=0.2738
[15] time=0.28, avg_loss=0.3779, train_err=5.3988
Eval: 16_h1=0.3647, 16_l2=0.2222, 32_h1=0.4885, 32_l2=0.2301
[16] time=0.29, avg_loss=0.3327, train_err=4.7523
Eval: 16_h1=0.3624, 16_l2=0.2319, 32_h1=0.4977, 32_l2=0.2450
[17] time=0.28, avg_loss=0.3194, train_err=4.5629
Eval: 16_h1=0.3759, 16_l2=0.2348, 32_h1=0.4892, 32_l2=0.2361
[18] time=0.29, avg_loss=0.3482, train_err=4.9740
Eval: 16_h1=0.3483, 16_l2=0.2251, 32_h1=0.4770, 32_l2=0.2444
[19] time=0.28, avg_loss=0.3083, train_err=4.4047
Eval: 16_h1=0.3184, 16_l2=0.2044, 32_h1=0.4294, 32_l2=0.2149

{&#39;train_err&#39;: 4.404683658054897, &#39;avg_loss&#39;: 0.3083278560638428, &#39;avg_lasso_loss&#39;: None, &#39;epoch_train_time&#39;: 0.2826887019999731, &#39;16_h1&#39;: tensor(0.3184), &#39;16_l2&#39;: tensor(0.2044), &#39;32_h1&#39;: tensor(0.4294), &#39;32_l2&#39;: tensor(0.2149)}
</pre></div>
</div>
<div style="margin-top: 3em;"></div></section>
<section id="visualizing-incremental-fno-predictions">
<h2>Visualizing incremental FNO predictions<a class="headerlink" href="#visualizing-incremental-fno-predictions" title="Link to this heading"></a></h2>
<p>We visualize the modelâ€™s predictions after incremental training.
Note that we trained on a very small resolution for a very small number of epochs.
In practice, we would train at larger resolution on many more samples.</p>
<p>However, for practicality, we created a minimal example that:
i) fits in just a few MB of memory
ii) can be trained quickly on CPU</p>
<p>In practice we would train a Neural Operator on one or multiple GPUs</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">test_samples</span> <span class="o">=</span> <span class="n">test_loaders</span><span class="p">[</span><span class="mi">32</span><span class="p">]</span><span class="o">.</span><span class="n">dataset</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">test_samples</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
    <span class="c1"># Input x</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># Ground-truth</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># Model prediction: incremental FNO output</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

    <span class="c1"># Plot input x</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">index</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">index</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Input x&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([],</span> <span class="p">[])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([],</span> <span class="p">[])</span>

    <span class="c1"># Plot ground-truth y</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">index</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">index</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Ground-truth y&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([],</span> <span class="p">[])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([],</span> <span class="p">[])</span>

    <span class="c1"># Plot model prediction</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">index</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">index</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Incremental FNO prediction&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([],</span> <span class="p">[])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([],</span> <span class="p">[])</span>

<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Incremental FNO predictions on Darcy-Flow data&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.98</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img src="../../_images/sphx_glr_plot_incremental_FNO_darcy_001.png" srcset="../../_images/sphx_glr_plot_incremental_FNO_darcy_001.png" alt="Incremental FNO predictions on Darcy-Flow data, Input x, Ground-truth y, Incremental FNO prediction" class = "sphx-glr-single-img"/><p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 10.879 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-examples-training-plot-incremental-fno-darcy-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/cfa67f99f9c5464f9ef46d85700f3993/plot_incremental_FNO_darcy.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_incremental_FNO_darcy.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/6f0cb8afeee9a2f4cf43365cf9462331/plot_incremental_FNO_darcy.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_incremental_FNO_darcy.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/603c477adddf2a819b9a26b979fc009e/plot_incremental_FNO_darcy.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">plot_incremental_FNO_darcy.zip</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>


      </div>

      
        <nav class="pagination" role="navigation" aria-label="pagination">
    
    <a class="button pagination-previous" href="checkpoint_FNO_darcy.html" title="previous page" accesskey="p">
        <span class="icon">
            <i class="fa fa-arrow-circle-left"></i>
        </span>
        <span>Checkpointing and loading training states</span>
    </a>
    
    
    <a class="button pagination-next" href="../../dev_guide/index.html" title="next page" accesskey="n">
        <span>Development guide </span>
        <span class="icon">
            <i class="fa fa-arrow-circle-right"></i>
        </span>
    </a>
    
</nav>

      

        <footer class="footer">
    <div class="content has-text-centered">
        <div class="block">
          &copy; Copyright 2025, Jean Kossaifi, David Pitt, Nikola Kovachki, Zongyi Li and Anima Anandkumar.<br/>
        </div>
    </div>
  </footer>

    </div>

  </div>  

	
    
    <div class="column is-hidden-touch is-2-desktop is-one-fifth-widescreen" id="localtoc-column">

    <aside class="sticky-nav localtoc"> 
        <p class="menu-label"> 
            <span class="icon-text">
                <span class="icon"><i class="fas fa-duotone fa-list"></i></span>
                <span> On this page </span>
            </span>
        </p>

        <div class="menu menu-list localtoc-list">
        <ul>
<li><a class="reference internal" href="#">Training an FNO with incremental meta-learning</a><ul>
<li><a class="reference internal" href="#import-dependencies">Import dependencies</a></li>
<li><a class="reference internal" href="#loading-the-darcy-flow-dataset">Loading the Darcy-Flow dataset</a></li>
<li><a class="reference internal" href="#configuring-incremental-training">Configuring incremental training</a></li>
<li><a class="reference internal" href="#creating-the-incremental-fno-model">Creating the incremental FNO model</a></li>
<li><a class="reference internal" href="#setting-up-the-optimizer-and-scheduler">Setting up the optimizer and scheduler</a></li>
<li><a class="reference internal" href="#configuring-incremental-data-processing">Configuring incremental data processing</a></li>
<li><a class="reference internal" href="#setting-up-loss-functions">Setting up loss functions</a></li>
<li><a class="reference internal" href="#displaying-training-configuration">Displaying training configuration</a></li>
<li><a class="reference internal" href="#configuring-the-incrementalfnotrainer">Configuring the IncrementalFNOTrainer</a></li>
<li><a class="reference internal" href="#training-the-incremental-fno-model">Training the incremental FNO model</a></li>
<li><a class="reference internal" href="#visualizing-incremental-fno-predictions">Visualizing incremental FNO predictions</a></li>
</ul>
</li>
</ul>

        </div>
    </aside>
    </div>

  

  </div>  
  </div> 

  
  <script>
    function toggle_sidebar() {
        var element = document.getElementById("sidebar");
        var container = document.getElementById("column-container");
        var localtoccolumn = document.getElementById("localtoc-column");
        element.classList.toggle("hide-tablet");
        element.classList.toggle("is-hidden-mobile");
        container.classList.toggle("sidemenu-hidden");
        localtoccolumn.classList.toggle("is-one-fifth-widescreen");
        localtoccolumn.classList.toggle("is-2-desktop");
        localtoccolumn.classList.toggle("is-3-desktop");
    }
  </script> 



  </body>
</html>