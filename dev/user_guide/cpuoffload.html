<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>CPU Offloading for Training Neural Operators &#8212; neuraloperator 1.0.2 documentation</title> 
<link rel="stylesheet" href="../_static/tensorly_style.css">
<link rel="apple-touch-icon" sizes="180x180" href="../_static/favicon/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="../_static/favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="../_static/favicon/favicon-16x16.png">
<link rel="manifest" href="../_static/favicon/site.webmanifest">
<link rel="mask-icon" href="../_static/favicon/safari-pinned-tab.svg" color="#5bbad5">
<link rel="shortcut icon" href="../_static/favicon/favicon.ico">
<meta name="theme-color" content="#ffffff">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/tensorly_style.css?v=a02e9698" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />

  
    <script src="../_static/documentation_options.js?v=1ed6394b"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
 <script src="../_static/navbar_burger.js"></script>
 <script defer src="https://use.fontawesome.com/releases/v5.14.0/js/all.js"></script>
 
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

  </head>
<body  class="has-navbar-fixed-top">

  <header>
    <navbar>
      <nav class="navbar top-navbar is-fixed-top has-shadow is-flex-wrap-wrap" role="navigation" aria-label="main top navigation">
        <div class="navbar-brand">
        

          <a class="navbar-item" href="../index.html">
            <img src="../_static/neuraloperator_logo.png" height="28">
          </a>
          <a class="navbar-item is-hidden-desktop" href="https://github.com/neuraloperator/neuraloperator" target="_blank">
              <span class="icon"><i class="fab fa-github"></i></span>
          </a>

          <a role="button" class="navbar-burger" data-target="top-nav-menu" aria-label="menu" aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
          </a>

        </div>
        
        <div class="navbar-menu" id="top-nav-menu">
        

          <div class="navbar-start">
            
              <a class="navbar-item" href="../install.html">
              Install
            </a>
              <a class="navbar-item" href="../theory_guide/index.html">
              Theory Guide
            </a>
              <a class="navbar-item" href="index.html">
              User Guide
            </a>
              <a class="navbar-item" href="../modules/api.html">
              API
            </a>
              <a class="navbar-item" href="../auto_examples/index.html">
              Examples
            </a>
              <a class="navbar-item" href="../dev_guide/index.html">
              Developer's Guide
            </a>
          </div>
        
          <div class="navbar-end">
            <div class="navbar-item">
            
            <a class="button is-hidden-touch is-dark" href="https://github.com/neuraloperator/neuraloperator" target="_blank">
              <span class="icon-text">
                <span class="icon is-large">
                  <i class="fab fa-github"></i>
                </span>
                <span>Github</span>
              </span>
            </a>

            </div> 
          </div> 
        </div> 

      </nav>
      
    </navbar>
  </header>


  <div id="column-container">
  <div class="columns is-mobile is-centered">
	
  
      <div class="column is-10-mobile is-one-third-tablet is-3-desktop is-hidden-mobile" id="sidebar">
    
    <aside class="sticky-nav sidebar-menu">
<div class="sidebar-search">
  <form class="field" id="searchbox" role="search" action="../search.html" method="get">
    <!-- <label class="label" id="searchlabel">Quick search</label> -->
    <div class="field has-addons">
      <div class="control is-expanded">
        <input class="input" type="text" placeholder="Search the doc" name="q" aria-labelledby="searchlabel autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      </div>
      <div class="control">
        <input class="button is-info" type="submit" value="Go" />
      </div>
    </div>
  </form>
  <script>document.getElementById('searchbox').style.display = "block"</script>

</div>
      
      <div class="sidebar-menu-toc">
      <ul>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installing NeuralOperator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../theory_guide/index.html">Theory Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/api.html">API reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_examples/index.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dev_guide/index.html">Development guide</a></li>
</ul>
 
      </div>
    </aside>
  </div>
  

  <div class="column main-column">

    
    <div class="main-section">

      
      
      <div class="side-menu-toggle">
        <button class="button" id="toggle-sidebar" onclick="toggle_sidebar()">
          <span class="icon"><i class="fa fa-bars" aria-hidden="true"></i></span>
          <span>menu</span> 
        </button>
      </div>
      

      <div class="container content main-content">
        
  <section id="cpu-offloading-for-training-neural-operators">
<h1>CPU Offloading for Training Neural Operators<a class="headerlink" href="#cpu-offloading-for-training-neural-operators" title="Link to this heading"></a></h1>
<p>Enable CPU offloading to reduce GPU memory usage during training with high-resolution inputs.</p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading"></a></h2>
<p>When training neural operators with high-resolution inputs, GPU memory usage can become a bottleneck. The peak memory consumption often exceeds CUDA limits because all intermediate activations in the computation graph are stored on the GPU by default.</p>
<p>Each activation tensor typically has a shape of:</p>
<div class="math notranslate nohighlight">
\[\text{batch\_size} \times \text{hidden\_dim} \times N_x \times N_y \times \dots\]</div>
<p>where <span class="math notranslate nohighlight">\(N_x, N_y, \dots\)</span> are the spatial or temporal resolutions of the input. As the computation graph grows deeper during forward and backward passes, a large number of such intermediate tensors accumulate, leading to high GPU memory consumption.</p>
<p><strong>CPU offloading</strong> addresses this by moving activations to CPU memory during training, allowing:</p>
<ul class="simple">
<li><p>Training with higher-resolution inputs under limited GPU memory</p></li>
<li><p>Training larger models without reducing batch size</p></li>
<li><p>Better memory utilization across CPU and GPU</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>CPU offloading trades memory for compute time, as data transfer between CPU and GPU adds overhead.</p>
</div>
</section>
<section id="example-usage">
<h2>Example Usage<a class="headerlink" href="#example-usage" title="Link to this heading"></a></h2>
<p>Below is a complete example demonstrating CPU offloading integration with NeuralOperator training.</p>
<section id="setup-and-data-loading">
<h3>1. Setup and Data Loading<a class="headerlink" href="#setup-and-data-loading" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">wraps</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">neuralop.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">FNO</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">neuralop</span><span class="w"> </span><span class="kn">import</span> <span class="n">Trainer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">neuralop.training</span><span class="w"> </span><span class="kn">import</span> <span class="n">AdamW</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">neuralop.data.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_darcy_flow_small</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">neuralop.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">count_model_params</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">neuralop</span><span class="w"> </span><span class="kn">import</span> <span class="n">LpLoss</span><span class="p">,</span> <span class="n">H1Loss</span>

<span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span>
</pre></div>
</div>
<p>Load the dataset:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load Darcy flow dataset with specified resolutions</span>
<span class="n">train_loader</span><span class="p">,</span> <span class="n">test_loaders</span><span class="p">,</span> <span class="n">data_processor</span> <span class="o">=</span> <span class="n">load_darcy_flow_small</span><span class="p">(</span>
    <span class="n">n_train</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">test_resolutions</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">],</span>
    <span class="n">n_tests</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span>
    <span class="n">test_batch_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">],</span>
<span class="p">)</span>
<span class="n">data_processor</span> <span class="o">=</span> <span class="n">data_processor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="model-creation">
<h3>2. Model Creation<a class="headerlink" href="#model-creation" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create FNO model with specified parameters</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">FNO</span><span class="p">(</span>
    <span class="n">n_modes</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span>           <span class="c1"># Fourier modes for each dimension</span>
    <span class="n">in_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>              <span class="c1"># Input channels</span>
    <span class="n">out_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>             <span class="c1"># Output channels</span>
    <span class="n">hidden_channels</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>         <span class="c1"># Hidden layer width</span>
    <span class="n">projection_channel_ratio</span><span class="o">=</span><span class="mi">2</span>  <span class="c1"># Channel expansion ratio</span>
<span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model parameters: </span><span class="si">{</span><span class="n">count_model_params</span><span class="p">(</span><span class="n">model</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="enable-cpu-offloading">
<h3>3. Enable CPU Offloading<a class="headerlink" href="#enable-cpu-offloading" title="Link to this heading"></a></h3>
<p>Wrap the modelâ€™s forward function to enable automatic CPU offloading:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">wrap_forward_with_offload</span><span class="p">(</span><span class="n">forward_fn</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Wrap a forward function to enable CPU offloading of activations.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    forward_fn : callable</span>
<span class="sd">        The original forward function to wrap</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    callable</span>
<span class="sd">        Wrapped forward function with CPU offloading enabled</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@wraps</span><span class="p">(</span><span class="n">forward_fn</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">wrapped_forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># Enable CPU offloading context for this forward pass</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">save_on_cpu</span><span class="p">(</span><span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">forward_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">wrapped_forward</span>

<span class="c1"># Apply CPU offloading to the model</span>
<span class="n">model</span><span class="o">.</span><span class="n">forward</span> <span class="o">=</span> <span class="n">wrap_forward_with_offload</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="training-loop">
<h3>4. Training Loop<a class="headerlink" href="#training-loop" title="Link to this heading"></a></h3>
<p>No changes are needed in your existing training code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup optimizer and loss function</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">8e-3</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
<span class="n">l2loss</span> <span class="o">=</span> <span class="n">LpLoss</span><span class="p">(</span><span class="n">d</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">h1loss</span> <span class="o">=</span> <span class="n">H1Loss</span><span class="p">(</span><span class="n">d</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Training step - works exactly as before</span>
<span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">target_data</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
    <span class="c1"># Move data to device</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>    <span class="c1"># Shape: (batch, channels, height, width)</span>
    <span class="n">target_data</span> <span class="o">=</span> <span class="n">target_data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># Shape: (batch, channels, height, width)</span>

    <span class="c1"># Forward pass - activations automatically offloaded to CPU</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>

    <span class="c1"># Compute loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">l2loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target_data</span><span class="p">)</span>

    <span class="c1"># Backward pass - gradients computed with CPU-stored activations</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="performance-considerations">
<h2>Performance Considerations<a class="headerlink" href="#performance-considerations" title="Link to this heading"></a></h2>
<dl class="simple">
<dt><strong>Memory vs Speed Trade-off</strong></dt><dd><p>CPU offloading reduces GPU memory usage at the cost of increased training time due to data transfer overhead between CPU and GPU memory.</p>
</dd>
<dt><strong>When to Use</strong></dt><dd><ul class="simple">
<li><p>Training fails with CUDA out-of-memory errors</p></li>
<li><p>You want to increase batch size or model resolution</p></li>
<li><p>GPU memory is the primary bottleneck</p></li>
</ul>
</dd>
<dt><strong>When Not to Use</strong></dt><dd><ul class="simple">
<li><p>GPU memory is sufficient for your current setup</p></li>
<li><p>Training speed is more critical than memory usage</p></li>
<li><p>CPU memory is also limited</p></li>
</ul>
</dd>
<dt><strong>Optimization Tips</strong></dt><dd><ul class="simple">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">pin_memory=True</span></code> for faster CPU-GPU transfers</p></li>
<li><p>Consider gradient checkpointing as an alternative memory-saving technique</p></li>
<li><p>Monitor both GPU and CPU memory usage during training</p></li>
</ul>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>CPU offloading requires PyTorch version 1.12.0 or higher. Ensure your environment meets this requirement before using this feature.</p>
</div>
</section>
</section>


      </div>

      
        <nav class="pagination" role="navigation" aria-label="pagination">
    
    
</nav>

      

        <footer class="footer">
    <div class="content has-text-centered">
        <div class="block">
          &copy; Copyright 2025, Jean Kossaifi, David Pitt, Nikola Kovachki, Zongyi Li and Anima Anandkumar.<br/>
        </div>
    </div>
  </footer>

    </div>

  </div>  

	
    
    <div class="column is-hidden-touch is-2-desktop is-one-fifth-widescreen" id="localtoc-column">

    <aside class="sticky-nav localtoc"> 
        <p class="menu-label"> 
            <span class="icon-text">
                <span class="icon"><i class="fas fa-duotone fa-list"></i></span>
                <span> On this page </span>
            </span>
        </p>

        <div class="menu menu-list localtoc-list">
        <ul>
<li><a class="reference internal" href="#">CPU Offloading for Training Neural Operators</a><ul>
<li><a class="reference internal" href="#overview">Overview</a></li>
<li><a class="reference internal" href="#example-usage">Example Usage</a><ul>
<li><a class="reference internal" href="#setup-and-data-loading">1. Setup and Data Loading</a></li>
<li><a class="reference internal" href="#model-creation">2. Model Creation</a></li>
<li><a class="reference internal" href="#enable-cpu-offloading">3. Enable CPU Offloading</a></li>
<li><a class="reference internal" href="#training-loop">4. Training Loop</a></li>
</ul>
</li>
<li><a class="reference internal" href="#performance-considerations">Performance Considerations</a></li>
</ul>
</li>
</ul>

        </div>
    </aside>
    </div>

  

  </div>  
  </div> 

  
  <script>
    function toggle_sidebar() {
        var element = document.getElementById("sidebar");
        var container = document.getElementById("column-container");
        var localtoccolumn = document.getElementById("localtoc-column");
        element.classList.toggle("hide-tablet");
        element.classList.toggle("is-hidden-mobile");
        container.classList.toggle("sidemenu-hidden");
        localtoccolumn.classList.toggle("is-one-fifth-widescreen");
        localtoccolumn.classList.toggle("is-2-desktop");
        localtoccolumn.classList.toggle("is-3-desktop");
    }
  </script> 



  </body>
</html>