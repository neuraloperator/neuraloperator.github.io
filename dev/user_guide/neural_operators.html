<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Neural Operators: an Introduction &#8212; neuraloperator 0.3.0 documentation</title> 
<link rel="stylesheet" href="../_static/tensorly_style.css">
<link rel="apple-touch-icon" sizes="180x180" href="../_static/favicon/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="../_static/favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="../_static/favicon/favicon-16x16.png">
<link rel="manifest" href="../_static/favicon/site.webmanifest">
<link rel="mask-icon" href="../_static/favicon/safari-pinned-tab.svg" color="#5bbad5">
<link rel="shortcut icon" href="../_static/favicon/favicon.ico">
<meta name="theme-color" content="#ffffff">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/tensorly_style.css?v=a02e9698" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />

  
    <script src="../_static/documentation_options.js?v=e259d695"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
 <script src="../_static/navbar_burger.js"></script>
 <script defer src="https://use.fontawesome.com/releases/v5.14.0/js/all.js"></script>
 
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Fourier Neural Operators" href="fno.html" />
    <link rel="prev" title="Quick-Start" href="quickstart.html" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

  </head>
<body  class="has-navbar-fixed-top">

  <header>
    <navbar>
      <nav class="navbar top-navbar is-fixed-top has-shadow is-flex-wrap-wrap" role="navigation" aria-label="main top navigation">
        <div class="navbar-brand">
        

          <a class="navbar-item" href="../index.html">
            <img src="../_static/neuraloperator_logo.png" height="28">
          </a>
          <a class="navbar-item is-hidden-desktop" href="https://github.com/neuraloperator/neuraloperator" target="_blank">
              <span class="icon"><i class="fab fa-github"></i></span>
          </a>

          <a role="button" class="navbar-burger" data-target="top-nav-menu" aria-label="menu" aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
          </a>

        </div>
        
        <div class="navbar-menu" id="top-nav-menu">
        

          <div class="navbar-start">
            
              <a class="navbar-item" href="../install.html">
              Install
            </a>
              <a class="navbar-item" href="index.html">
              User Guide
            </a>
              <a class="navbar-item" href="../modules/api.html">
              API
            </a>
              <a class="navbar-item" href="../auto_examples/index.html">
              Examples
            </a>
          </div>
        
          <div class="navbar-end">
            <div class="navbar-item">
            
            <a class="button is-hidden-touch is-dark" href="https://github.com/neuraloperator/neuraloperator" target="_blank">
              <span class="icon-text">
                <span class="icon is-large">
                  <i class="fab fa-github"></i>
                </span>
                <span>Github</span>
              </span>
            </a>

            </div> 
          </div> 
        </div> 

      </nav>
      
    </navbar>
  </header>


  <div id="column-container">
  <div class="columns is-mobile is-centered">
	
  
      <div class="column is-10-mobile is-one-third-tablet is-3-desktop is-hidden-mobile" id="sidebar">
    
    <aside class="sticky-nav sidebar-menu">
<div class="sidebar-search">
  <form class="field" id="searchbox" role="search" action="../search.html" method="get">
    <!-- <label class="label" id="searchlabel">Quick search</label> -->
    <div class="field has-addons">
      <div class="control is-expanded">
        <input class="input" type="text" placeholder="Search the doc" name="q" aria-labelledby="searchlabel autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      </div>
      <div class="control">
        <input class="button is-info" type="submit" value="Go" />
      </div>
    </div>
  </form>
  <script>document.getElementById('searchbox').style.display = "block"</script>

</div>
      
      <div class="sidebar-menu-toc">
      <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installing NeuralOperator</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">User guide</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="quickstart.html">Quick-Start</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Neural Operators: an Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="fno.html">Fourier Neural Operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="training.html">Training neural operator models</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../modules/api.html">API reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_examples/index.html">Gallery of examples</a></li>
</ul>
 
      </div>
    </aside>
  </div>
  

  <div class="column main-column">

    
    <div class="main-section">

      
      
      <div class="side-menu-toggle">
        <button class="button" id="toggle-sidebar" onclick="toggle_sidebar()">
          <span class="icon"><i class="fa fa-bars" aria-hidden="true"></i></span>
          <span>menu</span> 
        </button>
      </div>
      

      <div class="container content main-content">
        
  <section id="neural-operators-an-introduction">
<h1>Neural Operators: an Introduction<a class="headerlink" href="#neural-operators-an-introduction" title="Link to this heading"></a></h1>
<p>Here, we introduce neural operators, a class of models that learn
mappings between function spaces and solve partial differential equations.
You can also check out the papers <a class="footnote-reference brackets" href="#id4" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> and <a class="footnote-reference brackets" href="#id5" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a> for more formal derivations,
as well as the blog post <a class="footnote-reference brackets" href="#id6" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>.</p>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h2>
<p>Scientific computations are expensive.
It could take days and months for numerical solvers to simulate fluid dynamics and many-body motions.
Because to achieve good accuracy,
the numerical solvers need to discretize the space and time into very fine grids
and solve a great number of equations on the grids.
Recently, people are developing data-driven methods based on machine learning techniques such as deep learning.
Instead of directly solving the problems, data-driven solvers learn from the data of the problems and solutions.
When querying new problems, data-driven solvers can directly give predictions based on the data.
Since they don’t need to discretize the space into very small pieces and solve all these equations,
these data-driven solvers are usually much faster compared to traditional numerical solvers.</p>
<p>However, data-driven solvers are subject to the quality of the data given.
If the training data is not good enough, they can’t make good predictions.
In scientific computing, usually, the training data are generated by the traditional numerical solvers.
And to generate good enough data, it still takes days and months for these numerical solvers.
Sometime, data are observed from experiments and there are just no good training data.
Especially, people consider neural networks as interpolators which may not be able to extrapolate.
It is unclear whether neural networks can generalize to unseen data.
So if the training data are of one resolution,
the learned solvers can only solve the problem in this specific resolution.
In general, generalization is a crucial problem in machine learning.
It becomes a trade-off: these machine learning based methods make evaluation easier,
but the training process could be even more painful.</p>
<p>To dealing with this problem, we purpose operator learning. By encoding certain structures,
we let the neural network learn the mapping of functions and generalize among different resolutions.
As a result, we can first use a numerical method generated some less-accurate, low-resolution data,
but the learned solver is still able to give reasonable, high-resolution predictions.
In some sense, both training and evaluation can be pain-free.</p>
</section>
<section id="operator-learning">
<h2>Operator Learning<a class="headerlink" href="#operator-learning" title="Link to this heading"></a></h2>
<p>In mathematics, operators are usually referring to the mappings between function spaces.
You most likely have already encountered some operators.
For example, the differentiation and integration are operators.
When we take the derivative  or do an indefinite integration of a function,
we will get another function.
In other words, the differentiation and integration are mappings from function space to function space.</p>
<p>In scientific computing, usually the problem is to solve some form of differential equations. Consider a general differential equation of the form:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}u = f\]</div>
<p>where  <span class="math notranslate nohighlight">\(u\)</span> and <span class="math notranslate nohighlight">\(f\)</span> are some functions on the physical domain, and
<span class="math notranslate nohighlight">\(\mathcal{L}\)</span> is some differential operator that maps
the function <span class="math notranslate nohighlight">\(u\)</span> to the function <span class="math notranslate nohighlight">\(f\)</span>.
Usually, <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> and <span class="math notranslate nohighlight">\(f\)</span> are given. The task is to solve for <span class="math notranslate nohighlight">\(u\)</span>.
That is, we want to learn an operator like the inverse of <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> that
maps the function <span class="math notranslate nohighlight">\(f\)</span> to the function <span class="math notranslate nohighlight">\(u\)</span>.
So the problem of PDE is indeed an operator learning problem.</p>
<p>The classical development of neural networks has been primarily
for mappings between a finite-dimensional Euclidean space and a set of classes
(e.g. an image vector to a label),
or between two finite-dimensional Euclidean spaces (e.g. an image vector to another image vector).
However, many problems in physics and math involve learning the mapping between function spaces,
which poses a limitation on the classical neural network based methods.
Besides all these problem governed by differential equations,
we are learning operators in many common machine learning setting.
For a bold example, images should be considered as functions of light defined on a continuous region,
instead of as <span class="math notranslate nohighlight">\(32 \times 32\)</span> pixel vectors.</p>
<p>In this work, we aim to generalize neural networks so that they can learn operators,
the mappings between infinite-dimensional spaces, with a special focus on PDEs.</p>
</section>
<section id="limitation-of-fixed-discretization">
<h2>Limitation of Fixed Discretization<a class="headerlink" href="#limitation-of-fixed-discretization" title="Link to this heading"></a></h2>
<p>PDEs are, unfortunately, hard.
Instead of learning the operator, people usually discretize the physical domain
and cast the problem in finite-dimensional Euclidean space.
Indeed, hundred years of effort has been made to develop numerical solvers
such as the finite element method and finite difference method.</p>
<a class="reference internal image-reference" href="../_images/grids.jpg"><img alt="../_images/grids.jpg" src="../_images/grids.jpg" style="width: 800px;" />
</a>
<p>Three examples of discretization.
The left one is a regular grid used in the finite difference method;
the middle one is a triangulated grid used in the finite element method;
the right one is a cylinder mesh for real-world airfoil problem.</p>
<p>Just like how we store images by pixels in <em>.PNG</em> and <em>.JPG</em> formats,
we need to discretize the domain of PDEs into some grid and solve the equation on the grid.
It really makes the thing easier.
These traditional numerical solvers are awesome, but they have some drawbacks:</p>
<ul class="simple">
<li><p>The error scales steeply with the resolution. We need a high resolution to get good approximations.</p></li>
<li><p>The computation and storage steeply scale with the resolution (i.e. the size of the grid).</p></li>
<li><p>When the equation is solved on one discretization, we cannot change the discretization anymore.</p></li>
</ul>
<p><em>.PNG</em> and <em>.JPG</em> formats are good.
But sometimes maybe we want to save the images as vector images in <em>.EPS</em> or <em>.SVG</em> formats,
so that it can be used and displayed in any context.
And for some images, the vector image format is more convenient and efficient.
Similarly, we want to find the continuous version for PDEs, an operator that is invariant of discretization.</p>
<p>Furthermore, mathematically speaking, such continuous,
discretization-invariant format is in some sense, closer to the real, analytic solution.
It has an important mathematical meaning.
Bear the motivation in mind. Let’s develop a rigorous formulation.</p>
</section>
<section id="problem-setting">
<h2>Problem Setting<a class="headerlink" href="#problem-setting" title="Link to this heading"></a></h2>
<p>Let’s be more concrete. Consider the standard second order elliptic PDE</p>
<div class="math notranslate nohighlight">
\[- \nabla \cdot (a(x) \nabla u(x))  = f(x), \quad  x \in D\]</div>
<div class="math notranslate nohighlight">
\[u(x) = 0, \quad x \in \partial D\]</div>
<p>for some bounded, open domain <span class="math notranslate nohighlight">\(D \subset \mathbb{R}^d\)</span> and a fixed source function
<span class="math notranslate nohighlight">\(f\)</span>. This equation is prototypical of PDEs arising in
numerous applications including hydrology  and elasticity.
For a given function <span class="math notranslate nohighlight">\(a\)</span>,
the equation has a unique weak solution <span class="math notranslate nohighlight">\(u\)</span>
and therefore we can define the solution operator <span class="math notranslate nohighlight">\(\mathcal{F}_{true}\)</span>
as the map from function to function <span class="math notranslate nohighlight">\(a \mapsto u\)</span>.</p>
<p>Our goal is to learn a operator <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> approximating <span class="math notranslate nohighlight">\(\mathcal{F}_{true}\)</span>,
by using a finite collection of observations of input-output pairs
<span class="math notranslate nohighlight">\(\{a_j, u_j\}_{j=1}^N\)</span>, where each <span class="math notranslate nohighlight">\(a_j\)</span> and <span class="math notranslate nohighlight">\(u_j\)</span> are functions on <span class="math notranslate nohighlight">\(D\)</span>.
In practice, the training data is solved numerically or observed in experiments.
In other words, functions <span class="math notranslate nohighlight">\(a_j\)</span> and <span class="math notranslate nohighlight">\(u_j\)</span> come with discretization.
Let <span class="math notranslate nohighlight">\(P_K = \{x_1,\dots,x_K\} \subset D\)</span> be a <span class="math notranslate nohighlight">\(K\)</span>-point discretization of the domain
<span class="math notranslate nohighlight">\(D\)</span> and assume we have observations <span class="math notranslate nohighlight">\(a_j|_{P_K}, u_j|_{P_K}\)</span>, for a finite
collection  of input-output pairs indexed by <span class="math notranslate nohighlight">\(j\)</span>.
We will show how to learn a discretization-invariant mapping based on discretized data.</p>
</section>
<section id="kernel-formulation">
<h2>Kernel Formulation<a class="headerlink" href="#kernel-formulation" title="Link to this heading"></a></h2>
<p>For a general PDE of the form:</p>
<div class="math notranslate nohighlight">
\[(\mathcal{L}_a u)(x)= f(x), \quad x \in D\]</div>
<div class="math notranslate nohighlight">
\[u(x) = 0, \quad x \in \partial D\]</div>
<p>Under fairly general conditions on <span class="math notranslate nohighlight">\(\mathcal{L}_a\)</span>,
we may define the Green’s function <span class="math notranslate nohighlight">\(G : D \times D \to \mathbb{R}\)</span> as the
unique solution to the problem</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_a G(x, \cdot) = \delta_x\]</div>
<p>where <span class="math notranslate nohighlight">\(\delta_x\)</span> is the delta measure on <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> centered at <span class="math notranslate nohighlight">\(x\)</span>.
Note that <span class="math notranslate nohighlight">\(G\)</span> will depend on the coefficient <span class="math notranslate nohighlight">\(a\)</span> thus we will henceforth denote it as <span class="math notranslate nohighlight">\(G_a\)</span>.
Then operator <span class="math notranslate nohighlight">\(\mathcal{F}_{true}\)</span> can be written as an integral operator of green function:</p>
<div class="math notranslate nohighlight">
\[u(x) = \int_D G_a(x,y)f(y) \: dy\]</div>
<p>Generally the Green’s function is continuous at points <span class="math notranslate nohighlight">\(x \neq y\)</span>,
for example, when <span class="math notranslate nohighlight">\(\mathcal{L}_a\)</span> is uniformly elliptic.
Hence it is natural to model the kernel via a neural network <span class="math notranslate nohighlight">\(\kappa\)</span>.
Just as the Green function, the kernel network <span class="math notranslate nohighlight">\(\kappa\)</span> takes input <span class="math notranslate nohighlight">\((x,y)\)</span>.
Since the kernel depends on <span class="math notranslate nohighlight">\(a\)</span>, we let <span class="math notranslate nohighlight">\(\kappa\)</span> also take input <span class="math notranslate nohighlight">\((a(x),a(y))\)</span>.</p>
<div class="math notranslate nohighlight">
\[u(x) = \int_D \kappa(x,y,a(x),a(y))f(y) \: dy\]</div>
</section>
<section id="as-an-iterative-solver">
<h2>As an Iterative Solver<a class="headerlink" href="#as-an-iterative-solver" title="Link to this heading"></a></h2>
<p>In our setting, <span class="math notranslate nohighlight">\(f\)</span> is an unknown but fixed function.
Instead of doing the kernel convolution with <span class="math notranslate nohighlight">\(f\)</span>,
we will formulate it as an iterative solver
that approximated <span class="math notranslate nohighlight">\(u\)</span> by <span class="math notranslate nohighlight">\(u_t\)</span>,
where <span class="math notranslate nohighlight">\(t = 0,\ldots,T\)</span> is the time step.</p>
<p>The algorithm starts from an initialization <span class="math notranslate nohighlight">\(u_0\)</span>,
for which we use <span class="math notranslate nohighlight">\(u_0(x) = (x, a(x))\)</span>.
At each time step <span class="math notranslate nohighlight">\(t\)</span>, it updates <span class="math notranslate nohighlight">\(u_{t+1}\)</span> by an kernel convolution of <span class="math notranslate nohighlight">\(u_{t}\)</span>.</p>
<div class="math notranslate nohighlight">
\[u_{t+1}(x) = \int_D \kappa(x,y,a(x),a(y))u_{t}(x) \: dy\]</div>
<p>It works like an implicit iteration.
At each iteration the algorithm solves an equation of <span class="math notranslate nohighlight">\(u_{t}(x)\)</span> and <span class="math notranslate nohighlight">\(u_{t+1}(x)\)</span>
by the kernel integral. <span class="math notranslate nohighlight">\(u_T\)</span> will be output as the final prediction.</p>
<p>To further take the advantage of neural networks,
we will lift <span class="math notranslate nohighlight">\(u(x) \in \mathbb{R}^d\)</span>
to a high dimensional representation <span class="math notranslate nohighlight">\(v(x) \in \mathbb{R}^n\)</span>,
with <span class="math notranslate nohighlight">\(n\)</span> the dimension of the hidden representation.</p>
<p>The overall algorithmic framework follow:</p>
<div class="math notranslate nohighlight">
\[v_0(x) = NN_1 (x, a(x))\]</div>
<div class="math notranslate nohighlight">
\[v_{t+1}(x) = \sigma\Big( W v_t(x) + \int_{B(x,r)} \kappa_{\phi}\big(x,y,a(x),a(y)\big) v_t(y)\: \mathrm{d}y \Big) \quad \text{for } \ t=1,\ldots,T\]</div>
<div class="math notranslate nohighlight">
\[u(x) = NN_2 (v_T (x))\]</div>
<p>where <span class="math notranslate nohighlight">\(NN_1\)</span> and <span class="math notranslate nohighlight">\(NN_2\)</span> are two feed-forward neural networks
that lifts the initialization to hidden representation <span class="math notranslate nohighlight">\(v\)</span>
and projects the representation back to the solution <span class="math notranslate nohighlight">\(u\)</span>, respective.
<span class="math notranslate nohighlight">\(\sigma\)</span> is an activation function such as ReLU.
the additional term <span class="math notranslate nohighlight">\(W \in \mathbb{R}^{n \times n}\)</span> is a linear transformation
that acts on $v$.
Notice, since the kernel integration happens in the high dimensional representation,
the output of <span class="math notranslate nohighlight">\(\kappa_{\phi}\)</span> is not a scalar,
but a linear transformation <span class="math notranslate nohighlight">\(\kappa_{\phi}\big(x,y,a(x),a(y)\big)\in \mathbb{R}^{n \times n}\)</span>.</p>
</section>
<section id="graph-neural-networks">
<h2>Graph Neural Networks<a class="headerlink" href="#graph-neural-networks" title="Link to this heading"></a></h2>
<p>To do the integration, we again need some discretization.
Assuming a uniform distribution of <span class="math notranslate nohighlight">\(y\)</span>,
the integral <span class="math notranslate nohighlight">\(\int_{B(x,r)} \kappa_{\phi}\big(x,y,a(x),a(y)\big)
v_t(y)\: \mathrm{d}y\)</span> can be approximated by a sum:</p>
<div class="math notranslate nohighlight">
\[\frac{1}{|N|}\sum_{y \in N(x)} \kappa(x,y,a(x),a(y))v_t(y)\]</div>
<p>Observation: the kernel integral is equivalent to the message passing on graphs</p>
<p>If you are similar with graph neural network,
you may have already realized this formulation is the same as
the aggregation of messages in graph network.
Message passing graph networks comprise a standard architecture employing edge features
(gilmer et al, 2017).</p>
<p>If we properly construct graphs on the spatial domain <span class="math notranslate nohighlight">\(D\)</span> of the PDE,
the kernel integration can be viewed as an aggregation of messages.
Given node features <span class="math notranslate nohighlight">\(v_t(x) \in \mathbb{R}^{n}\)</span>,
edge features <span class="math notranslate nohighlight">\(e(x,y) \in \mathbb{R}^{n_e}\)</span>,
and a graph <span class="math notranslate nohighlight">\(G\)</span>, the message passing neural network with averaging aggregation is</p>
<div class="math notranslate nohighlight">
\[v_{t+1}(x) =  \sigma\Big(W v_t(x) + \frac{1}{|N(x)|} \sum_{y \in N(x)} \kappa_{\phi}\big(e(x,y)\big) v_t(y)\Big)\]</div>
<p>where <span class="math notranslate nohighlight">\(W \in \mathbb{R}^{n \times n}\)</span>,
<span class="math notranslate nohighlight">\(N(x)\)</span> is the neighborhood of <span class="math notranslate nohighlight">\(x\)</span> according to the graph,
<span class="math notranslate nohighlight">\(\kappa_{\phi}\big(e(x,y)\big)\)</span> is a neural network
taking as input edge features and as output
a matrix in <span class="math notranslate nohighlight">\(\mathbb{R}^{n \times n}\)</span>.
Relating to our kernel formulation, <span class="math notranslate nohighlight">\(e(x,y) = (x,y,a(x),a(y))\)</span>.</p>
<a class="reference internal image-reference" href="../_images/graph.jpg"><img alt="../_images/graph.jpg" src="../_images/graph.jpg" style="width: 800px;" />
</a>
</section>
<section id="nystrom-approximation">
<h2>Nystrom Approximation<a class="headerlink" href="#nystrom-approximation" title="Link to this heading"></a></h2>
<p>Ideally, to use all the information available,
we should construct <span class="math notranslate nohighlight">\(K\)</span> nodes in the graph for all the points in the discretization
<span class="math notranslate nohighlight">\(P_k = \{x_1,\ldots, x_K\}\)</span>, which will create <span class="math notranslate nohighlight">\(O(K^2)\)</span> edges.
It is quite expensive.
Thankfully, we don’t need all the points to get an accurate approximation.
For each graph, the error of Monte Carlo approximation of the kernel integral
<span class="math notranslate nohighlight">\(\int_{B(x,r)} \kappa_{\phi}(x,y) v_t(y)\: \mathrm{d}y\)</span> scales with <span class="math notranslate nohighlight">\(m^{-1/2}\)</span>,
where <span class="math notranslate nohighlight">\(m\)</span> is the number of nodes sampled.</p>
<p>Since we will sample <span class="math notranslate nohighlight">\(N\)</span> graphs in total for all <span class="math notranslate nohighlight">\(N\)</span> training examples <span class="math notranslate nohighlight">\(\{a_j, u_j\}^N\)</span>,
the overall error of the kernel is much smaller than <span class="math notranslate nohighlight">\(m^{-1/2}\)</span>.
In practice, sampling <span class="math notranslate nohighlight">\(m \sim 200\)</span> nodes is sufficient for <span class="math notranslate nohighlight">\(K \sim 100000\)</span> points.</p>
<p>It is possible to further improve the approximation
using more sophisticated Nystrom Approximation methods.
For example, we can estimate the importance of each points,
and add more nodes to the difficult and singularity areas in the PDEs.</p>
</section>
<section id="experiments-poisson-equations">
<h2>Experiments: Poisson Equations<a class="headerlink" href="#experiments-poisson-equations" title="Link to this heading"></a></h2>
<p>First let’s do a sanity check. Consider a simple poisson equation:</p>
<div class="math notranslate nohighlight">
\[-\Delta u = f\]</div>
<p>We set <span class="math notranslate nohighlight">\(v_0 = f\)</span> and <span class="math notranslate nohighlight">\(T=1\)</span>, using one iteration of the graph kernel network
to learn the operator <span class="math notranslate nohighlight">\(\mathcal{F}: f \mapsto u\)</span>.</p>
<section id="poisson-equation">
<h3>poisson equation<a class="headerlink" href="#poisson-equation" title="Link to this heading"></a></h3>
<a class="reference internal image-reference" href="../_images/nik_kernel.jpg"><img alt="../_images/nik_kernel.jpg" src="../_images/nik_kernel.jpg" style="width: 800px;" />
</a>
<p>As shown in the figure above, we compare the true analytic Green function <span class="math notranslate nohighlight">\(G(x,y)\)</span> (left)
with the learned kernel <span class="math notranslate nohighlight">\(\kappa_{\phi}(x,y)\)</span>  (right).
The learned kernel is almost the same as the true kernel,
which means are neural network formulation does match the Green function expression.</p>
</section>
<section id="d-poisson-equation">
<h3>2D poisson equation<a class="headerlink" href="#d-poisson-equation" title="Link to this heading"></a></h3>
<a class="reference internal image-reference" href="../_images/GKN_compare.jpg"><img alt="../_images/GKN_compare.jpg" src="../_images/GKN_compare.jpg" style="width: 800px;" />
</a>
<p>By assuming the kernel structure,
graph kernel networks need only a few training examples to learn the shape of solution <span class="math notranslate nohighlight">\(u\)</span>.
As shown in the figure above, the graph kernel network can roughly learn <span class="math notranslate nohighlight">\(u\)</span> with <span class="math notranslate nohighlight">\(5\)</span> training pairs,
which a feedforward neural network need at least <span class="math notranslate nohighlight">\(100\)</span> training examples.</p>
</section>
</section>
<section id="experiments-generalization-of-resolution">
<h2>Experiments: generalization of resolution<a class="headerlink" href="#experiments-generalization-of-resolution" title="Link to this heading"></a></h2>
<p>For the large scale experiments, we use Darcy equation of the form</p>
<div class="math notranslate nohighlight">
\[- \nabla \cdot (a(x) \nabla u(x))  = f(x), \quad  x \in D\]</div>
<div class="math notranslate nohighlight">
\[u(x) = 0, \quad x \in \partial D\]</div>
<p>and learn the operator <span class="math notranslate nohighlight">\(\mathcal{F}: a \mapsto u\)</span>.</p>
<p>To show the generalization property, we train the graph kernel network
with nodes sampled from the resolution <span class="math notranslate nohighlight">\(s \times s\)</span>
and test on another resolution <span class="math notranslate nohighlight">\(s' \times s'\)</span> .</p>
<p>As shown in the table above for each row,
the test errors on different resolutions are about the same,
which means the graph kernel network can
also generalize in the semi-supervised setting.
An figure for <span class="math notranslate nohighlight">\(s=16, s'=241\)</span> is following (where error is absolute squared error):</p>
<a class="reference internal image-reference" href="../_images/uai_16to241.jpg"><img alt="../_images/uai_16to241.jpg" src="../_images/uai_16to241.jpg" style="width: 800px;" />
</a>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading"></a></h2>
<p>In the work we purposed to use graph networks for operator learning in PDE problems.
By varying the underlying graph and discretization,
the learned kernel is invariant of the discretization.
Experiments confirm the graph kernel networks are able to generalize among different discretization.
And in the fixed discretization setting, the graph kernel networks also have good performances compared to several benchmark.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id4" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Neural operator: Graph kernel network for partial differential equations,
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, Anima Anandkumar</p>
</aside>
<aside class="footnote brackets" id="id5" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>Neural operator: Learning maps between function spaces,
Nikola Kovachki, Zongyi Li, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, Anima Anandkumar</p>
</aside>
<aside class="footnote brackets" id="id6" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">3</a><span class="fn-bracket">]</span></span>
<p>Blog post by Zongyi Li, <a class="reference external" href="https://zongyi-li.github.io/blog/2020/graph-pde/">https://zongyi-li.github.io/blog/2020/graph-pde/</a></p>
</aside>
</aside>
</section>
</section>


      </div>

      
        <nav class="pagination" role="navigation" aria-label="pagination">
    
    <a class="button pagination-previous" href="quickstart.html" title="previous page" accesskey="p">
        <span class="icon">
            <i class="fa fa-arrow-circle-left"></i>
        </span>
        <span>Quick-Start</span>
    </a>
    
    
    <a class="button pagination-next" href="fno.html" title="next page" accesskey="n">
        <span>Fourier Neural Operators </span>
        <span class="icon">
            <i class="fa fa-arrow-circle-right"></i>
        </span>
    </a>
    
</nav>

      

        <footer class="footer">
    <div class="content has-text-centered">
        <div class="block">
          &copy; Copyright 2024, Jean Kossaifi, David Pitt, Nikola Kovachki, Zongyi Li and Anima Anandkumar.<br/>
        </div>
    </div>
  </footer>

    </div>

  </div>  

	
    
    <div class="column is-hidden-touch is-2-desktop is-one-fifth-widescreen" id="localtoc-column">

    <aside class="sticky-nav localtoc"> 
        <p class="menu-label"> 
            <span class="icon-text">
                <span class="icon"><i class="fas fa-duotone fa-list"></i></span>
                <span> On this page </span>
            </span>
        </p>

        <div class="menu menu-list localtoc-list">
        <ul>
<li><a class="reference internal" href="#">Neural Operators: an Introduction</a><ul>
<li><a class="reference internal" href="#introduction">Introduction</a></li>
<li><a class="reference internal" href="#operator-learning">Operator Learning</a></li>
<li><a class="reference internal" href="#limitation-of-fixed-discretization">Limitation of Fixed Discretization</a></li>
<li><a class="reference internal" href="#problem-setting">Problem Setting</a></li>
<li><a class="reference internal" href="#kernel-formulation">Kernel Formulation</a></li>
<li><a class="reference internal" href="#as-an-iterative-solver">As an Iterative Solver</a></li>
<li><a class="reference internal" href="#graph-neural-networks">Graph Neural Networks</a></li>
<li><a class="reference internal" href="#nystrom-approximation">Nystrom Approximation</a></li>
<li><a class="reference internal" href="#experiments-poisson-equations">Experiments: Poisson Equations</a><ul>
<li><a class="reference internal" href="#poisson-equation">poisson equation</a></li>
<li><a class="reference internal" href="#d-poisson-equation">2D poisson equation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#experiments-generalization-of-resolution">Experiments: generalization of resolution</a></li>
<li><a class="reference internal" href="#conclusion">Conclusion</a></li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

        </div>
    </aside>
    </div>

  

  </div>  
  </div> 

  
  <script>
    function toggle_sidebar() {
        var element = document.getElementById("sidebar");
        var container = document.getElementById("column-container");
        var localtoccolumn = document.getElementById("localtoc-column");
        element.classList.toggle("hide-tablet");
        element.classList.toggle("is-hidden-mobile");
        container.classList.toggle("sidemenu-hidden");
        localtoccolumn.classList.toggle("is-one-fifth-widescreen");
        localtoccolumn.classList.toggle("is-2-desktop");
        localtoccolumn.classList.toggle("is-3-desktop");
    }
  </script> 



  </body>
</html>