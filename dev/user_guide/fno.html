<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Fourier Neural Operators &#8212; neuraloperator 0.3.0 documentation</title> 
<link rel="stylesheet" href="../_static/tensorly_style.css">
<link rel="apple-touch-icon" sizes="180x180" href="../_static/favicon/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="../_static/favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="../_static/favicon/favicon-16x16.png">
<link rel="manifest" href="../_static/favicon/site.webmanifest">
<link rel="mask-icon" href="../_static/favicon/safari-pinned-tab.svg" color="#5bbad5">
<link rel="shortcut icon" href="../_static/favicon/favicon.ico">
<meta name="theme-color" content="#ffffff">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/tensorly_style.css?v=a02e9698" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />

  
    <script src="../_static/documentation_options.js?v=e259d695"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
 <script src="../_static/navbar_burger.js"></script>
 <script defer src="https://use.fontawesome.com/releases/v5.14.0/js/all.js"></script>
 
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Training neural operator models" href="training.html" />
    <link rel="prev" title="Neural Operators: an Introduction" href="neural_operators.html" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

  </head>
<body  class="has-navbar-fixed-top">

  <header>
    <navbar>
      <nav class="navbar top-navbar is-fixed-top has-shadow is-flex-wrap-wrap" role="navigation" aria-label="main top navigation">
        <div class="navbar-brand">
        

          <a class="navbar-item" href="../index.html">
            <img src="../_static/neuraloperator_logo.png" height="28">
          </a>
          <a class="navbar-item is-hidden-desktop" href="https://github.com/neuraloperator/neuraloperator" target="_blank">
              <span class="icon"><i class="fab fa-github"></i></span>
          </a>

          <a role="button" class="navbar-burger" data-target="top-nav-menu" aria-label="menu" aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
          </a>

        </div>
        
        <div class="navbar-menu" id="top-nav-menu">
        

          <div class="navbar-start">
            
              <a class="navbar-item" href="../install.html">
              Install
            </a>
              <a class="navbar-item" href="index.html">
              User Guide
            </a>
              <a class="navbar-item" href="../modules/api.html">
              API
            </a>
              <a class="navbar-item" href="../auto_examples/index.html">
              Examples
            </a>
          </div>
        
          <div class="navbar-end">
            <div class="navbar-item">
            
            <a class="button is-hidden-touch is-dark" href="https://github.com/neuraloperator/neuraloperator" target="_blank">
              <span class="icon-text">
                <span class="icon is-large">
                  <i class="fab fa-github"></i>
                </span>
                <span>Github</span>
              </span>
            </a>

            </div> 
          </div> 
        </div> 

      </nav>
      
    </navbar>
  </header>


  <div id="column-container">
  <div class="columns is-mobile is-centered">
	
  
      <div class="column is-10-mobile is-one-third-tablet is-3-desktop is-hidden-mobile" id="sidebar">
    
    <aside class="sticky-nav sidebar-menu">
<div class="sidebar-search">
  <form class="field" id="searchbox" role="search" action="../search.html" method="get">
    <!-- <label class="label" id="searchlabel">Quick search</label> -->
    <div class="field has-addons">
      <div class="control is-expanded">
        <input class="input" type="text" placeholder="Search the doc" name="q" aria-labelledby="searchlabel autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      </div>
      <div class="control">
        <input class="button is-info" type="submit" value="Go" />
      </div>
    </div>
  </form>
  <script>document.getElementById('searchbox').style.display = "block"</script>

</div>
      
      <div class="sidebar-menu-toc">
      <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installing NeuralOperator</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">User guide</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="quickstart.html">Quick-Start</a></li>
<li class="toctree-l2"><a class="reference internal" href="neural_operators.html">Neural Operators: an Introduction</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Fourier Neural Operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="training.html">Training neural operator models</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../modules/api.html">API reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_examples/index.html">Gallery of examples</a></li>
</ul>
 
      </div>
    </aside>
  </div>
  

  <div class="column main-column">

    
    <div class="main-section">

      
      
      <div class="side-menu-toggle">
        <button class="button" id="toggle-sidebar" onclick="toggle_sidebar()">
          <span class="icon"><i class="fa fa-bars" aria-hidden="true"></i></span>
          <span>menu</span> 
        </button>
      </div>
      

      <div class="container content main-content">
        
  <section id="fourier-neural-operators">
<h1>Fourier Neural Operators<a class="headerlink" href="#fourier-neural-operators" title="Link to this heading"></a></h1>
<p>This page (which takes about 10 minutes to read), introduces the Fourier neural operator that solves a family of PDEs from scratch.
It the first work that can learn resolution-invariant solution operators on Navier-Stokes equation,
achieving state-of-the-art accuracy among all existing deep learning methods and
up to 1000x faster than traditional solvers.
Also check out the paper <a class="footnote-reference brackets" href="#id3" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> and article <a class="footnote-reference brackets" href="#id4" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>.</p>
<section id="operator-learning">
<h2>Operator learning<a class="headerlink" href="#operator-learning" title="Link to this heading"></a></h2>
<p>Thinking in continuum gives us an advantage when dealing with PDE.
We want to design mesh-indepedent, resolution-invariant operators.</p>
<p>Problems in science and engineering involve solving
partial differential equations (PDE) systems.
Unfortunately, these PDEs can be very hard.
Traditional PDE solver such as finite element methods (FEM) and finite difference methods (FDM)
rely on discretizing the space into a very fine mesh.
And it can be slow and inefficient.</p>
<p>In the previous doc,
we introduced the neural operators that use neural networks
to learn the solution operators for PDEs.
That is, given the initial conditions or the boundary conditions,
the neural network directly output the solution,
kind of like an image-to-image mapping.</p>
<p>The neural operator is mesh-independent,
different from the standard deep learning methods such as CNN.
It can be trained on one mesh and evaluated on another.
By parameterizing the model in function space,
it learns the continuous function instead of discretized vectors.</p>
<blockquote>
<div><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Conventional PDE solvers</p></th>
<th class="head"><p>Neural operators</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Solve one instance</p></td>
<td><p>Learn a family of PDE</p></td>
</tr>
<tr class="row-odd"><td><p>Require the explicit form</p></td>
<td><p>Black-box, data-driven</p></td>
</tr>
<tr class="row-even"><td><p>Speed-accuracy trade-off on resolution</p></td>
<td><p>Resolution-invariant, mesh-invariant</p></td>
</tr>
<tr class="row-odd"><td><p>Slow on fine grids; fast on coarse grids</p></td>
<td><p>Slow to train; fast to evaluate</p></td>
</tr>
</tbody>
</table>
</div></blockquote>
<p>Operator learning can be taken as an image-to-image problem.
The Fourier layer can be viewed as a substitute for the convolution layer.</p>
</section>
<section id="framework-of-neural-operators">
<h2>Framework of Neural Operators<a class="headerlink" href="#framework-of-neural-operators" title="Link to this heading"></a></h2>
<p>Just like neural networks consist of linear transformations and non-linear activation functions,
neural operators consist of linear operators and non-linear activation operators.</p>
<p>Let <span class="math notranslate nohighlight">\(v\)</span> be the input vector, <span class="math notranslate nohighlight">\(u\)</span> be the output vector.
A standard deep neural network can be written in the form:</p>
<div class="math notranslate nohighlight">
\[u = \left(K_l \circ \sigma_l \circ \cdots \circ \sigma_1 \circ K_0 \right) v\]</div>
<p>where <span class="math notranslate nohighlight">\(K\)</span> are the linear layer or convolution layer,
and <span class="math notranslate nohighlight">\(\sigma\)</span> are the activation function such as ReLU.</p>
<p>The neural operator shares a similar framework.
It’s just now <span class="math notranslate nohighlight">\(v\)</span> and <span class="math notranslate nohighlight">\(u\)</span> are functions with different discretizations
(say, some inputs are <span class="math notranslate nohighlight">\(28 \times 28\)</span>, some are <span class="math notranslate nohighlight">\(256 \times 256\)</span>,
and some are in triangular mesh).
To deal with functions input, the linear transformation <span class="math notranslate nohighlight">\(K\)</span> is formulated as an integral operator.
Let <span class="math notranslate nohighlight">\(x, y\)</span> be the points in the domain.</p>
<p>The map <span class="math notranslate nohighlight">\(K: v_{t} \mapsto v_{t+1}\)</span> is parameterized as</p>
<div class="math notranslate nohighlight">
\[v'(x) = \int \kappa(x,y) v(y) dy + W v(x)\]</div>
<p>Where <span class="math notranslate nohighlight">\(\kappa\)</span> is a kernel function and <span class="math notranslate nohighlight">\(W\)</span> is the bias term.</p>
<p>For the Fourier neural operator, we formulate <span class="math notranslate nohighlight">\(K\)</span> as a convolution
and implement it by Fourier transformation.</p>
</section>
<section id="fourier-layer">
<h2>Fourier Layer<a class="headerlink" href="#fourier-layer" title="Link to this heading"></a></h2>
<p>The real-world images have lots of edges and shapes,
so CNN can capture them well with local convolution kernel.
On the other hand, the inputs and outputs of PDEs are continuous functions.
It is more efficient to represent them in Fourier space and do global convolution.</p>
<p>There are two main motivations to use Fourier transformation.
First, it’s fast. A full standard integration of <span class="math notranslate nohighlight">\(n\)</span> points has complexity <span class="math notranslate nohighlight">\(O(n^2)\)</span>,
while convolution via Fourier transform is quasilinear.
Second, it’s efficient. The inputs and outputs of PDEs are continuous functions.
So it’s usually more efficient to represent them in Fourier space.</p>
<p>The convolution in the spatial domain is equivalent to the pointwise multiplication in the Fourier domain. To implement the (global) convolution operator,
we first do a Fourier transform, then a linear transform, and an inverse Fourier transform,
As shown in the top part of the figure:</p>
<a class="reference internal image-reference" href="../_images/fourier_layer.jpg"><img alt="../_images/fourier_layer.jpg" src="../_images/fourier_layer.jpg" style="width: 800px;" />
</a>
<dl class="simple">
<dt>The Fourier layer just consists of three steps:</dt><dd><ul class="simple">
<li><p>Fourier transform <span class="math notranslate nohighlight">\(\mathcal{F}\)</span></p></li>
<li><p>Linear transform on the lower Fourier modes <span class="math notranslate nohighlight">\(R\)</span></p></li>
<li><p>Inverse Fourier transform <span class="math notranslate nohighlight">\(\mathcal{F}^{-1}\)</span></p></li>
</ul>
</dd>
</dl>
<p>We then add the output of the Fourier layer
with the bias term <span class="math notranslate nohighlight">\(W v\)</span> (a linear transformation)
and apply the activation function <span class="math notranslate nohighlight">\(\sigma\)</span>.
Simple as it is.</p>
<p>In practice, it’s usually sufficient to only take the lower frequency modes
and truncate out these higher frequency modes.
Therefore, we apply the linear transformation on the lower frequency modes
and set the higher modes to zeros.</p>
<p>Notice the activation functions shall be applied on the spatial domain.
They help to recover the Higher frequency modes and non-periodic boundary
which are left out in the Fourier layers.
Therefore it’s necessary to the Fourier transform and its inverse at each layer.</p>
</section>
<section id="implementation">
<h2>Implementation<a class="headerlink" href="#implementation" title="Link to this heading"></a></h2>
<p>We can easily create a 2d Fourier layer using <cite>neuralop</cite> as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">neuralop.models.spectral_convolution</span> <span class="kn">import</span> <span class="n">FactorizedSpectralConv</span>
<span class="n">fourier_layer</span> <span class="o">=</span> <span class="n">FactorizedSpectralConv</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">n_modes</span><span class="o">=</span><span class="p">(</span><span class="n">modes1</span><span class="p">,</span> <span class="n">modes2</span><span class="p">))</span>
</pre></div>
</div>
<p>To illustrate the implementation details of the Fourier layer, we provide a simple implementation from scratch that is equivalent to the above code based on PyTorch’s fast Fourier transform <code class="code docutils literal notranslate"><span class="pre">torch.fft.rfft()</span></code> and <code class="code docutils literal notranslate"><span class="pre">torch.fft.irfft()</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">class</span> <span class="nc">SpectralConv2d</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_channels</span><span class="p">,</span>   <span class="c1"># Number of input channels</span>
                 <span class="n">out_channels</span><span class="p">,</span>  <span class="c1"># Number of output channels</span>
                 <span class="n">modes1</span><span class="p">,</span>        <span class="c1"># Number of Fourier modes to multiply in the first dimension</span>
                 <span class="n">modes2</span><span class="p">):</span>       <span class="c1"># Number of Fourier modes to multiply in the second dimension</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SpectralConv2d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">in_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">modes1</span> <span class="o">=</span> <span class="n">modes1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">modes2</span> <span class="o">=</span> <span class="n">modes2</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">in_channels</span> <span class="o">*</span> <span class="n">out_channels</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">modes1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">modes2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cfloat</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">modes1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">modes2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cfloat</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">batchsize</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1">#Compute Fourier coeffcients</span>
        <span class="n">x_ft</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">rfft2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Multiply relevant Fourier modes</span>
        <span class="n">out_ft</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batchsize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span>  <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">//</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cfloat</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">out_ft</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">modes1</span><span class="p">,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">modes2</span><span class="p">]</span> <span class="o">=</span> \
            <span class="bp">self</span><span class="o">.</span><span class="n">compl_mul2d</span><span class="p">(</span><span class="n">x_ft</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">modes1</span><span class="p">,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">modes2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights1</span><span class="p">)</span>
        <span class="n">out_ft</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">modes1</span><span class="p">:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">modes2</span><span class="p">]</span> <span class="o">=</span> \
            <span class="bp">self</span><span class="o">.</span><span class="n">compl_mul2d</span><span class="p">(</span><span class="n">x_ft</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">modes1</span><span class="p">:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">modes2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights2</span><span class="p">)</span>

        <span class="c1">#Return to physical space</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">irfft2</span><span class="p">(</span><span class="n">out_ft</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">compl_mul2d</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
        <span class="c1"># (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -&gt; (batch, out_channel, x,y)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bixy,ioxy-&gt;boxy&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
</pre></div>
</div>
<p>where the input <code class="code docutils literal notranslate"><span class="pre">x</span></code> has the shape (N,C,H,W),
<code class="code docutils literal notranslate"><span class="pre">self.weights1</span></code> and <code class="code docutils literal notranslate"><span class="pre">self.weights2</span></code> are the weight matrices;
<code class="code docutils literal notranslate"><span class="pre">self.mode1</span></code> and <code class="code docutils literal notranslate"><span class="pre">self.mode2</span></code> truncate the lower frequency modes;
and <code class="code docutils literal notranslate"><span class="pre">compl_mul2d()</span></code> is the matrix multiplication for complex numbers.</p>
<a class="reference internal image-reference" href="../_images/filters.jpg"><img alt="../_images/filters.jpg" src="../_images/filters.jpg" style="width: 800px;" />
</a>
<p>Filters in convolution neural networks are usually local.
They are good to capture local patterns such as edges and shapes.
Fourier filters are global sinusoidal functions.
They are better for representing continuous functions.</p>
<p><strong>Higher frequency modes and non-periodic boundary</strong>
The Fourier layer on its own loses higher frequency modes
and works only with periodic boundary conditions.
However, the Fourier neural operator as a whole does not have these limitations
(examples shown in the experiments).
The encoder-decoder structure
helps to recover the higher Fourier modes.
And the bias term <span class="math notranslate nohighlight">\(W\)</span>
helps to recover the non-periodic boundary.</p>
<p><strong>Complexity</strong>
The Fourier layer has a quasilinear complexity.
Denote the number of points (pixels) <span class="math notranslate nohighlight">\(n\)</span> and truncating at <span class="math notranslate nohighlight">\(k_{max}\)</span> frequency modes.
The multiplication has complexity <span class="math notranslate nohighlight">\(% &lt;![CDATA[
O(k_{max}) &lt; O(n) %]]&gt;\)</span> .
The majority of the computational cost lies in computing the Fourier transform and its inverse.
General Fourier transforms have complexity <span class="math notranslate nohighlight">\(O(n^2)\)</span>,
however, since we truncate the series the complexity is in fact <span class="math notranslate nohighlight">\(O(n k_{max})\)</span>,
while the FFT has complexity <span class="math notranslate nohighlight">\(O(n \log n)\)</span>.</p>
<p><strong>resolution-invariance”&gt;Resolution-invariance</strong>
The Fourier layers are discretization-invariant,
because they can learn from and evaluate functions
which are discretized in an arbitrary way.
Since parameters are learned directly in Fourier space,
resolving the functions in physical space simply amounts to projecting on the basis
of wave functions which are well-defined everywhere on the space.
This allows us to transfer among discretization.
If implemented with standard FFT, then it will be restricted to uniform mesh,
but still resolution-invariant.</p>
</section>
<section id="experiments">
<h2>Experiments<a class="headerlink" href="#experiments" title="Link to this heading"></a></h2>
<p><strong>Burgers Equation</strong>
The 1-d Burgers’ equation is a non-linear PDE with various applications
including modeling the one-dimensional flow of a viscous fluid. It takes the form</p>
<div class="math notranslate nohighlight">
\[\partial_t u(x,t) + \partial_x ( u^2(x,t)/2) = \nu \partial_{xx} u(x,t), \qquad x \in (0,1), t \in (0,1]\]</div>
<div class="math notranslate nohighlight">
\[u(x,0) = u_0(x), \qquad \qquad \:\: x \in (0,1)\]</div>
<p>with periodic boundary conditions where <span class="math notranslate nohighlight">\(u_0 \in L^2_{\text{per}}((0,1);\mathbb{R})\)</span>
is the initial condition and <span class="math notranslate nohighlight">\(\nu \in \mathbb{R}_+\)</span> is the viscosity coefficient.
We aim to learn the operator mapping the initial condition to the solution
at time one, defined by <span class="math notranslate nohighlight">\(u_0 \mapsto u(\cdot, 1)\)</span> for any <span class="math notranslate nohighlight">\(r &gt; 0\)</span>.</p>
<blockquote>
<div><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Networks</p></th>
<th class="head"><p>s=256</p></th>
<th class="head"><p>s=512</p></th>
<th class="head"><p>s=1024</p></th>
<th class="head"><p>s=2048</p></th>
<th class="head"><p>s=4096</p></th>
<th class="head"><p>s=8192</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>FCN</p></td>
<td><p>0.0958</p></td>
<td><p>0.1407</p></td>
<td><p>0.1877</p></td>
<td><p>0.2313</p></td>
<td><p>0.2855</p></td>
<td><p>0.3238</p></td>
</tr>
<tr class="row-odd"><td><p>PCA+NN</p></td>
<td><p>0.0398</p></td>
<td><p>0.0395</p></td>
<td><p>0.0391</p></td>
<td><p>0.0383</p></td>
<td><p>0.0392</p></td>
<td><p>0.0393</p></td>
</tr>
<tr class="row-even"><td><p>LNO</p></td>
<td><p>0.0212</p></td>
<td><p>0.0221</p></td>
<td><p>0.0217</p></td>
<td><p>0.0219</p></td>
<td><p>0.0200</p></td>
<td><p>0.0189</p></td>
</tr>
<tr class="row-odd"><td><p>FNO</p></td>
<td><p>0.0149</p></td>
<td><p>0.0158</p></td>
<td><p>0.0160</p></td>
<td><p>0.0146</p></td>
<td><p>0.0142</p></td>
<td><p>0.0139</p></td>
</tr>
</tbody>
</table>
</div></blockquote>
<p><strong>Darcy Flow</strong></p>
<p>We consider the steady-state of the 2-d Darcy Flow equation
on the unit box which is the second order, linear, elliptic PDE</p>
<div class="math notranslate nohighlight">
\[- \nabla \cdot (a(x) \nabla u(x)) = f(x) \qquad x \in (0,1)^2\]</div>
<div class="math notranslate nohighlight">
\[u(x) = 0 \qquad \quad \:\:x \in \partial (0,1)^2\]</div>
<p>with a Dirichlet boundary where <span class="math notranslate nohighlight">\(a \in L^\infty((0,1)^2;\mathbb{R}_+)\)</span>  is the diffusion coefficient and <span class="math notranslate nohighlight">\(f \in L^2((0,1)^2;\mathbb{R})\)</span> is the forcing function.
This PDE has numerous applications including modeling the pressure of the subsurface flow,
the deformation of linearly elastic materials, and the electric potential in conductive materials.
We are interested in learning the operator mapping the diffusion coefficient to the solution,
defined by <span class="math notranslate nohighlight">\(a \mapsto u\)</span>. Note that although the PDE is linear, the solution operator is not.</p>
<blockquote>
<div><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Networks</p></th>
<th class="head"><p>s=85</p></th>
<th class="head"><p>s=141</p></th>
<th class="head"><p>s=211</p></th>
<th class="head"><p>s=421</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>FCN</p></td>
<td><p>0.0253</p></td>
<td><p>0.0493</p></td>
<td><p>0.0727</p></td>
<td><p>0.1097</p></td>
</tr>
<tr class="row-odd"><td><p>PCA+NN</p></td>
<td><p>0.0299</p></td>
<td><p>0.0298</p></td>
<td><p>0.0298</p></td>
<td><p>0.0299</p></td>
</tr>
<tr class="row-even"><td><p>RBM</p></td>
<td><p>0.0244</p></td>
<td><p>0.0251</p></td>
<td><p>0.0255</p></td>
<td><p>0.0259</p></td>
</tr>
<tr class="row-odd"><td><p>LNO</p></td>
<td><p>0.0520</p></td>
<td><p>0.0461</p></td>
<td><p>0.0445</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p>FNO</p></td>
<td><p>0.0108</p></td>
<td><p>0.0109</p></td>
<td><p>0.0109</p></td>
<td><p>0.0098</p></td>
</tr>
</tbody>
</table>
</div></blockquote>
<a class="reference internal image-reference" href="../_images/fourier_error.jpg"><img alt="../_images/fourier_error.jpg" src="../_images/fourier_error.jpg" style="width: 800px;" />
</a>
<p>Benchmarks for time-independent problems (Burgers and Darcy):</p>
<blockquote>
<div><ul class="simple">
<li><p>NN: a simple point-wise feedforward neural network.</p></li>
<li><p>RBM: the classical Reduced Basis Method (using a POD basis).</p></li>
<li><p>FCN: a the-state-of-the-art neural network architecture based on Fully Convolution Networks.</p></li>
<li><p>PCANN: an operator method using PCA as an autoencoder on both the input and output data and interpolating the latent spaces with a neural network.</p></li>
<li><p>GNO: the original graph neural operator.</p></li>
<li><p>MGNO: the multipole graph neural operator.</p></li>
<li><p>LNO: a neural operator method based on the low-rank decomposition of the kernel.</p></li>
<li><p>FNO: the newly purposed Fourier neural operator.</p></li>
</ul>
</div></blockquote>
<p><strong>Navier-Stokes Equation</strong></p>
<p>We consider the 2-d Navier-Stokes equation for a viscous,
incompressible fluid in vorticity form on the unit torus:</p>
<div class="math notranslate nohighlight">
\[\partial_t w(x,t) + u(x,t) \cdot \nabla w(x,t) = \nu \Delta w(x,t) + f(x), \qquad x \in (0,1)^2, t \in (0,T]\]</div>
<div class="math notranslate nohighlight">
\[\nabla \cdot u(x,t) = 0, \qquad \qquad  x \in (0,1)^2, t \in [0,T]\]</div>
<div class="math notranslate nohighlight">
\[w(x,0) = w_0(x), \qquad \qquad \qquad  x \in (0,1)^2\]</div>
<p>where <span class="math notranslate nohighlight">\(u\)</span> is the velocity field,
<span class="math notranslate nohighlight">\(w = \nabla \times u\)</span> is the vorticity,
<span class="math notranslate nohighlight">\(w_0\)</span> is the initial vorticity,&lt;br /&gt;
<span class="math notranslate nohighlight">\(\nu\)</span> is the viscosity coefficient,
and <span class="math notranslate nohighlight">\(f\)</span> is the forcing function.
We are interested in learning the operator mapping the vorticity up to time 10
to the vorticity up to some later time <span class="math notranslate nohighlight">\(T &gt; 10\)</span>,
defined by <span class="math notranslate nohighlight">\(w|_{(0,1)^2 \times [0,10]} \mapsto w|_{(0,1)^2 \times (10,T]}\)</span>.
We experiment with the viscosities
<span class="math notranslate nohighlight">\(\nu = 1\mathrm{e}{-3}, 1\mathrm{e}{-4}, 1\mathrm{e}{-5}\)</span>,
decreasing the final time <span class="math notranslate nohighlight">\(T\)</span> as the dynamic becomes chaotic.</p>
<blockquote>
<div><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Configs</p></th>
<th class="head"><p>Parameters</p></th>
<th class="head"><p>Time per epoch</p></th>
<th class="head"><p>nu=1e-3</p></th>
<th class="head"><p>nu=1e-4</p></th>
<th class="head"><p>nu=1e-5</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>FNO-3D</p></td>
<td><p>6,558,537</p></td>
<td><p>38.99s</p></td>
<td><p>0.0086</p></td>
<td><p>0.0820</p></td>
<td><p>0.1893</p></td>
</tr>
<tr class="row-odd"><td><p>FNO-2D</p></td>
<td><p>414,517</p></td>
<td><p>127.80s</p></td>
<td><p>0.0128</p></td>
<td><p>0.0973</p></td>
<td><p>0.1556</p></td>
</tr>
<tr class="row-even"><td><p>U-Net</p></td>
<td><p>24,950,491</p></td>
<td><p>48.67s</p></td>
<td><p>0.0245</p></td>
<td><p>0.1190</p></td>
<td><p>0.1982</p></td>
</tr>
<tr class="row-odd"><td><p>TF-Net</p></td>
<td><p>7,451,724</p></td>
<td><p>47.21s</p></td>
<td><p>0.0225</p></td>
<td><p>0.1168</p></td>
<td><p>0.2268</p></td>
</tr>
<tr class="row-even"><td><p>ResNet</p></td>
<td><p>266,641</p></td>
<td><p>78.47s</p></td>
<td><p>0.0701</p></td>
<td><p>0.2311</p></td>
<td><p>0.2753</p></td>
</tr>
</tbody>
</table>
</div></blockquote>
<a class="reference internal image-reference" href="../_images/fourier_ns1e4.jpg"><img alt="../_images/fourier_ns1e4.jpg" src="../_images/fourier_ns1e4.jpg" style="width: 800px;" />
</a>
<p>Benchmarks for time-dependent problems (Navier-Stokes):</p>
<blockquote>
<div><ul class="simple">
<li><p>ResNet: 18 layers of 2-d convolution with residual connections.</p></li>
<li><p>U-Net: A popular choice for image-to-image regression tasks consisting of four blocks with 2-d convolutions and deconvolutions.</p></li>
<li><p>TF-Net: A network designed for learning turbulent flows based on a combination of spatial and temporal convolutions.</p></li>
<li><p>FNO-2d: 2-d Fourier neural operator with an RNN structure in time.</p></li>
<li><p>FNO-3d: 3-d Fourier neural operator that directly convolves in space-time.</p></li>
</ul>
</div></blockquote>
<p>The FNO-3D has the best performance
when there is sufficient data
(<span class="math notranslate nohighlight">\(\nu=1\mathrm{e}{-3}, N=1000\)</span> and <span class="math notranslate nohighlight">\(\nu=1\mathrm{e}{-4}, N=10000\)</span>).
For the configurations where the amount of data is insufficient
(<span class="math notranslate nohighlight">\(\nu=1\mathrm{e}{-4}, N=1000\)</span> and <span class="math notranslate nohighlight">\(\nu=1\mathrm{e}{-5}, N=1000\)</span>),
all methods have <span class="math notranslate nohighlight">\(&gt;15\%\)</span> error with FNO-2D achieving the lowest.
Note that we only present results for spatial resolution <span class="math notranslate nohighlight">\(64 \times 64\)</span>
since all benchmarks we compare against are designed for this resolution.
Increasing it degrades their performance while FNO achieves the same errors.</p>
<p>FNO-2D, U-Net, TF-Net, and ResNet all use 2D-convolution in the spatial domain
and recurrently propagate in the time domain (2D+RNN).
On the other hand, FNO-3D performs convolution in space-time.</p>
<p><strong>Bayesian Inverse Problem</strong></p>
<p>In this experiment, we use a function space Markov chain Monte Carlo (MCMC) method
to draw samples from the posterior distribution of the initial vorticity
in Navier-Stokes given sparse, noisy observations at time <span class="math notranslate nohighlight">\(T=50\)</span>.
We compare the Fourier neural operator acting as a surrogate model
with the traditional solvers used to generate our train-test data (both run on GPU).
We generate 25,000 samples from the posterior (with a 5,000 sample burn-in period),
requiring 30,000 evaluations of the forward operator.</p>
<a class="reference internal image-reference" href="../_images/fourier_bayesian.jpg"><img alt="../_images/fourier_bayesian.jpg" src="../_images/fourier_bayesian.jpg" style="width: 800px;" />
</a>
<p>The top left panel shows the true initial vorticity while the bottom left panel shows
the true observed vorticity at <span class="math notranslate nohighlight">\(T=50\)</span> with black dots indicating
the locations of the observation points placed on a <span class="math notranslate nohighlight">\(7 \times 7\)</span> grid.
The top middle panel shows the posterior mean of the initial vorticity
given the noisy observations estimated with MCMC using the traditional solver,
while the top right panel shows the same thing but using FNO as a surrogate model.
The bottom middle and right panels show the vorticity at <span class="math notranslate nohighlight">\(T=50\)</span>
when the respective approximate posterior means are used as initial conditions.</p>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading"></a></h2>
<p>We propose a neural operator based on Fourier Transformation.
It is the first work that learns the resolution-invariant solution operator
for the family of Navier-Stokes equation in the turbulent regime,
where previous graph-based neural operators do not converge.
By construction, the method shares the same learned network parameters
irrespective of the dis- cretization used on the input and output spaces.
It can do zero-shot super-resolution: trained on a lower resolution
directly evaluated on a higher resolution.
The proposed method consistently outperforms all existing deep learning methods for parametric PDEs.
It achieves error rates that are <span class="math notranslate nohighlight">\(30\%\)</span> lower on Burgers’ Equation,
<span class="math notranslate nohighlight">\(60\%\)</span> lower on Darcy Flow, and <span class="math notranslate nohighlight">\(30\%\)</span> lower on Navier Stokes
(turbulent regime with Reynolds number <span class="math notranslate nohighlight">\(10000\)</span>).
On a <span class="math notranslate nohighlight">\(256 \times 256\)</span> grid,
the Fourier neural operator has an inference time of only <span class="math notranslate nohighlight">\(0.005\)</span>
compared to the <span class="math notranslate nohighlight">\(2.2s\)</span> of the pseudo-spectral method used to solve Navier-Stokes.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id3" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Fourier Neural Operator for Parametric Partial Differential Equations,
Zongyi Li and Nikola Kovachki and Kamyar Azizzadenesheli
and Burigede Liu and Kaushik Bhattacharya and Andrew Stuart and Anima Anandkumar, 2020.</p>
</aside>
<aside class="footnote brackets" id="id4" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>Hao, K. (2021, October 20). Ai has cracked a key mathematical puzzle for understanding our world.
MIT Technology Review. <a class="reference external" href="https://www.technologyreview.com/2020/10/30/1011435/ai-fourier-neural-network-cracks-navier-stokes-and-partial-differential-equations/">https://www.technologyreview.com/2020/10/30/1011435/ai-fourier-neural-network-cracks-navier-stokes-and-partial-differential-equations/</a></p>
</aside>
</aside>
</section>
</section>


      </div>

      
        <nav class="pagination" role="navigation" aria-label="pagination">
    
    <a class="button pagination-previous" href="neural_operators.html" title="previous page" accesskey="p">
        <span class="icon">
            <i class="fa fa-arrow-circle-left"></i>
        </span>
        <span>Neural Operators: an Introduction</span>
    </a>
    
    
    <a class="button pagination-next" href="training.html" title="next page" accesskey="n">
        <span>Training neural operator models </span>
        <span class="icon">
            <i class="fa fa-arrow-circle-right"></i>
        </span>
    </a>
    
</nav>

      

        <footer class="footer">
    <div class="content has-text-centered">
        <div class="block">
          &copy; Copyright 2024, Jean Kossaifi, David Pitt, Nikola Kovachki, Zongyi Li and Anima Anandkumar.<br/>
        </div>
    </div>
  </footer>

    </div>

  </div>  

	
    
    <div class="column is-hidden-touch is-2-desktop is-one-fifth-widescreen" id="localtoc-column">

    <aside class="sticky-nav localtoc"> 
        <p class="menu-label"> 
            <span class="icon-text">
                <span class="icon"><i class="fas fa-duotone fa-list"></i></span>
                <span> On this page </span>
            </span>
        </p>

        <div class="menu menu-list localtoc-list">
        <ul>
<li><a class="reference internal" href="#">Fourier Neural Operators</a><ul>
<li><a class="reference internal" href="#operator-learning">Operator learning</a></li>
<li><a class="reference internal" href="#framework-of-neural-operators">Framework of Neural Operators</a></li>
<li><a class="reference internal" href="#fourier-layer">Fourier Layer</a></li>
<li><a class="reference internal" href="#implementation">Implementation</a></li>
<li><a class="reference internal" href="#experiments">Experiments</a></li>
<li><a class="reference internal" href="#conclusion">Conclusion</a></li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

        </div>
    </aside>
    </div>

  

  </div>  
  </div> 

  
  <script>
    function toggle_sidebar() {
        var element = document.getElementById("sidebar");
        var container = document.getElementById("column-container");
        var localtoccolumn = document.getElementById("localtoc-column");
        element.classList.toggle("hide-tablet");
        element.classList.toggle("is-hidden-mobile");
        container.classList.toggle("sidemenu-hidden");
        localtoccolumn.classList.toggle("is-one-fifth-widescreen");
        localtoccolumn.classList.toggle("is-2-desktop");
        localtoccolumn.classList.toggle("is-3-desktop");
    }
  </script> 



  </body>
</html>