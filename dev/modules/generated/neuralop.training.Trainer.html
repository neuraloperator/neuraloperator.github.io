<!DOCTYPE html>

<html lang="en" data-content_root="../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>neuralop.training.Trainer &#8212; neuraloperator 1.0.2 documentation</title> 
<link rel="stylesheet" href="../../_static/tensorly_style.css">
<link rel="apple-touch-icon" sizes="180x180" href="../../_static/favicon/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../_static/favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="../../_static/favicon/favicon-16x16.png">
<link rel="manifest" href="../../_static/favicon/site.webmanifest">
<link rel="mask-icon" href="../../_static/favicon/safari-pinned-tab.svg" color="#5bbad5">
<link rel="shortcut icon" href="../../_static/favicon/favicon.ico">
<meta name="theme-color" content="#ffffff">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../_static/tensorly_style.css?v=a02e9698" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />

  
    <script src="../../_static/documentation_options.js?v=1ed6394b"></script>
    <script src="../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
 <script src="../../_static/navbar_burger.js"></script>
 <script defer src="https://use.fontawesome.com/releases/v5.14.0/js/all.js"></script>
 
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="neuralop.training.IncrementalFNOTrainer" href="neuralop.training.IncrementalFNOTrainer.html" />
    <link rel="prev" title="neuralop.models.base_model.available_models" href="neuralop.models.base_model.available_models.html" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

  </head>
<body  class="has-navbar-fixed-top">

  <header>
    <navbar>
      <nav class="navbar top-navbar is-fixed-top has-shadow is-flex-wrap-wrap" role="navigation" aria-label="main top navigation">
        <div class="navbar-brand">
        

          <a class="navbar-item" href="../../index.html">
            <img src="../../_static/neuraloperator_logo.png" height="28">
          </a>
          <a class="navbar-item is-hidden-desktop" href="https://github.com/neuraloperator/neuraloperator" target="_blank">
              <span class="icon"><i class="fab fa-github"></i></span>
          </a>

          <a role="button" class="navbar-burger" data-target="top-nav-menu" aria-label="menu" aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
          </a>

        </div>
        
        <div class="navbar-menu" id="top-nav-menu">
        

          <div class="navbar-start">
            
              <a class="navbar-item" href="../../install.html">
              Install
            </a>
              <a class="navbar-item" href="../../theory_guide/index.html">
              Theory Guide
            </a>
              <a class="navbar-item" href="../../user_guide/index.html">
              User Guide
            </a>
              <a class="navbar-item" href="../api.html">
              API
            </a>
              <a class="navbar-item" href="../../auto_examples/index.html">
              Examples
            </a>
              <a class="navbar-item" href="../../dev_guide/index.html">
              Developer's Guide
            </a>
          </div>
        
          <div class="navbar-end">
            <div class="navbar-item">
            
            <a class="button is-hidden-touch is-dark" href="https://github.com/neuraloperator/neuraloperator" target="_blank">
              <span class="icon-text">
                <span class="icon is-large">
                  <i class="fab fa-github"></i>
                </span>
                <span>Github</span>
              </span>
            </a>

            </div> 
          </div> 
        </div> 

      </nav>
      
    </navbar>
  </header>


  <div id="column-container">
  <div class="columns is-mobile is-centered">
	
  
      <div class="column is-10-mobile is-one-third-tablet is-3-desktop is-hidden-mobile" id="sidebar">
    
    <aside class="sticky-nav sidebar-menu">
<div class="sidebar-search">
  <form class="field" id="searchbox" role="search" action="../../search.html" method="get">
    <!-- <label class="label" id="searchlabel">Quick search</label> -->
    <div class="field has-addons">
      <div class="control is-expanded">
        <input class="input" type="text" placeholder="Search the doc" name="q" aria-labelledby="searchlabel autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      </div>
      <div class="control">
        <input class="button is-info" type="submit" value="Go" />
      </div>
    </div>
  </form>
  <script>document.getElementById('searchbox').style.display = "block"</script>

</div>
      
      <div class="sidebar-menu-toc">
      <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../install.html">Installing NeuralOperator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../theory_guide/index.html">Theory Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user_guide/index.html">User Guide</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../api.html">API reference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../api.html#models">Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#module-neuralop.layers">Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#model-dispatching">Model Dispatching</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../api.html#training">Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#data">Data</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../auto_examples/index.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dev_guide/index.html">Development guide</a></li>
</ul>
 
      </div>
    </aside>
  </div>
  

  <div class="column main-column">

    
    <div class="main-section">

      
      
      <div class="side-menu-toggle">
        <button class="button" id="toggle-sidebar" onclick="toggle_sidebar()">
          <span class="icon"><i class="fa fa-bars" aria-hidden="true"></i></span>
          <span>menu</span> 
        </button>
      </div>
      

      <div class="container content main-content">
        
  <section id="neuralop-training-trainer">
<h1><a class="reference internal" href="../api.html#module-neuralop.training" title="neuralop.training"><code class="xref py py-mod docutils literal notranslate"><span class="pre">neuralop.training</span></code></a>.Trainer<a class="headerlink" href="#neuralop-training-trainer" title="Link to this heading"></a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="neuralop.training.Trainer">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neuralop.training.</span></span><span class="sig-name descname"><span class="pre">Trainer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_epochs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wandb_log</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'cpu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mixed_precision</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_processor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_interval</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_output</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_distributed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/neuralop/training/trainer.html#Trainer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neuralop.training.Trainer" title="Link to this definition"></a></dt>
<dd><p>A general Trainer class to train neural-operators on given datasets.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Our Trainer expects datasets to provide batches as key-value dictionaries, ex.: 
<code class="docutils literal notranslate"><span class="pre">{'x':</span> <span class="pre">x,</span> <span class="pre">'y':</span> <span class="pre">y}</span></code>, that are keyed to the arguments expected by models and losses. 
For specifics and an example, check <code class="docutils literal notranslate"><span class="pre">neuralop.data.datasets.DarcyDataset</span></code>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>model</strong><span class="classifier">nn.Module</span></dt><dd></dd>
<dt><strong>n_epochs</strong><span class="classifier">int</span></dt><dd></dd>
<dt><strong>wandb_log</strong><span class="classifier">bool, default is False</span></dt><dd><p>whether to log results to wandb</p>
</dd>
<dt><strong>device</strong><span class="classifier">torch.device, or str ‘cpu’ or ‘cuda’</span></dt><dd></dd>
<dt><strong>mixed_precision</strong><span class="classifier">bool, default is False</span></dt><dd><p>whether to use torch.autocast to compute mixed precision</p>
</dd>
<dt><strong>data_processor</strong><span class="classifier">DataProcessor class to transform data, default is None</span></dt><dd><p>if not None, data from the loaders is transform first with data_processor.preprocess,
then after getting an output from the model, that is transformed with data_processor.postprocess.</p>
</dd>
<dt><strong>eval_interval</strong><span class="classifier">int, default is 1</span></dt><dd><p>how frequently to evaluate model and log training stats</p>
</dd>
<dt><strong>log_output</strong><span class="classifier">bool, default is False</span></dt><dd><p>if True, and if wandb_log is also True, log output images to wandb</p>
</dd>
<dt><strong>use_distributed</strong><span class="classifier">bool, default is False</span></dt><dd><p>whether to use DDP</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">bool, default is False</span></dt><dd></dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#neuralop.training.Trainer.checkpoint" title="neuralop.training.Trainer.checkpoint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">checkpoint</span></code></a>(save_dir)</p></td>
<td><p>checkpoint saves current training state to a directory for resuming later.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#neuralop.training.Trainer.eval_one_batch" title="neuralop.training.Trainer.eval_one_batch"><code class="xref py py-obj docutils literal notranslate"><span class="pre">eval_one_batch</span></code></a>(sample, eval_losses[, ...])</p></td>
<td><p>eval_one_batch runs inference on one batch and returns eval_losses for that batch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#neuralop.training.Trainer.eval_one_batch_autoreg" title="neuralop.training.Trainer.eval_one_batch_autoreg"><code class="xref py py-obj docutils literal notranslate"><span class="pre">eval_one_batch_autoreg</span></code></a>(sample, eval_losses)</p></td>
<td><p>eval_one_batch runs inference on one batch and returns eval_losses for that batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#neuralop.training.Trainer.evaluate" title="neuralop.training.Trainer.evaluate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluate</span></code></a>(loss_dict, data_loader[, ...])</p></td>
<td><p>Evaluates the model on a dictionary of losses</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#neuralop.training.Trainer.evaluate_all" title="neuralop.training.Trainer.evaluate_all"><code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluate_all</span></code></a>(epoch, eval_losses, ...[, ...])</p></td>
<td><p>evaluate_all iterates through the entire dict of test_loaders to perform evaluation on the whole dataset stored in each one.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#neuralop.training.Trainer.log_eval" title="neuralop.training.Trainer.log_eval"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log_eval</span></code></a>(epoch, eval_metrics)</p></td>
<td><p>log_eval logs outputs from evaluation on all test loaders to stdout and wandb</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#neuralop.training.Trainer.log_training" title="neuralop.training.Trainer.log_training"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log_training</span></code></a>(epoch, time, avg_loss, train_err)</p></td>
<td><p>Basic method to log results from a single training epoch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#neuralop.training.Trainer.on_epoch_start" title="neuralop.training.Trainer.on_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_epoch_start</span></code></a>(epoch)</p></td>
<td><p>on_epoch_start runs at the beginning of each training epoch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#neuralop.training.Trainer.resume_state_from_dir" title="neuralop.training.Trainer.resume_state_from_dir"><code class="xref py py-obj docutils literal notranslate"><span class="pre">resume_state_from_dir</span></code></a>(save_dir)</p></td>
<td><p>Resume training from save_dir created by <cite>neuralop.training.save_training_state</cite></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#neuralop.training.Trainer.train" title="neuralop.training.Trainer.train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">train</span></code></a>(train_loader, test_loaders, optimizer, ...)</p></td>
<td><p>Trains the given model on the given dataset.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#neuralop.training.Trainer.train_one_batch" title="neuralop.training.Trainer.train_one_batch"><code class="xref py py-obj docutils literal notranslate"><span class="pre">train_one_batch</span></code></a>(idx, sample, training_loss)</p></td>
<td><p>Run one batch of input through model</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#neuralop.training.Trainer.train_one_epoch" title="neuralop.training.Trainer.train_one_epoch"><code class="xref py py-obj docutils literal notranslate"><span class="pre">train_one_epoch</span></code></a>(epoch, train_loader, ...)</p></td>
<td><p>train_one_epoch trains self.model on train_loader for one epoch and returns training metrics</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="neuralop.training.Trainer.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_loader</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_loaders</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scheduler</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">regularizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_losses</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_modes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_every</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_best</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Path</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'./ckpt'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">resume_from_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Path</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_autoregressive_steps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/neuralop/training/trainer.html#Trainer.train"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neuralop.training.Trainer.train" title="Link to this definition"></a></dt>
<dd><p>Trains the given model on the given dataset.</p>
<p>If a device is provided, the model and data processor are loaded to device here.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>train_loader: torch.utils.data.DataLoader</strong></dt><dd><p>training dataloader</p>
</dd>
<dt><strong>test_loaders: dict[torch.utils.data.DataLoader]</strong></dt><dd><p>testing dataloaders</p>
</dd>
<dt><strong>optimizer: torch.optim.Optimizer</strong></dt><dd><p>optimizer to use during training</p>
</dd>
<dt><strong>scheduler: torch.optim.lr_scheduler</strong></dt><dd><p>learning rate scheduler to use during training</p>
</dd>
<dt><strong>training_loss: training.losses function</strong></dt><dd><p>cost function to minimize</p>
</dd>
<dt><strong>eval_losses: dict[Loss]</strong></dt><dd><p>dict of losses to use in self.eval()</p>
</dd>
<dt><strong>eval_modes: dict[str], optional</strong></dt><dd><p>optional mapping from the name of each loader to its evaluation mode.</p>
<ul class="simple">
<li><p>if ‘single_step’, predicts one input-output pair and evaluates loss.</p></li>
<li><p>if ‘autoregressive’, autoregressively predicts output using last step’s</p></li>
</ul>
<p>output as input for a number of steps defined by the temporal dimension of the batch.
This requires specially batched data with a data processor whose <code class="docutils literal notranslate"><span class="pre">.preprocess</span></code> and 
<code class="docutils literal notranslate"><span class="pre">.postprocess</span></code> both take <code class="docutils literal notranslate"><span class="pre">idx</span></code> as an argument.</p>
</dd>
<dt><strong>save_every: int, optional, default is None</strong></dt><dd><p>if provided, interval at which to save checkpoints</p>
</dd>
<dt><strong>save_best: str, optional, default is None</strong></dt><dd><p>if provided, key of metric f”{loader_name}_{loss_name}”
to monitor and save model with best eval result
Overrides save_every and saves on eval_interval</p>
</dd>
<dt><strong>save_dir: str | Path, default “./ckpt”</strong></dt><dd><p>directory at which to save training states if
save_every and/or save_best is provided</p>
</dd>
<dt><strong>resume_from_dir: str | Path, default None</strong></dt><dd><p>if provided, resumes training state (model, 
optimizer, regularizer, scheduler) from state saved in
<cite>resume_from_dir</cite></p>
</dd>
<dt><strong>max_autoregressive_steps</strong><span class="classifier">int, default None</span></dt><dd><p>if provided, and a dataloader is to be evaluated in autoregressive mode,
limits the number of autoregressive in each rollout to be performed.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt>all_metrics: dict</dt><dd><p>dictionary keyed f”{loader_name}_{loss_name}”
of metric results for last validation epoch across
all test_loaders</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neuralop.training.Trainer.train_one_epoch">
<span class="sig-name descname"><span class="pre">train_one_epoch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_loader</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_loss</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/neuralop/training/trainer.html#Trainer.train_one_epoch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neuralop.training.Trainer.train_one_epoch" title="Link to this definition"></a></dt>
<dd><p>train_one_epoch trains self.model on train_loader
for one epoch and returns training metrics</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>epoch</strong><span class="classifier">int</span></dt><dd><p>epoch number</p>
</dd>
<dt><strong>train_loader</strong><span class="classifier">torch.utils.data.DataLoader</span></dt><dd><p>data loader of train examples</p>
</dd>
<dt><strong>test_loaders</strong><span class="classifier">dict</span></dt><dd><p>dict of test torch.utils.data.DataLoader objects</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt>all_errors</dt><dd><p>dict of all eval metrics for the last epoch</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neuralop.training.Trainer.evaluate_all">
<span class="sig-name descname"><span class="pre">evaluate_all</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_losses</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_loaders</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_modes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_autoregressive_steps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/neuralop/training/trainer.html#Trainer.evaluate_all"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neuralop.training.Trainer.evaluate_all" title="Link to this definition"></a></dt>
<dd><p>evaluate_all iterates through the entire dict of test_loaders
to perform evaluation on the whole dataset stored in each one.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>epoch</strong><span class="classifier">int</span></dt><dd><p>current training epoch</p>
</dd>
<dt><strong>eval_losses</strong><span class="classifier">dict[Loss]</span></dt><dd><p>keyed <code class="docutils literal notranslate"><span class="pre">loss_name:</span> <span class="pre">loss_obj</span></code> for each pair. Full set of 
losses to use in evaluation for each test loader.</p>
</dd>
<dt><strong>test_loaders</strong><span class="classifier">dict[DataLoader]</span></dt><dd><p>keyed <code class="docutils literal notranslate"><span class="pre">loader_name:</span> <span class="pre">loader</span></code> for each test loader.</p>
</dd>
<dt><strong>eval_modes</strong><span class="classifier">dict[str], optional</span></dt><dd><p>keyed <code class="docutils literal notranslate"><span class="pre">loader_name:</span> <span class="pre">eval_mode</span></code> for each test loader.
* If <code class="docutils literal notranslate"><span class="pre">eval_modes.get(loader_name)</span></code> does not return a value, 
the evaluation is automatically performed in <code class="docutils literal notranslate"><span class="pre">single_step</span></code> mode.</p>
</dd>
<dt><strong>max_autoregressive_steps</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code>, optional</span></dt><dd><p>if provided, and one of the test loaders has <code class="docutils literal notranslate"><span class="pre">eval_mode</span> <span class="pre">==</span> <span class="pre">&quot;autoregressive&quot;</span></code>,
limits the number of autoregressive steps performed per rollout.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt>all_metrics: dict</dt><dd><p>collected eval metrics for each loader.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neuralop.training.Trainer.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_loader</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'single_step'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_steps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/neuralop/training/trainer.html#Trainer.evaluate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neuralop.training.Trainer.evaluate" title="Link to this definition"></a></dt>
<dd><p>Evaluates the model on a dictionary of losses</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>loss_dict</strong><span class="classifier">dict of functions</span></dt><dd><p>each function takes as input a tuple (prediction, ground_truth)
and returns the corresponding loss</p>
</dd>
<dt><strong>data_loader</strong><span class="classifier">data_loader to evaluate on</span></dt><dd></dd>
<dt><strong>log_prefix</strong><span class="classifier">str, default is ‘’</span></dt><dd><p>if not ‘’, used as prefix in output dictionary</p>
</dd>
<dt><strong>epoch</strong><span class="classifier">int | None</span></dt><dd><p>current epoch. Used when logging both train and eval
default None</p>
</dd>
<dt><strong>mode</strong><span class="classifier">Literal {‘single_step’, ‘autoregression’}</span></dt><dd><p>if ‘single_step’, performs standard evaluation
if ‘autoregression’ loops through <cite>max_steps</cite> steps</p>
</dd>
<dt><strong>max_steps</strong><span class="classifier">int, optional</span></dt><dd><p>max number of steps for autoregressive rollout. 
If None, runs the full rollout.</p>
</dd>
<dt><strong>Returns</strong></dt><dd></dd>
<dt><strong>——-</strong></dt><dd></dd>
<dt><strong>errors</strong><span class="classifier">dict</span></dt><dd><p>dict[f’{log_prefix}_{loss_name}] = loss for loss in loss_dict</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neuralop.training.Trainer.on_epoch_start">
<span class="sig-name descname"><span class="pre">on_epoch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/neuralop/training/trainer.html#Trainer.on_epoch_start"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neuralop.training.Trainer.on_epoch_start" title="Link to this definition"></a></dt>
<dd><p>on_epoch_start runs at the beginning
of each training epoch. This method is a stub
that can be overwritten in more complex cases.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>epoch</strong><span class="classifier">int</span></dt><dd><p>index of epoch</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt>None</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neuralop.training.Trainer.train_one_batch">
<span class="sig-name descname"><span class="pre">train_one_batch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_loss</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/neuralop/training/trainer.html#Trainer.train_one_batch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neuralop.training.Trainer.train_one_batch" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Run one batch of input through model</dt><dd><p>and return training loss on outputs</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>idx</strong><span class="classifier">int</span></dt><dd><p>index of batch within train_loader</p>
</dd>
<dt><strong>sample</strong><span class="classifier">dict</span></dt><dd><p>data dictionary holding one batch</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt>loss: float | Tensor</dt><dd><p>float value of training loss</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neuralop.training.Trainer.eval_one_batch">
<span class="sig-name descname"><span class="pre">eval_one_batch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_losses</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_output</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/neuralop/training/trainer.html#Trainer.eval_one_batch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neuralop.training.Trainer.eval_one_batch" title="Link to this definition"></a></dt>
<dd><p>eval_one_batch runs inference on one batch
and returns eval_losses for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>sample</strong><span class="classifier">dict</span></dt><dd><p>data batch dictionary</p>
</dd>
<dt><strong>eval_losses</strong><span class="classifier">dict</span></dt><dd><p>dictionary of named eval metrics</p>
</dd>
<dt><strong>return_outputs</strong><span class="classifier">bool</span></dt><dd><p>whether to return model outputs for plotting
by default False</p>
</dd>
<dt><strong>Returns</strong></dt><dd></dd>
<dt><strong>——-</strong></dt><dd></dd>
<dt><strong>eval_step_losses</strong><span class="classifier">dict</span></dt><dd><p>keyed “loss_name”: step_loss_value for each loss name</p>
</dd>
<dt><strong>outputs: torch.Tensor | None</strong></dt><dd><p>optionally returns batch outputs</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neuralop.training.Trainer.eval_one_batch_autoreg">
<span class="sig-name descname"><span class="pre">eval_one_batch_autoreg</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_losses</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_output</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_steps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/neuralop/training/trainer.html#Trainer.eval_one_batch_autoreg"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neuralop.training.Trainer.eval_one_batch_autoreg" title="Link to this definition"></a></dt>
<dd><p>eval_one_batch runs inference on one batch
and returns eval_losses for that batch.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>sample</strong><span class="classifier">dict</span></dt><dd><p>data batch dictionary</p>
</dd>
<dt><strong>eval_losses</strong><span class="classifier">dict</span></dt><dd><p>dictionary of named eval metrics</p>
</dd>
<dt><strong>return_outputs</strong><span class="classifier">bool</span></dt><dd><p>whether to return model outputs for plotting
by default False</p>
</dd>
<dt><strong>max_steps: int</strong></dt><dd><p>number of timesteps to roll out
typically the full trajectory length
If max_steps is none, runs until the full length</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If a value for <code class="docutils literal notranslate"><span class="pre">max_steps</span></code> is not provided, a data_processor
must be provided to handle rollout logic.</p>
</div>
</dd>
<dt><strong>Returns</strong></dt><dd></dd>
<dt><strong>——-</strong></dt><dd></dd>
<dt><strong>eval_step_losses</strong><span class="classifier">dict</span></dt><dd><p>keyed “loss_name”: step_loss_value for each loss name</p>
</dd>
<dt><strong>outputs: torch.Tensor | None</strong></dt><dd><p>optionally returns batch outputs</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neuralop.training.Trainer.log_training">
<span class="sig-name descname"><span class="pre">log_training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">avg_loss</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_err</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">avg_lasso_loss</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/neuralop/training/trainer.html#Trainer.log_training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neuralop.training.Trainer.log_training" title="Link to this definition"></a></dt>
<dd><p>Basic method to log results
from a single training epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>epoch: int</strong></dt><dd></dd>
<dt><strong>time: float</strong></dt><dd><p>training time of epoch</p>
</dd>
<dt><strong>avg_loss: float</strong></dt><dd><p>average train_err per individual sample</p>
</dd>
<dt><strong>train_err: float</strong></dt><dd><p>train error for entire epoch</p>
</dd>
<dt><strong>avg_lasso_loss: float</strong></dt><dd><p>average lasso loss from regularizer, optional</p>
</dd>
<dt><strong>lr: float</strong></dt><dd><p>learning rate at current epoch</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neuralop.training.Trainer.log_eval">
<span class="sig-name descname"><span class="pre">log_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_metrics</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/neuralop/training/trainer.html#Trainer.log_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neuralop.training.Trainer.log_eval" title="Link to this definition"></a></dt>
<dd><p>log_eval logs outputs from evaluation
on all test loaders to stdout and wandb</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>epoch</strong><span class="classifier">int</span></dt><dd><p>current training epoch</p>
</dd>
<dt><strong>eval_metrics</strong><span class="classifier">dict</span></dt><dd><p>metrics collected during evaluation
keyed f”{test_loader_name}_{metric}” for each test_loader</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neuralop.training.Trainer.resume_state_from_dir">
<span class="sig-name descname"><span class="pre">resume_state_from_dir</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">save_dir</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/neuralop/training/trainer.html#Trainer.resume_state_from_dir"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neuralop.training.Trainer.resume_state_from_dir" title="Link to this definition"></a></dt>
<dd><p>Resume training from save_dir created by <cite>neuralop.training.save_training_state</cite></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neuralop.training.Trainer.checkpoint">
<span class="sig-name descname"><span class="pre">checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">save_dir</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/neuralop/training/trainer.html#Trainer.checkpoint"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neuralop.training.Trainer.checkpoint" title="Link to this definition"></a></dt>
<dd><p>checkpoint saves current training state
to a directory for resuming later. Only saves 
training state on the first GPU. 
See neuralop.training.training_state</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>save_dir</strong><span class="classifier">str | Path</span></dt><dd><p>directory in which to save training state</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<div class="clearer"></div></section>


      </div>

      
        <nav class="pagination" role="navigation" aria-label="pagination">
    
    <a class="button pagination-previous" href="neuralop.models.base_model.available_models.html" title="previous page" accesskey="p">
        <span class="icon">
            <i class="fa fa-arrow-circle-left"></i>
        </span>
        <span><code class="xref py py-mod docutils literal notranslate"><span class="pre">neuralop.models.base_model</span></code>.available_models</span>
    </a>
    
    
    <a class="button pagination-next" href="neuralop.training.IncrementalFNOTrainer.html" title="next page" accesskey="n">
        <span><code class="xref py py-mod docutils literal notranslate"><span class="pre">neuralop.training</span></code>.IncrementalFNOTrainer </span>
        <span class="icon">
            <i class="fa fa-arrow-circle-right"></i>
        </span>
    </a>
    
</nav>

      

        <footer class="footer">
    <div class="content has-text-centered">
        <div class="block">
          &copy; Copyright 2025, Jean Kossaifi, David Pitt, Nikola Kovachki, Zongyi Li and Anima Anandkumar.<br/>
        </div>
    </div>
  </footer>

    </div>

  </div>  

	
    
    <div class="column is-hidden-touch is-2-desktop is-one-fifth-widescreen" id="localtoc-column">

    <aside class="sticky-nav localtoc"> 
        <p class="menu-label"> 
            <span class="icon-text">
                <span class="icon"><i class="fas fa-duotone fa-list"></i></span>
                <span> On this page </span>
            </span>
        </p>

        <div class="menu menu-list localtoc-list">
        <ul>
<li><a class="reference internal" href="#"><code class="xref py py-mod docutils literal notranslate"><span class="pre">neuralop.training</span></code>.Trainer</a><ul>
<li><a class="reference internal" href="#neuralop.training.Trainer"><code class="docutils literal notranslate"><span class="pre">Trainer</span></code></a><ul>
<li><a class="reference internal" href="#neuralop.training.Trainer.train"><code class="docutils literal notranslate"><span class="pre">Trainer.train()</span></code></a></li>
<li><a class="reference internal" href="#neuralop.training.Trainer.train_one_epoch"><code class="docutils literal notranslate"><span class="pre">Trainer.train_one_epoch()</span></code></a></li>
<li><a class="reference internal" href="#neuralop.training.Trainer.evaluate_all"><code class="docutils literal notranslate"><span class="pre">Trainer.evaluate_all()</span></code></a></li>
<li><a class="reference internal" href="#neuralop.training.Trainer.evaluate"><code class="docutils literal notranslate"><span class="pre">Trainer.evaluate()</span></code></a></li>
<li><a class="reference internal" href="#neuralop.training.Trainer.on_epoch_start"><code class="docutils literal notranslate"><span class="pre">Trainer.on_epoch_start()</span></code></a></li>
<li><a class="reference internal" href="#neuralop.training.Trainer.train_one_batch"><code class="docutils literal notranslate"><span class="pre">Trainer.train_one_batch()</span></code></a></li>
<li><a class="reference internal" href="#neuralop.training.Trainer.eval_one_batch"><code class="docutils literal notranslate"><span class="pre">Trainer.eval_one_batch()</span></code></a></li>
<li><a class="reference internal" href="#neuralop.training.Trainer.eval_one_batch_autoreg"><code class="docutils literal notranslate"><span class="pre">Trainer.eval_one_batch_autoreg()</span></code></a></li>
<li><a class="reference internal" href="#neuralop.training.Trainer.log_training"><code class="docutils literal notranslate"><span class="pre">Trainer.log_training()</span></code></a></li>
<li><a class="reference internal" href="#neuralop.training.Trainer.log_eval"><code class="docutils literal notranslate"><span class="pre">Trainer.log_eval()</span></code></a></li>
<li><a class="reference internal" href="#neuralop.training.Trainer.resume_state_from_dir"><code class="docutils literal notranslate"><span class="pre">Trainer.resume_state_from_dir()</span></code></a></li>
<li><a class="reference internal" href="#neuralop.training.Trainer.checkpoint"><code class="docutils literal notranslate"><span class="pre">Trainer.checkpoint()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
    </aside>
    </div>

  

  </div>  
  </div> 

  
  <script>
    function toggle_sidebar() {
        var element = document.getElementById("sidebar");
        var container = document.getElementById("column-container");
        var localtoccolumn = document.getElementById("localtoc-column");
        element.classList.toggle("hide-tablet");
        element.classList.toggle("is-hidden-mobile");
        container.classList.toggle("sidemenu-hidden");
        localtoccolumn.classList.toggle("is-one-fifth-widescreen");
        localtoccolumn.classList.toggle("is-2-desktop");
        localtoccolumn.classList.toggle("is-3-desktop");
    }
  </script> 



  </body>
</html>